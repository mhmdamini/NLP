{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a0a02b7-92ba-4467-9dc0-7757b41639da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a141a618-d363-4637-928b-d593b2d56e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "e54d80b3-17e1-4058-9f74-0b21a7a57d62",
   "metadata": {
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "\n",
    "###############################################################################\n",
    "# 1. Configure OpenAI\n",
    "###############################################################################\n",
    "# Replace this with a safer method (e.g., environment variable) in production\n",
    "# openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "def test_api_access():\n",
    "    \"\"\"\n",
    "    Attempts to list OpenAI models to confirm that the API key is valid.\n",
    "    Prints a success or failure message, along with a list of available models if successful.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        models = openai.Model.list()\n",
    "        # print(\"Access to OpenAI API successful! Available models:\")\n",
    "        # for model in models['data']:\n",
    "        #     print(f\" - {model['id']}\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to access the OpenAI API:\")\n",
    "        print(e)\n",
    "\n",
    "###############################################################################\n",
    "# 2. Core LLM function (using OpenAI GPT models)\n",
    "###############################################################################\n",
    "def generate_gpt(prompt, model_name=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Sends a prompt to the OpenAI GPT (ChatCompletion) and returns the response text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message[\"content\"]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with the GPT call: {e}\")\n",
    "        return None\n",
    "\n",
    "###############################################################################\n",
    "# 3. Extract first word utility\n",
    "###############################################################################\n",
    "def extract_word_from_response(response_text):\n",
    "    \"\"\"\n",
    "    Extracts and returns the first word-like token from the response_text.\n",
    "    Returns None if no valid word is found or if response_text is None.\n",
    "    \"\"\"\n",
    "    if response_text is None:\n",
    "        return None\n",
    "    cleaned_text = re.sub(r\"[^\\w'-]\", \" \", response_text)\n",
    "    word = re.search(r'\\b\\w+\\b', cleaned_text)\n",
    "    return word.group(0) if word else None\n",
    "\n",
    "###############################################################################\n",
    "# 4. Few-shot example formatter\n",
    "###############################################################################\n",
    "def prompt_few_shot(data, Question_Type, num_examples):\n",
    "    \"\"\"\n",
    "    Filters the dataset by Question_Type, samples up to num_examples,\n",
    "    and formats them as example references to be appended to the main prompt.\n",
    "    \"\"\"\n",
    "    filtered_df = data[data.iloc[:, 0] == Question_Type]\n",
    "    if filtered_df.empty:\n",
    "        return \"\"\n",
    "    else:\n",
    "        examples = filtered_df.sample(n=min(num_examples, len(filtered_df)))\n",
    "        formatted_examples = \"\"\n",
    "        for _, row in examples.iterrows():\n",
    "            correct_choice = row[f\"Choice_{row['Correct_Answer']}\"]\n",
    "            formatted_examples += f\"For Example:\\n\"\n",
    "            formatted_examples += f\"Question: {row['Instruction']}\\n\"\n",
    "            formatted_examples += f\"A) {row['Choice_1']}\\n\"\n",
    "            formatted_examples += f\"B) {row['Choice_2']}\\n\"\n",
    "            formatted_examples += f\"C) {row['Choice_3']}\\n\"\n",
    "            formatted_examples += f\"Correct Answer: {correct_choice}\\n\"\n",
    "            formatted_examples += (\n",
    "                f\"Explanation: Task difficulty of this question is {row['Task_Difficulty']}, \"\n",
    "                f\"and word difficulty of this question is {row['Word_Difficulty']}\\n\\n\"\n",
    "                f\"This is few_shot examples, generate different questions from these examples\\n\\n\"\n",
    "            )\n",
    "        return formatted_examples\n",
    "\n",
    "###############################################################################\n",
    "# 5. Chain-of-thought (single-prompt)\n",
    "###############################################################################\n",
    "def prompt_chain_of_thought():\n",
    "    \"\"\"\n",
    "    Appends a general chain-of-thought instruction.\n",
    "    No CSV examples are used—just an instruction telling the model\n",
    "    to 'think aloud' before finalizing the question.\n",
    "    \"\"\"\n",
    "    # Ensuring we keep the final question in a parseable format\n",
    "    chain_instruction = f\"\"\"\n",
    "--- Chain of Thought ---\n",
    "Please think aloud, and provide your reasoning before providing the final 3-choice question.\n",
    "Include your reasoning in the final output as well.\n",
    "\n",
    "Finally, PRESENT the final question in this format:\n",
    "Question: [your question]\n",
    "A) [option A]\n",
    "B) [option B]\n",
    "C) [option C]\n",
    "Correct Answer: [the correct choice]\n",
    "\"\"\"\n",
    "    return chain_instruction\n",
    "\n",
    "###############################################################################\n",
    "# 6A. \"Fake\" single-prompt chain_of_thought_plus_sequential\n",
    "###############################################################################\n",
    "def prompt_chain_of_thought_plus_sequential(question_type, word_difficulty, task_difficulty):\n",
    "    \"\"\"\n",
    "    A multi-step chain-of-thought approach. The final prompt instructs the model\n",
    "    to create the MCQ in 3 steps (selecting words, drafting a question, adding distractors),\n",
    "    each time showing its chain-of-thought reasoning.\n",
    "    \"\"\"\n",
    "    multi_step_instructions = f\"\"\"\n",
    "--- Chain of Thought + Sequential Steps ---\n",
    "\n",
    "We want a 3-choice question for question_type={question_type}.\n",
    "Word difficulty = {word_difficulty}, Task difficulty = {task_difficulty}.\n",
    "\n",
    "Please follow these steps in your final output (all in one go):\n",
    "\n",
    "Step 1: List three suitable Grade 3-5 words that illustrate the morphological concept\n",
    "         (prefix, suffix, root, etc. depending on question_type).\n",
    "         Show reasoning why each word is appropriate.\n",
    "         Then select exactly ONE of them.\n",
    "\n",
    "Step 2: Using the single selected word, generate a DRAFT 3-choice question.\n",
    "        Provide chain-of-thought: i.e., explain your reasoning for how the question is framed.\n",
    "\n",
    "Step 3: Add one correct answer choice and two distractors.\n",
    "        Provide chain-of-thought for how each distractor might trick the student,\n",
    "        and confirm which is correct.\n",
    "\n",
    "Finally, PRESENT the final question in this format:\n",
    "Question: [your question]\n",
    "A) [option A]\n",
    "B) [option B]\n",
    "C) [option C]\n",
    "Correct Answer: [the correct choice]\n",
    "\n",
    "Be explicit with your chain-of-thought reasoning for each step,\n",
    "but ensure the final output ends with the standard question format shown above.\n",
    "    \"\"\"\n",
    "    return multi_step_instructions\n",
    "\n",
    "\n",
    "def parse_chosen_word(response_text):\n",
    "    \"\"\"\n",
    "    Try to parse the chosen word from the multi-step response text.\n",
    "    We look for specific cues like 'Final word:' or 'Chosen word:' or\n",
    "    text like 'I would choose \"XYZ\".'\n",
    "    If none of these are found, we fall back to a naive approach.\n",
    "    \"\"\"\n",
    "    if not response_text:\n",
    "        return None\n",
    "\n",
    "    # 0) Look for something like: Final word choice: XYZ\n",
    "    match = re.search(r'(?i)final word choice:\\s*([A-Za-z\\'\\-]+)', response_text)\n",
    "    if match:\n",
    "        return match.group(1)    \n",
    "        \n",
    "    # 1) Look for something like: Final word: XYZ\n",
    "    match = re.search(r'(?i)final word:\\s*([A-Za-z\\'\\-]+)', response_text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    # 2) Look for something like: Chosen word: XYZ\n",
    "    match = re.search(r'(?i)chosen word:\\s*([A-Za-z\\'\\-]+)', response_text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    # 3) Look for: I would choose \"XYZ\"\n",
    "    match = re.search(r'(?i)i would choose\\s+\"([^\"]+)\"', response_text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    # 4) Look for: I choose \"XYZ\"\n",
    "    match = re.search(r'(?i)i choose\\s+\"([^\"]+)\"', response_text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    # 5) As a last resort, do a naive approach: first alphabetic word.\n",
    "    cleaned_text = re.sub(r\"[^\\w'-]\", \" \", response_text)\n",
    "    first_word = re.search(r'\\b[a-zA-Z\\'\\-]+\\b', cleaned_text)\n",
    "    return first_word.group(0) if first_word else None\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6B. REAL multi-step approach: chain_of_thought_plus_sequential_rl\n",
    "###############################################################################\n",
    "def prompt_chain_of_thought_plus_sequential_rl(question_type, word_difficulty, task_difficulty, forbidden_list=None, **kwargs):\n",
    "    \"\"\"\n",
    "    TRUE multi-step approach that calls GPT multiple times.\n",
    "\n",
    "    Step 0 (New): We prepend an instruction prompt based on question_type.\n",
    "    Step 1: We ask GPT for 3 suitable words + chain-of-thought, then parse out\n",
    "            the chosen word.\n",
    "    Step 2: We feed that chosen word to GPT, ask for a draft 3-choice question\n",
    "            (with chain-of-thought).\n",
    "    Step 3: We ask GPT to add distractors & finalize the parseable question.\n",
    "\n",
    "    Returns (final_text, the_word).\n",
    "    \"\"\"\n",
    "        \n",
    "    word_list = kwargs.get(\"word_list\", None)\n",
    "    if question_type in [4, 5] and word_list is None:\n",
    "        raise ValueError(\"word_list is required for question_type 4 and 5\")\n",
    "\n",
    "\n",
    "    if forbidden_list is None:\n",
    "        forbidden_list = []\n",
    "        \n",
    "    # ---------------------------\n",
    "    # Define all question prompts\n",
    "    # ---------------------------\n",
    "    prompt_instruction_qt1 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about prefixes. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to identify the prefix in a chosen word and provide \"\n",
    "        f\"two incorrect choices along with the correct answer.\"\n",
    "    )\n",
    "    prompt_instruction_qt2 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about suffixes. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to identify the suffix in the chosen word \"\n",
    "        f\"and provide two incorrect choices along with the correct answer. \"\n",
    "    )\n",
    "    prompt_instruction_qt3 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about root words. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to identify the root word in the chosen word \"\n",
    "        f\"and provide two incorrect choices along with the correct answer. \"\n",
    "    )\n",
    "    prompt_instruction_qt4 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about morphemes. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to identify the word that does NOT share \"\n",
    "        f\"the same prefix as the others from the given words {word_list}. \"\n",
    "    )\n",
    "    prompt_instruction_qt5 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about morphemes. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to identify the word that does NOT share \"\n",
    "        f\"the same suffix as the others from the given words {word_list}. \"\n",
    "    )\n",
    "    prompt_instruction_qt6 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about word transformations. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to transform the chosen word to a new meaning, \"\n",
    "        f\"with two incorrect choices and one correct answer. \"\n",
    "    )\n",
    "    prompt_instruction_qt7 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about affixed words. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to select the correct meaning of the chosen word  \"\n",
    "        f\"from three answer choices. \"\n",
    "    )\n",
    "    # prompt_restriction used in qt8 is assumed defined elsewhere or can be set to an empty string if not needed.\n",
    "    prompt_instruction_qt8 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about spelling based on morpheme meaning\"\n",
    "        f\"{{prompt_restriction}} \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should include a word with a suffix, provide two misspelled variations \"\n",
    "        f\"and one correct spelling.\"\n",
    "    )\n",
    "    prompt_instruction_qt9 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning to break affixed words into parts. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to break the chosen word into its correct parts \"\n",
    "        f\"(prefix, root, suffix) and provide two incorrect choices along with the correct answer.\"\n",
    "    )\n",
    "    prompt_instruction_qt10 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about prefixes. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to select the correct definition of the prefix \"\n",
    "        f\"in the chosen word from three answer choices.\"\n",
    "    )\n",
    "    prompt_instruction_qt11 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about root words in affixed words. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to select the correct definition of the root word \"\n",
    "        f\"in the chosen word from three answer choices.\"\n",
    "    )\n",
    "    prompt_instruction_qt12 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about suffixes. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to select the correct definition or function of the suffix \"\n",
    "        f\"in the chosen word from three answer choices.\"\n",
    "    )\n",
    "    prompt_instruction_qt13 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about morphologically complex words. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to select the correct definition of the chosen word \"\n",
    "        f\"based on its morphemes.\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Map question_type to the corresponding prompt instruction\n",
    "    # -------------------------------------------------------\n",
    "    instructions_map = {\n",
    "        1: prompt_instruction_qt1,\n",
    "        2: prompt_instruction_qt2,\n",
    "        3: prompt_instruction_qt3,\n",
    "        4: prompt_instruction_qt4,\n",
    "        5: prompt_instruction_qt5,\n",
    "        6: prompt_instruction_qt6,\n",
    "        7: prompt_instruction_qt7,\n",
    "        8: prompt_instruction_qt8,\n",
    "        9: prompt_instruction_qt9,\n",
    "        10: prompt_instruction_qt10,\n",
    "        11: prompt_instruction_qt11,\n",
    "        12: prompt_instruction_qt12,\n",
    "        13: prompt_instruction_qt13\n",
    "    }\n",
    "\n",
    "    # ---------------------------\n",
    "    # Retrieve the question prompt\n",
    "    # ---------------------------\n",
    "    question_prompt_instruction = instructions_map.get(\n",
    "        question_type,\n",
    "        f\"[No prompt defined for question_type={question_type}]\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ---------------------------\n",
    "    # Step 1: Pick Words\n",
    "    # ---------------------------\n",
    "    # We prepend the relevant instruction prompt here\n",
    "    if question_type not in [4, 5]:\n",
    "        step1_prompt = f\"\"\"\n",
    "    {question_prompt_instruction}\n",
    "\n",
    "    Step 1 (Pick Words):\n",
    "    Question Type = {question_type}\n",
    "    Word Difficulty = {word_difficulty}, Task Difficulty = {task_difficulty}\n",
    "\n",
    "    Please list three suitable Grade 3-5 words that fit the morphological concept\n",
    "    (prefix, suffix, root, etc.) for this Question Type: {question_type}. Have in mind that this word is going to be used for this instruction {question_prompt_instruction}. \n",
    "    Do NOT generate the whole question yet. In this step just generate and choose appropriate word considering what the question is about. Explain your chain-of-thought for each choice (why is it appropriate?).\n",
    "    Then select exactly ONE of the three as the final word, and provide reasoning for it (think aloud).\n",
    "    \"\"\"\n",
    "        step1_result = generate_gpt(step1_prompt)\n",
    "        print(f\"==> Step 1 result: {step1_result}\\n\")\n",
    "        # Just do a naive parse: find \"Chosen Word:\" or something\n",
    "        # If there's no consistent structure, we could guess or rely on a simpler approach\n",
    "        chosen_word = parse_chosen_word(step1_result)\n",
    "\n",
    "        # If we can't parse the chosen word, fallback\n",
    "        if not chosen_word:\n",
    "            chosen_word = \"mysteryWord\"\n",
    "\n",
    "                \n",
    "        # Single-check if chosen_word is in forbidden_list\n",
    "        if chosen_word.lower() in [fw.lower() for fw in forbidden_list]:\n",
    "            # Force a single re-ask with a \"temp_prompt\"\n",
    "            conflict_prompt = (\n",
    "                f\"You gave the word '{chosen_word}', but it's in the forbidden list: {forbidden_list}.\\n\"\n",
    "                f\"Please choose a different word that meets the previously-defined criteria, and also is NOT in that list.\"\n",
    "            )\n",
    "            step1_result_2 = generate_gpt(conflict_prompt)\n",
    "            chosen_word = parse_chosen_word(step1_result_2)\n",
    "            if not chosen_word:\n",
    "                chosen_word = \"mysteryWord\"\n",
    "                    \n",
    "        \n",
    "    else:\n",
    "        step1_prompt = f\"\"\"\n",
    "    {question_prompt_instruction}\n",
    "\n",
    "    Step 1 (Pick Words):\n",
    "    Question Type = {question_type}\n",
    "    Word Difficulty = {word_difficulty}, Task Difficulty = {task_difficulty}\n",
    "\n",
    "    Have in mind that everything should be suitable for Grade 3-5 words that fit the morphological concept\n",
    "    (prefix, suffix, root, etc.) for this Question Type: {question_type}. \n",
    "    You should only consider words that one of them does NOT share the same prefix as the others from the given words {word_list}. \n",
    "    Have in mind that this word is going to be used for this instruction {question_prompt_instruction}. \n",
    "    Do NOT generate the whole question yet. In this step just consider choosing the appropriate word considering what the question is about. \n",
    "    Explain your chain-of-thought for each choice (why is it appropriate?). \n",
    "    SKIP THIS STEP, AND MOVE FORWARD WITH STEP 2.\n",
    "    \"\"\"\n",
    "        step1_result = generate_gpt(step1_prompt)\n",
    "        print(f\"==> Step 1 result: {step1_result}\\n\")\n",
    "        # Just do a naive parse: find \"Chosen Word:\" or something\n",
    "        # If there's no consistent structure, we could guess or rely on a simpler approach\n",
    "        chosen_word = parse_chosen_word(step1_result)\n",
    "\n",
    "        # If we can't parse the chosen word, fallback\n",
    "        if not chosen_word:\n",
    "            chosen_word = \"mysteryWord\"\n",
    "\n",
    "        # # Single-check if chosen_word is in forbidden_list\n",
    "        # if chosen_word.lower() in [fw.lower() for fw in forbidden_list]:\n",
    "        #     # Force a single re-ask with a \"temp_prompt\"\n",
    "        #     conflict_prompt = (\n",
    "        #         f\"You gave the word '{chosen_word}', but it's in the forbidden list: {forbidden_list}.\\n\"\n",
    "        #         f\"Please choose a different word that meets the previously-defined criteria, and also is NOT in that list.\"\n",
    "        #     )\n",
    "        #     step1_result_2 = generate_gpt(conflict_prompt)\n",
    "        #     chosen_word = parse_chosen_word(step1_result_2)\n",
    "        #     if not chosen_word:\n",
    "        #         chosen_word = \"mysteryWord\"\n",
    "                \n",
    "    # ---------------------------\n",
    "    # Step 2: Draft question\n",
    "    # ---------------------------\n",
    "    step2_prompt = f\"\"\"\n",
    "    Step 2 (Draft Question):\n",
    "    We have chosen the word: {chosen_word}.\n",
    "\n",
    "    Now draft a 3-choice question based on the following instruction: {question_prompt_instruction}. Provide a chain-of-thought explaining\n",
    "    how you formed the question, and provide the correct answer.\n",
    "\n",
    "    Do NOT finalize the answer choices yet. Give the question text\n",
    "    and placeholders for A/B/C. For instance:\n",
    "    \"Question: ... A) ... B) ... C) ...\"\n",
    "\n",
    "    \"\"\"\n",
    "    step2_result = generate_gpt(step2_prompt)\n",
    "    # We'll accept step2_result as a partial question\n",
    "    print(f\"==> Step 2 result: {step2_result}\\n\")\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Step 3: Add distractors & finalize\n",
    "    # ---------------------------\n",
    "    step3_prompt = f\"\"\"\n",
    "    Step 3 (Add Choices & Finalize):\n",
    "    Based on the draft question and the correct answer you provided in the previous step:\n",
    "\n",
    "    {step2_result}\n",
    "\n",
    "    Update the TWO distractors with reasoning, considering the specified task difficulty of: {task_difficulty}. Provide chain-of-thought\n",
    "    about how each distractor might trick the student, and confirm the correct answer.\n",
    "\n",
    "    Finally, shuffle the answer choices and present the final question in a parseable format:\n",
    "    Question: ...\n",
    "    A) ...\n",
    "    B) ...\n",
    "    C) ...\n",
    "    Correct Answer: ...\n",
    "    Do NOT provide the reasonings in the above parseable format. Write them here, after everything is done.\n",
    "    \"\"\"\n",
    "    step3_result = generate_gpt(step3_prompt)\n",
    "    print(f\"==> Step 3 result: {step3_result}\\n\")\n",
    "    # The final text to parse is step3_result\n",
    "    final_text = step3_result\n",
    "    print(f\"====== HERE IS THE FORBIDDEN_LIST: {forbidden_list}\\n\")\n",
    "    return final_text, chosen_word\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 7. NEW Strategy: chain_of_thought_plus_role_chain\n",
    "###############################################################################\n",
    "def prompt_chain_of_thought_plus_role_chain(question_type, word_difficulty, task_difficulty):\n",
    "    \"\"\"\n",
    "    A multi-role chain-of-thought approach. The model is asked to produce an MCQ \n",
    "    (with exactly 3 choices) for a grade 3-5 audience, but does so by iterating \n",
    "    through three different 'roles' in its reasoning:\n",
    "\n",
    "    Role 1) Teacher:\n",
    "       - Think about how to instruct a Grade 3-5 student. \n",
    "       - Provide a suitable question and partial answer choices in plain, accessible language.\n",
    "       - Make sure the question is neither too trivial nor too advanced for grades 3-5.\n",
    "       - Provide your chain-of-thought on how you arrived at this version.\n",
    "\n",
    "    Role 2) Student:\n",
    "       - Evaluate the Teacher's question from a student’s perspective.\n",
    "       - Check if any distractor is obviously or trivially wrong, or if the question is unclear.\n",
    "       - Suggest if the question might be confusing or if it needs improvement.\n",
    "       - Provide your chain-of-thought on how you analyzed the question.\n",
    "\n",
    "    Role 3) Technometrician (or Psychometrician):\n",
    "       - Verify that the question meets the morphological objective implied by question_type\n",
    "         (e.g., identifying prefix/suffix, ensuring correct root word, etc.).\n",
    "       - Check that the difficulty aligns with the stated word/task difficulty.\n",
    "       - Offer final refinements if needed, then present the final version of the question \n",
    "         in a parseable format:\n",
    "\n",
    "         Final Question: ...\n",
    "         A) ...\n",
    "         B) ...\n",
    "         C) ...\n",
    "         Correct Answer: ...\n",
    "\n",
    "    The model should output the chain-of-thought for each role, followed by the final question.\n",
    "    The final question must always end in the parseable format shown above. \n",
    "    \"\"\"\n",
    "\n",
    "    role_instructions = f\"\"\"\n",
    "--- 3-Role Reasoning for a Grade 3-5 3-choice question ---\n",
    "\n",
    "We want a 3-choice question for:\n",
    "  question_type = {question_type}\n",
    "  word_difficulty = {word_difficulty}\n",
    "  task_difficulty = {task_difficulty}\n",
    "\n",
    "You should act in all of the following three roles, one by one, and think aloud in each of them (provide reasonings):\n",
    "==========\n",
    "Roles & Instructions:\n",
    "==========\n",
    "\n",
    "(1) Teacher Role\n",
    "  - Act as a Grade 3–5 teacher.\n",
    "  - Propose a question that is suitable for a grade 3-5 student, focusing on the morphological concept \n",
    "    (prefix, suffix, root, etc.) relevant to question_type={question_type}.\n",
    "  - Provide your chain-of-thought on how the question was formed (provide reasoning, or think aloud), \n",
    "    ensuring it's neither too trivial nor too advanced for grades 3–5.\n",
    "  - Then pass along your question and partial choices to the next role.\n",
    "\n",
    "(2) Student Role\n",
    "  - Act as a Grade 3–5 student.\n",
    "  - Read what the Teacher proposed. \n",
    "  - Comment if the question is confusing, or if any distractor is obviously incorrect.\n",
    "  - Provide your chain-of-thought as a student (provide reasoning, or think aloud).\n",
    "  - Then pass along your outputs to the next role.\n",
    "\n",
    "(3) Technometrician Role\n",
    "  - Act as a test-design specialist focusing on morphological objectives (prefix, suffix, root, etc.) \n",
    "    and checking alignment with word_difficulty={word_difficulty} & task_difficulty={task_difficulty}.\n",
    "  - Evaluate the question from both Teacher and Student roles:\n",
    "    - Are we accurately testing the morphological skill for question_type={question_type}?\n",
    "    - Are the difficulty levels appropriate?\n",
    "  - Provide final refinements if needed. Be strict if you can make the question or the distractors more aligned with what is asked.\n",
    "  - Then present the finalized question in a parseable format, exactly as follows:\n",
    "\n",
    "      Final Question: [refined question text here]\n",
    "      A) [choice A]\n",
    "      B) [choice B]\n",
    "      C) [choice C]\n",
    "      Correct Answer: [the correct choice]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return role_instructions\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 8. Prompt Generators for Each Question Type\n",
    "###############################################################################\n",
    "def generate_prefix_prompt(word_difficulty, task_difficulty, data, prompting, words):  # question_type = 1\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has a prefix{word_exclusion} \"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    # -- 1) GPT call to pick a word\n",
    "    result = generate_gpt(gen_prompt)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    # -- 2) The main MCQ prompt (Zero-shot style)\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about prefixes. \"\n",
    "        f\"The word is '{word}', with word difficulty {word_difficulty} and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to identify the prefix in '{word}' and provide \"\n",
    "        f\"two incorrect choices along with the correct answer. Please specify the correct answer.\"\n",
    "    )\n",
    "        \n",
    "    # -- 3) Append optional prompting strategy\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 1, 6)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(1, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        # Call the RL version, which returns final_text and chosen_word\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            1, word_difficulty, task_difficulty, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(1, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        # default zero-shot\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_suffix_prompt(word_difficulty, task_difficulty, data, prompting, words):  # question_type = 2\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has a suffix{word_exclusion} \"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about suffixes. \"\n",
    "        f\"The word is '{word}', with word difficulty {word_difficulty} and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to identify the suffix in '{word}' and provide two \"\n",
    "        f\"incorrect choices along with the correct answer. Please specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 2, 6)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(2, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            2, word_difficulty, task_difficulty, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(2, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_root_word_prompt(word_difficulty, task_difficulty, data, prompting, words):  # question_type = 3\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has a prefix or suffix{word_exclusion} \"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about root words. \"\n",
    "        f\"The word is '{word}', with word difficulty {word_difficulty} and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to identify the root word in '{word}' and provide two \"\n",
    "        f\"incorrect choices along with the correct answer. Clearly specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 3, 6)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(3, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            3, word_difficulty, task_difficulty, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(3, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_common_prefix_prompt(word_list, word_difficulty, task_difficulty, data, prompting):  # question_type = 4\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about morphemes. \"\n",
    "        f\"The words given are {word_list}, with word difficulty {word_difficulty} and \"\n",
    "        f\"task difficulty {task_difficulty}. The question should ask the student to identify the word \"\n",
    "        f\"that does NOT share the same prefix as the others. Clearly specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 4, 6)\n",
    "        return prompt, word_list\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word_list\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(4, word_difficulty, task_difficulty)\n",
    "        return prompt, word_list\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            4, word_difficulty, task_difficulty, forbidden_list=[], word_list=word_list\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(4, word_difficulty, task_difficulty)\n",
    "        return prompt, word_list\n",
    "    else:\n",
    "        return prompt, word_list\n",
    "\n",
    "\n",
    "def generate_common_suffix_prompt(word_list, word_difficulty, task_difficulty, data, prompting):  # question_type = 5\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about morphemes. \"\n",
    "        f\"The words given are {word_list}, with word difficulty {word_difficulty} and \"\n",
    "        f\"task difficulty {task_difficulty}. The question should ask the student to identify the word \"\n",
    "        f\"that does NOT share the same suffix as the others. Clearly specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 5, 6)\n",
    "        return prompt, word_list\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word_list\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(5, word_difficulty, task_difficulty)\n",
    "        return prompt, word_list\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            5, word_difficulty, task_difficulty, forbidden_list=[], word_list=word_list\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(5, word_difficulty, task_difficulty)\n",
    "        return prompt, word_list\n",
    "    else:\n",
    "        return prompt, word_list\n",
    "\n",
    "\n",
    "def generate_word_transformation_prompt(word_difficulty, task_difficulty, data, prompting, words):  # question_type = 6\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has variations with different meanings{word_exclusion} \"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about word transformations. \"\n",
    "        f\"The word is '{word}', word difficulty {word_difficulty}, and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to transform '{word}' to a new meaning, with two incorrect \"\n",
    "        f\"choices and one correct answer. Clearly specify correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 6, 6)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(6, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            6, word_difficulty, task_difficulty, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(6, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_word_meaning_prompt(word_difficulty, task_difficulty, data, prompting, words):  # question_type = 7\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has a different meaning with a same prefix or suffix{word_exclusion} \"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about affixed words. \"\n",
    "        f\"The word is '{word}', with word difficulty {word_difficulty} and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to select the correct meaning of '{word}' from three answer choices. \"\n",
    "        f\"Clearly specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 7, 6)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(7, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            7, word_difficulty, task_difficulty, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(7, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_spelling_prompt(word_difficulty, task_difficulty, data, prompting, prompts):  # question_type = 8\n",
    "    if prompts:\n",
    "        prompt_restriction = (\n",
    "            f\" The question and the word should not be similar to any of the previously generated questions: \"\n",
    "            f\"{', '.join(prompts)}.\"\n",
    "        )\n",
    "    else:\n",
    "        prompt_restriction = \"\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about spelling based on morpheme meaning\"\n",
    "        f\"{prompt_restriction} The question should include a word with a suffix.\"\n",
    "        f\"Provide two misspelled variations and one correct spelling. The question should have word difficulty \"\n",
    "        f\"{word_difficulty} and task difficulty {task_difficulty}. Clearly specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 8, 6)\n",
    "        return prompt, prompt\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, prompt\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(8, word_difficulty, task_difficulty)\n",
    "        return prompt, prompt\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            8, word_difficulty, task_difficulty, forbidden_list=prompts\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(8, word_difficulty, task_difficulty)\n",
    "        return prompt, prompt\n",
    "    else:\n",
    "        return prompt, prompt\n",
    "\n",
    "\n",
    "def generate_affixed_word_breakdown_prompt(word_difficulty, task_difficulty, data, prompting, words):  # question_type = 9\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has at least three parts and at most four parts{word_exclusion} \"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning to break affixed words into parts. \"\n",
    "        f\"The word is '{word}', with word difficulty {word_difficulty} and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to break '{word}' into its correct parts (prefix, root, suffix) \"\n",
    "        f\"and provide two incorrect choices along with the correct answer. Please specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 9, 6)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(9, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            9, word_difficulty, task_difficulty, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(9, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_prefix_definition_prompt(word_difficulty, task_difficulty, data, prompting, words):  # question_type = 10\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has a prefix{word_exclusion} \"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about prefixes. \"\n",
    "        f\"The word is '{word}', with word difficulty {word_difficulty} and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to select the correct definition of the prefix in '{word}' from \"\n",
    "        f\"three answer choices. Please specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 10, 6)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(10, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            10, word_difficulty, task_difficulty, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(10, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_root_word_definition_prompt(word_difficulty, task_difficulty, data, prompting, words):  # question_type = 11\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has a prefix or suffix{word_exclusion} \"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about root words in affixed words. \"\n",
    "        f\"The word is '{word}', with word difficulty {word_difficulty} and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to select the correct definition of the root word in '{word}' from \"\n",
    "        f\"three answer choices. Please specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 11, 6)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(11, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            11, word_difficulty, task_difficulty, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(11, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_suffix_definition_prompt(word_difficulty, task_difficulty, data, prompting, words):  # question_type = 12\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has a suffix{word_exclusion}\"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about suffixes. \"\n",
    "        f\"The word is '{word}', with word difficulty {word_difficulty} and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to select the correct definition or function of the suffix in '{word}' \"\n",
    "        f\"from three answer choices. Please specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 12, 6)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(12, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            12, word_difficulty, task_difficulty, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(12, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_morphologically_complex_word_definition_prompt(word_difficulty, task_difficulty, data, prompting, words):  # question_type = 13\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word whose morpheme has a distinct meaning, can take a suffix or prefix,{word_exclusion}\"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about morphologically complex words. \"\n",
    "        f\"The word is '{word}'. With word difficulty {word_difficulty} and task difficulty {task_difficulty}, \"\n",
    "        f\"the question should ask the student to select the correct definition of '{word}' based on its morphemes. \"\n",
    "        f\"Please specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 13, 6)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(13, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            13, word_difficulty, task_difficulty, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(13, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "###############################################################################\n",
    "# 9. Dispatcher for Zero/Few/Chain-of-thought/Chain-of-thought-plus-sequential/\n",
    "#    Chain-of-thought-plus-role-chain\n",
    "###############################################################################\n",
    "\n",
    "def load_lists(file):\n",
    "    import json\n",
    "    with open(file,'r') as f:\n",
    "        list1=json.load(f)\n",
    "    return list1\n",
    "\n",
    "def prompt_generator(question_type, word_difficulty, task_difficulty, data, prompting, forbidden_list):\n",
    "    list1=load_lists('/blue/babajani.a/babak.ahmadi/NLP_Dorr/Project/MA/JsonLists/list1.json')\n",
    "    list2=load_lists('/blue/babajani.a/babak.ahmadi/NLP_Dorr/Project/MA/JsonLists/list2.json')\n",
    "\n",
    "    if question_type == 1:\n",
    "        return generate_prefix_prompt(word_difficulty, task_difficulty, data, prompting,forbidden_list)\n",
    "    elif question_type == 2:\n",
    "        return generate_suffix_prompt(word_difficulty, task_difficulty, data, prompting, forbidden_list)\n",
    "    elif question_type == 3:\n",
    "        return generate_root_word_prompt(word_difficulty, task_difficulty, data, prompting, forbidden_list)\n",
    "    elif question_type == 4:\n",
    "        allowed = [sublist for sublist in list1 if sublist not in forbidden_list]\n",
    "        random_word_list = random.choice(allowed)\n",
    "        return generate_common_prefix_prompt(random_word_list, word_difficulty, task_difficulty, data, prompting)\n",
    "    elif question_type == 5:\n",
    "        allowed = [sublist for sublist in list2 if sublist not in forbidden_list]\n",
    "        random_word_list = random.choice(allowed)\n",
    "        return generate_common_suffix_prompt(random_word_list, word_difficulty, task_difficulty, data, prompting)\n",
    "    elif question_type == 6:\n",
    "        return generate_word_transformation_prompt(word_difficulty, task_difficulty, data, prompting, forbidden_list)\n",
    "    elif question_type == 7:\n",
    "        return generate_word_meaning_prompt(word_difficulty, task_difficulty, data, prompting,forbidden_list)\n",
    "    elif question_type == 8:\n",
    "        return generate_spelling_prompt(word_difficulty, task_difficulty, data, prompting,forbidden_list)\n",
    "    elif question_type == 9:\n",
    "        return generate_affixed_word_breakdown_prompt(word_difficulty, task_difficulty, data, prompting,forbidden_list)\n",
    "    elif question_type == 10:\n",
    "        return generate_prefix_definition_prompt(word_difficulty, task_difficulty, data, prompting,forbidden_list)\n",
    "    elif question_type == 11:\n",
    "        return generate_root_word_definition_prompt(word_difficulty, task_difficulty, data, prompting,forbidden_list)\n",
    "    elif question_type == 12:\n",
    "        return generate_suffix_definition_prompt(word_difficulty, task_difficulty, data, prompting, forbidden_list)\n",
    "    elif question_type == 13:\n",
    "        return generate_morphologically_complex_word_definition_prompt(word_difficulty, task_difficulty, data, prompting,forbidden_list)\n",
    "    else:\n",
    "        # Catch-all for unspecified question_type\n",
    "        return generate_definition_prompt(word_difficulty, task_difficulty, data, prompting, forbidden_list)\n",
    "\n",
    "###############################################################################\n",
    "# 10. A more comprehensive question parser\n",
    "###############################################################################\n",
    "import re\n",
    "\n",
    "def parse_question_output(text):\n",
    "    \"\"\"\n",
    "    A comprehensive parser for all the prompting strategies we discussed:\n",
    "      - zero_shot\n",
    "      - few_shot\n",
    "      - chain_of_thought\n",
    "      - chain_of_thought_plus_sequential\n",
    "      - chain_of_thought_plus_sequential_rl\n",
    "      - chain_of_thought_plus_role_chain\n",
    "\n",
    "    This function carefully extracts:\n",
    "      - teacher_reasoning, student_reasoning, psychometrician_reasoning\n",
    "      - chain_of_thought\n",
    "      - up to 3 steps (step_1, step_2, step_3)\n",
    "      - question\n",
    "      - choice_a, choice_b, choice_c\n",
    "      - correct_answer\n",
    "\n",
    "    Key Logic / Fallbacks:\n",
    "      1) We match teacher/student/psychometrician with lines like:\n",
    "         'Teacher:', '(1) Teacher Role:', 'Student:', 'Psychometrician:', 'Technometrician:', etc.\n",
    "      2) Chain-of-thought can appear as '--- Chain of Thought ---', 'Chain-of-Thought:', etc.\n",
    "      3) Steps appear as 'Step 1:', 'Step 1 (Pick Words):', etc., up to Step 3.\n",
    "      4) Final question is searched in this order:\n",
    "           - 'Final Question:', 'Updated Question:', 'Final MCQ:', 'My Final Question is:'\n",
    "           - fallback: 'Question:'\n",
    "           - if that fails, we look for a line ending with '?'\n",
    "      5) Choices are labeled 'A)', 'A.', 'A:', 'B)', 'C)', etc. We use a multiline regex to capture\n",
    "         until we see the next choice label, 'Correct Answer:', or the end of the text (\\\\Z).\n",
    "      6) Correct Answer is first looked for with lines like 'Correct Answer:', 'Answer:', or\n",
    "         'The correct answer is:'. If not found, we see if one of the choices ends with '(Correct answer)'\n",
    "         (case-insensitive). If so, we remove that parenthetical from the choice and set that as correct_answer.\n",
    "\n",
    "    It returns a dictionary with these keys:\n",
    "      {\n",
    "        'question': str or None,\n",
    "        'choice_a': str or None,\n",
    "        'choice_b': str or None,\n",
    "        'choice_c': str or None,\n",
    "        'correct_answer': str or None,\n",
    "        'chain_of_thought': str or None,\n",
    "        'teacher_reasoning': str or None,\n",
    "        'student_reasoning': str or None,\n",
    "        'psychometrician_reasoning': str or None,\n",
    "        'step_1': str or None,\n",
    "        'step_2': str or None,\n",
    "        'step_3': str or None\n",
    "      }\n",
    "    \"\"\"\n",
    "\n",
    "    # 0) If there's no text or it's empty, return a blank parse.\n",
    "    if not text or not text.strip():\n",
    "        return {\n",
    "            'question': None,\n",
    "            'choice_a': None,\n",
    "            'choice_b': None,\n",
    "            'choice_c': None,\n",
    "            'correct_answer': None,\n",
    "            'chain_of_thought': None,\n",
    "            'teacher_reasoning': None,\n",
    "            'student_reasoning': None,\n",
    "            'psychometrician_reasoning': None,\n",
    "            'step_1': None,\n",
    "            'step_2': None,\n",
    "            'step_3': None\n",
    "        }\n",
    "\n",
    "    # Normalize line endings to \"\\n\"\n",
    "    text = text.replace('\\r\\n', '\\n')\n",
    "\n",
    "    # Our parsed results dictionary\n",
    "    parsed = {\n",
    "        'question': None,\n",
    "        'choice_a': None,\n",
    "        'choice_b': None,\n",
    "        'choice_c': None,\n",
    "        'correct_answer': None,\n",
    "        'chain_of_thought': None,\n",
    "        'teacher_reasoning': None,\n",
    "        'student_reasoning': None,\n",
    "        'psychometrician_reasoning': None,\n",
    "        'step_1': None,\n",
    "        'step_2': None,\n",
    "        'step_3': None\n",
    "    }\n",
    "\n",
    "    ################################################################\n",
    "    # 1) Extract Teacher / Student / Psychometrician reasonings\n",
    "    ################################################################\n",
    "    teacher_match = re.search(\n",
    "        r\"(?i)(?:\\(?\\d\\)?\\s*Teacher\\s*Role\\s*:|Teacher\\s*:)(.*?)(?=\\n\\s*(?:\\(?\\d\\)?\\s*Student\\s*Role\\s*:|Student\\s*:|\\(?\\d\\)?\\s*Psychometrician\\s*Role\\s*:|Psychometrician\\s*:|Technometrician\\s*:|\\(?\\d\\)?\\s*Technometrician\\s*Role\\s*:|Final\\s*Question\\s*:|Updated\\s*Question\\s*:|Question\\s*:|$))\",\n",
    "        text, re.DOTALL\n",
    "    )\n",
    "    if teacher_match:\n",
    "        parsed['teacher_reasoning'] = teacher_match.group(1).strip()\n",
    "\n",
    "    student_match = re.search(\n",
    "        r\"(?i)(?:\\(?\\d\\)?\\s*Student\\s*Role\\s*:|Student\\s*:)(.*?)(?=\\n\\s*(?:\\(?\\d\\)?\\s*Teacher\\s*Role\\s*:|Teacher\\s*:|\\(?\\d\\)?\\s*Psychometrician\\s*Role\\s*:|Psychometrician\\s*:|Technometrician\\s*:|\\(?\\d\\)?\\s*Technometrician\\s*Role\\s*:|Final\\s*Question\\s*:|Updated\\s*Question\\s*:|Question\\s*:|$))\",\n",
    "        text, re.DOTALL\n",
    "    )\n",
    "    if student_match:\n",
    "        parsed['student_reasoning'] = student_match.group(1).strip()\n",
    "\n",
    "    psych_match = re.search(\n",
    "        r\"(?i)(?:\\(?\\d\\)?\\s*Psychometrician\\s*Role\\s*:|Psychometrician\\s*:|\\(?\\d\\)?\\s*Technometrician\\s*Role\\s*:|Technometrician\\s*:)(.*?)(?=\\n\\s*(?:\\(?\\d\\)?\\s*Teacher\\s*Role\\s*:|Teacher\\s*:|\\(?\\d\\)?\\s*Student\\s*Role\\s*:|Student\\s*:|Final\\s*Question\\s*:|Updated\\s*Question\\s*:|Question\\s*:|$))\",\n",
    "        text, re.DOTALL\n",
    "    )\n",
    "    if psych_match:\n",
    "        parsed['psychometrician_reasoning'] = psych_match.group(1).strip()\n",
    "\n",
    "    ################################################################\n",
    "    # 2) Extract chain-of-thought\n",
    "    ################################################################\n",
    "    # Could appear as '--- Chain of Thought ---' or 'Chain-of-Thought:'\n",
    "    cot_match = re.search(\n",
    "        r\"(?i)(?:---\\s*Chain[\\s\\-_]*of[\\s\\-_]*Thought\\s*---|Chain[\\s\\-_]*of[\\s\\-_]*Thought\\s*:)(.*?)(?=\\n---|\\nQuestion|\\nStep|\\Z)\",\n",
    "        text, re.DOTALL\n",
    "    )\n",
    "    if cot_match:\n",
    "        parsed['chain_of_thought'] = cot_match.group(1).strip()\n",
    "\n",
    "    ################################################################\n",
    "    # 3) Extract up to 3 Steps: Step 1, Step 2, Step 3\n",
    "    ################################################################\n",
    "    # Usually from chain_of_thought_plus_sequential\n",
    "    for n in range(1, 4):\n",
    "        step_regex = (\n",
    "            rf\"(?is)(?:Step\\s*{n}\\s*(?:\\(.*?\\))?\\s*[:\\.]\\s*)(.*?)(?=\"\n",
    "            rf\"^\\s*Step\\s*{n+1}\\s*|^\\s*Question\\s*:|^\\s*Final\\s*Question\\s*:|^\\s*Updated\\s*Question\\s*:|\\Z)\"\n",
    "        )\n",
    "        match = re.search(step_regex, text, re.MULTILINE)\n",
    "        if match:\n",
    "            parsed[f'step_{n}'] = match.group(1).strip()\n",
    "            \n",
    "    # 3.1) Extract Step 1, Step 2, Step 3 (if present)\n",
    "    #    For example: \"Step 1 (Pick Words):\\n....\\nStep 2 (Draft Question): ...\"\n",
    "    #    We'll use a simple pattern that looks for lines starting with \"Step X\" up to the next \"Step\" or the end.\n",
    "    step_regex = re.compile(\n",
    "        r\"(?ims)(Step\\s*1\\s*\\(?.*?\\)?:\\s*)(.*?)(?=\\n\\s*Step\\s*2\\s*\\(?|$)\"\n",
    "    )\n",
    "    match = step_regex.search(text)\n",
    "    if match:\n",
    "        parsed['step_1'] = match.group(2).strip()\n",
    "\n",
    "    step_regex = re.compile(\n",
    "        r\"(?ims)(Step\\s*2\\s*\\(?.*?\\)?:\\s*)(.*?)(?=\\n\\s*Step\\s*3\\s*\\(?|$)\"\n",
    "    )\n",
    "    match = step_regex.search(text)\n",
    "    if match:\n",
    "        parsed['step_2'] = match.group(2).strip()\n",
    "\n",
    "    step_regex = re.compile(\n",
    "        r\"(?ims)(Step\\s*3\\s*\\(?.*?\\)?:\\s*)(.*)\"\n",
    "    )\n",
    "    match = step_regex.search(text)\n",
    "    if match:\n",
    "        parsed['step_3'] = match.group(2).strip()\n",
    "\n",
    "    ################################################################\n",
    "    # 4) Extract the final question from known headings\n",
    "    ################################################################\n",
    "    # \"Final Question:\", \"Updated Question:\", \"Final MCQ:\",\n",
    "    # \"My Final Question is:\", fallback \"Question:\"\n",
    "    question_pattern = re.compile(\n",
    "        r\"(?is)(?:\"\n",
    "        r\"(Final\\s+Question\\s*:\\s*)\"\n",
    "        r\"|(Updated\\s+Question\\s*:\\s*)\"\n",
    "        r\"|(Final\\s*MCQ\\s*:\\s*)\"\n",
    "        r\"|(My\\s+Final\\s+Question\\s+is\\s*:\\s*)\"\n",
    "        r\"|(Question\\s*:\\s*)\"\n",
    "        r\")(.+)\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    question_match = question_pattern.search(text)\n",
    "    if question_match:\n",
    "        remainder = question_match.group(len(question_match.groups()))\n",
    "        if remainder:\n",
    "            splitted = re.split(\n",
    "                r\"\\n\\s*(?:A\\)|A\\.|A:|Correct\\s*Answer\\s*:)\",\n",
    "                remainder,\n",
    "                maxsplit=1,\n",
    "                flags=re.IGNORECASE\n",
    "            )\n",
    "            parsed['question'] = splitted[0].strip() if splitted else remainder.strip()\n",
    "\n",
    "    ################################################################\n",
    "    # 5) Fallback: \"Question:\" if we didn't parse it above\n",
    "    ################################################################\n",
    "    if not parsed['question']:\n",
    "        q_fallback = re.search(r\"(?i)Question\\s*:\\s*(.*)\", text, re.DOTALL)\n",
    "        if q_fallback:\n",
    "            remainder = q_fallback.group(1)\n",
    "            splitted = re.split(\n",
    "                r\"\\n\\s*(?:A\\)|A\\.|A:|Correct\\s*Answer\\s*:)\",\n",
    "                remainder,\n",
    "                maxsplit=1,\n",
    "                flags=re.IGNORECASE\n",
    "            )\n",
    "            parsed['question'] = splitted[0].strip() if splitted else remainder.strip()\n",
    "\n",
    "    ################################################################\n",
    "    # 6) If STILL no question => find the first line ending with '?'\n",
    "    ################################################################\n",
    "    if not parsed['question']:\n",
    "        question_line_match = re.search(r\"^(.*\\?)\\s*$\", text, re.MULTILINE)\n",
    "        if question_line_match:\n",
    "            parsed['question'] = question_line_match.group(1).strip()\n",
    "\n",
    "    ################################################################\n",
    "    # 7) Extract choices A), B), C)\n",
    "    ################################################################\n",
    "    # We'll allow the pattern to continue until the next choice, a \"Correct Answer:\", \n",
    "    # a blank line, or the end of text (\\Z). That way we capture lines like\n",
    "    #   C) Un- (Correct answer)\n",
    "    # even if there's no trailing newline.\n",
    "    def capture_choice(label, full_text):\n",
    "        pattern = (\n",
    "            rf\"(?ims)^[ \\t]*{label}\\s*[\\)\\.:]\\s*(.*?)(?=\"\n",
    "            rf\"^[ \\t]*[{chr(ord(label)+1)}]\\s*[\\)\\.:]|\"\n",
    "            rf\"^[ \\t]*Correct\\s*Answer\\s*:|\"\n",
    "            rf\"^$|\"\n",
    "            rf\"\\Z)\"\n",
    "        )\n",
    "        m = re.search(pattern, full_text, re.MULTILINE)\n",
    "        if m:\n",
    "            return m.group(1).strip()\n",
    "        return None\n",
    "\n",
    "    parsed['choice_a'] = capture_choice('A', text)\n",
    "    parsed['choice_b'] = capture_choice('B', text)\n",
    "    parsed['choice_c'] = capture_choice('C', text)\n",
    "\n",
    "    ################################################################\n",
    "    # 8) Check for \"Correct Answer:\" or \"Answer:\" or \n",
    "    #    \"The correct answer is:\" lines\n",
    "    ################################################################\n",
    "    ans_match = re.search(\n",
    "        r\"(?i)(?:Correct\\s*Answer\\s*:\\s*|Answer\\s*:\\s*|The\\s*correct\\s*answer\\s*is\\s*)(.*)\",\n",
    "        text\n",
    "    )\n",
    "    if ans_match:\n",
    "        ans_raw = ans_match.group(1).strip()\n",
    "        # If the answer text starts with something like \"C) \" or \"C. \" or \"C:\", strip that off\n",
    "        ans_no_label = re.sub(r'^[ABCabc]\\s*[\\)\\.:]\\s*', '', ans_raw).strip()\n",
    "        parsed['correct_answer'] = ans_no_label\n",
    "\n",
    "    ################################################################\n",
    "    # 9) If STILL no correct_answer => check if any choice ends with \n",
    "    #    parentheses containing \"correct\", e.g. (Correct answer)\n",
    "    ################################################################\n",
    "    if not parsed['correct_answer']:\n",
    "        for label in ['a', 'b', 'c']:\n",
    "            choice_key = f'choice_{label}'\n",
    "            val = parsed[choice_key]\n",
    "            if not val:\n",
    "                continue\n",
    "            paren_match = re.search(r'\\(\\s*(.*?)\\s*\\)\\s*$', val)\n",
    "            if paren_match:\n",
    "                # e.g. \"Un- (Correct answer)\"\n",
    "                paren_text = paren_match.group(1)\n",
    "                if re.search(r'correct', paren_text, re.IGNORECASE):\n",
    "                    # remove that parenthetical from the choice\n",
    "                    val_no_paren = re.sub(r'\\(\\s*.*?\\s*\\)\\s*$', '', val).strip()\n",
    "                    parsed[choice_key] = val_no_paren\n",
    "                    parsed['correct_answer'] = val_no_paren\n",
    "                    break\n",
    "\n",
    "    return parsed\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test_api_access()\n",
    "    #model, tokenizer= train_gemma()\n",
    "    #model_path = \"/blue/babajani.a/babak.ahmadi/NLP_Dorr/Project/MA/last_gemma_morphology_20250404_014910/final_model\"\n",
    "    # common_words = [\n",
    "    # \"the\", \"be\", \"to\", \"of\", \"and\", \"in\", \"that\", \"have\", \"I\", \n",
    "    # \"it\", \"for\", \"not\", \"on\", \"with\", \"he\", \"as\", \"you\", \"do\", \"at\",\n",
    "    # \"this\", \"but\", \"his\", \"by\", \"from\", \"they\", \"we\", \"say\", \"her\", \"she\",\n",
    "    # \"or\", \"an\", \"will\", \"my\", \"one\", \"all\", \"would\", \"there\", \"their\", \"what\",\n",
    "    # \"so\", \"up\", \"out\", \"if\", \"about\", \"who\", \"get\", \"which\", \"go\", \"me\",\n",
    "    # \"when\", \"make\", \"can\", \"like\", \"time\", \"no\", \"just\", \"him\", \"know\", \"take\",\n",
    "    # \"people\", \"into\", \"year\", \"your\", \"good\", \"some\", \"could\", \"them\", \"see\", \"other\",\n",
    "    # \"than\", \"then\", \"now\", \"look\", \"only\", \"come\", \"its\", \"over\", \"think\", \"also\",\n",
    "    # \"back\", \"after\", \"use\", \"two\", \"how\", \"our\", \"work\", \"first\", \"well\", \"way\",\n",
    "    # \"even\", \"new\", \"want\", \"because\", \"any\", \"these\", \"give\", \"day\", \"most\", \"us\",\"none\" ,\"an\",\"like\",\"with\"]\n",
    "    \n",
    "    \n",
    "    data_file = 'MC_data_MA2.csv'\n",
    "    \n",
    "    strategies = [\n",
    "        'chain_of_thought_plus_sequential_rl',\n",
    "        'chain_of_thought_plus_role_chain',\n",
    "        'chain_of_thought',\n",
    "        'chain_of_thought_plus_sequential',\n",
    "        'few_shot',\n",
    "        'zero_shot'\n",
    "    ]\n",
    "\n",
    "    data = pd.read_csv(data_file, encoding='utf-8')\n",
    "    NUM_QUESTIONS = 3\n",
    "    word_difficulties = [1,2,3,4,5] #data[\"Word_Difficulty\"].unique()\n",
    "    task_difficulties = ['E', 'H', 'M']\n",
    "    # Question_Type_array = np.array(data[\"Question_Type\"].unique())\n",
    "    # questio_id = [int(float(x)) for x in Question_Type_array]\n",
    "    questio_id = list(range(1,14))\n",
    "    for strategy in strategies:\n",
    "        questions_data = []\n",
    "        # for question_type in questio_id:\n",
    "        #     if question_type not in [4,5]:\n",
    "        #         prev_prompts = {q_type: [common_words] for q_type in questio_id}\n",
    "        #     else:\n",
    "        prev_prompts = {q_type: [] for q_type in questio_id}\n",
    "\n",
    "        for word_difficulty in word_difficulties:\n",
    "            for task_difficulty in task_difficulties:\n",
    "                for question_type in questio_id:\n",
    "                    for i in range(NUM_QUESTIONS):\n",
    "                        print(f\"\\n=== Question #{i+1} === Question Type #{question_type} === {strategy} ===\")\n",
    "                        prompt_or_final_text, new_word = prompt_generator(\n",
    "                            question_type,\n",
    "                            word_difficulty,\n",
    "                            task_difficulty,\n",
    "                            data,\n",
    "                            strategy,\n",
    "                            prev_prompts[question_type]\n",
    "                        )\n",
    "                        # 2) \n",
    "                        if strategy == \"chain_of_thought_plus_sequential_rl\":\n",
    "                            # The function already did multi-step GPT calls. \n",
    "                            # So use the combined text returned from prompt_generator:\n",
    "                            generated_text = prompt_or_final_text\n",
    "                            parse_text = generated_text\n",
    "                        else:\n",
    "                            # single-step strategy\n",
    "                            generated_text = generate_gpt(prompt_or_final_text)\n",
    "                            parse_text = generated_text\n",
    "                        #parsed = parse_question_output(parse_text or \"\")\n",
    "                        # 3) Parse the relevant text\n",
    "                        if generated_text:\n",
    "                            parsed = parse_question_output(generated_text)\n",
    "                            if parsed:\n",
    "                                parsed['generated_text'] = generated_text\n",
    "                                parsed['question_type'] = question_type\n",
    "                                parsed['word_difficulty'] = word_difficulty\n",
    "                                parsed['task_difficulty'] = task_difficulty\n",
    "                                parsed['prompting_strategy'] = strategy\n",
    "                                questions_data.append(parsed)\n",
    "\n",
    "                        if new_word:\n",
    "                            prev_prompts[question_type].append(new_word)\n",
    "\n",
    "                        if generated_text:\n",
    "                            print(\"\\nGenerated Text from GPT-3.5 Model:\")\n",
    "                            print(generated_text)\n",
    "                        else:\n",
    "                            print(\"No text returned by the model.\")\n",
    "\n",
    "        # Save after finishing one strategy\n",
    "        df = pd.DataFrame(questions_data)\n",
    "        filename = f'Final_generated_questions_GPT_{strategy}.csv'\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"\\nSaved questions for strategy '{strategy}' to {filename}\")    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7091aa12-827c-4fa8-b0d7-e6cad16ef4be",
   "metadata": {},
   "source": [
    "## Models to choose from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e0b8d34-b5f2-4999-b570-cf926f36a782",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access to OpenAI API successful! Available models (sorted by date):\n",
      " - gpt-4.1-nano (created: 2025-04-10 21:48:27)\n",
      " - gpt-4.1-nano-2025-04-14 (created: 2025-04-10 21:37:05)\n",
      " - gpt-4.1-mini (created: 2025-04-10 20:49:33)\n",
      " - gpt-4.1-mini-2025-04-14 (created: 2025-04-10 20:39:07)\n",
      " - gpt-4.1 (created: 2025-04-10 20:22:22)\n",
      " - gpt-4.1-2025-04-14 (created: 2025-04-10 20:09:06)\n",
      " - o4-mini (created: 2025-04-09 19:02:31)\n",
      " - o4-mini-2025-04-16 (created: 2025-04-08 17:31:46)\n",
      " - gpt-4o-mini-tts (created: 2025-03-19 17:05:59)\n",
      " - gpt-4o-mini-transcribe (created: 2025-03-15 19:56:36)\n",
      " - gpt-4o-transcribe (created: 2025-03-15 19:54:23)\n",
      " - gpt-4o-mini-search-preview (created: 2025-03-07 23:46:01)\n",
      " - gpt-4o-mini-search-preview-2025-03-11 (created: 2025-03-07 23:40:58)\n",
      " - gpt-4o-search-preview (created: 2025-03-07 23:05:20)\n",
      " - gpt-4o-search-preview-2025-03-11 (created: 2025-03-07 22:56:10)\n",
      " - gpt-4.5-preview-2025-02-27 (created: 2025-02-27 02:28:24)\n",
      " - gpt-4.5-preview (created: 2025-02-27 02:24:19)\n",
      " - gpt-4o-2024-11-20 (created: 2025-02-12 03:39:03)\n",
      " - o3-mini-2025-01-31 (created: 2025-01-27 20:36:40)\n",
      " - o3-mini (created: 2025-01-17 20:39:43)\n",
      " - gpt-4o-mini-audio-preview (created: 2024-12-16 22:17:04)\n",
      " - gpt-4o-mini-realtime-preview (created: 2024-12-16 22:16:20)\n",
      " - o1 (created: 2024-12-16 19:03:36)\n",
      " - o1-2024-12-17 (created: 2024-12-16 05:29:36)\n",
      " - gpt-4o-mini-audio-preview-2024-12-17 (created: 2024-12-13 18:52:00)\n",
      " - gpt-4o-mini-realtime-preview-2024-12-17 (created: 2024-12-13 17:56:41)\n",
      " - gpt-4o-audio-preview-2024-12-17 (created: 2024-12-12 20:10:39)\n",
      " - gpt-4o-realtime-preview-2024-12-17 (created: 2024-12-11 19:30:30)\n",
      " - omni-moderation-2024-09-26 (created: 2024-11-27 19:07:46)\n",
      " - omni-moderation-latest (created: 2024-11-15 16:47:45)\n",
      " - gpt-4o-realtime-preview (created: 2024-09-30 01:33:18)\n",
      " - gpt-4o-audio-preview (created: 2024-09-27 18:07:23)\n",
      " - gpt-4o-audio-preview-2024-10-01 (created: 2024-09-26 22:17:22)\n",
      " - gpt-4o-realtime-preview-2024-10-01 (created: 2024-09-23 22:49:26)\n",
      " - o1-mini (created: 2024-09-06 18:56:48)\n",
      " - o1-mini-2024-09-12 (created: 2024-09-06 18:56:19)\n",
      " - o1-preview (created: 2024-09-06 18:54:57)\n",
      " - o1-preview-2024-09-12 (created: 2024-09-06 18:54:25)\n",
      " - chatgpt-4o-latest (created: 2024-08-13 02:12:11)\n",
      " - gpt-4o-2024-08-06 (created: 2024-08-04 23:38:39)\n",
      " - gpt-4o-mini (created: 2024-07-16 23:32:21)\n",
      " - gpt-4o-mini-2024-07-18 (created: 2024-07-16 23:31:57)\n",
      " - gpt-4o-2024-05-13 (created: 2024-05-10 19:08:52)\n",
      " - gpt-4o (created: 2024-05-10 18:50:49)\n",
      " - gpt-4-turbo-2024-04-09 (created: 2024-04-08 18:41:17)\n",
      " - gpt-4-turbo (created: 2024-04-05 23:57:21)\n",
      " - gpt-3.5-turbo-0125 (created: 2024-01-23 22:19:18)\n",
      " - gpt-4-turbo-preview (created: 2024-01-23 19:22:57)\n",
      " - gpt-4-0125-preview (created: 2024-01-23 19:20:12)\n",
      " - text-embedding-3-large (created: 2024-01-22 19:53:00)\n",
      " - text-embedding-3-small (created: 2024-01-22 18:43:17)\n",
      " - tts-1-hd-1106 (created: 2023-11-03 23:18:53)\n",
      " - tts-1-1106 (created: 2023-11-03 23:14:01)\n",
      " - tts-1-hd (created: 2023-11-03 21:13:35)\n",
      " - gpt-3.5-turbo-1106 (created: 2023-11-02 21:15:48)\n",
      " - gpt-4-1106-preview (created: 2023-11-02 20:33:26)\n",
      " - dall-e-2 (created: 2023-11-01 00:22:57)\n",
      " - dall-e-3 (created: 2023-10-31 20:46:29)\n",
      " - gpt-3.5-turbo-instruct-0914 (created: 2023-09-07 21:34:32)\n",
      " - gpt-3.5-turbo-instruct (created: 2023-08-24 18:23:47)\n",
      " - babbage-002 (created: 2023-08-21 16:16:55)\n",
      " - davinci-002 (created: 2023-08-21 16:11:41)\n",
      " - gpt-4 (created: 2023-06-27 16:13:31)\n",
      " - gpt-4-0613 (created: 2023-06-12 16:54:56)\n",
      " - gpt-3.5-turbo-16k (created: 2023-05-10 22:35:02)\n",
      " - tts-1 (created: 2023-04-19 21:49:11)\n",
      " - gpt-3.5-turbo (created: 2023-02-28 18:56:42)\n",
      " - whisper-1 (created: 2023-02-27 21:13:04)\n",
      " - text-embedding-ada-002 (created: 2022-12-16 19:01:39)\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from datetime import datetime\n",
    "\n",
    "def test_api_access():\n",
    "    \"\"\"\n",
    "    Attempts to list OpenAI models to confirm that the API key is valid.\n",
    "    Prints a success or failure message, along with a list of available models sorted by creation date.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        models = openai.Model.list()\n",
    "        sorted_models = sorted(\n",
    "            models['data'], key=lambda m: m['created'], reverse=True  # Newest first\n",
    "        )\n",
    "        print(\"Access to OpenAI API successful! Available models (sorted by date):\")\n",
    "        for model in sorted_models:\n",
    "            created_time = datetime.utcfromtimestamp(model['created']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(f\" - {model['id']} (created: {created_time})\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to access the OpenAI API:\")\n",
    "        print(e)\n",
    "\n",
    "test_api_access()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a561834-1bde-40d5-9be3-7b77ea95a3b8",
   "metadata": {},
   "source": [
    "## Parse the human expert evaluation into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f11d3d6-e2f7-49de-a1ac-b6c1d74304f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 63 rows → mcq_human_evals.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parse “Human Evaluation Metrics.docx” and write mcq_human_evals.csv.\n",
    "\n",
    "• One row per annotated sample.\n",
    "• Captures question data, human scores, AND the explanatory\n",
    "  sentences for each of the five evaluation dimensions.\n",
    "\"\"\"\n",
    "\n",
    "import re, csv, sys\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from docx import Document          # pip install python-docx\n",
    "\n",
    "\n",
    "DOCX_FILE = \"Human Evaluation Metrics.docx\"\n",
    "OUT_FILE  = \"mcq_human_evals.csv\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Pull non‑empty lines out of the .docx\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    paragraphs = Document(DOCX_FILE).paragraphs\n",
    "except Exception as e:\n",
    "    sys.exit(f\"Could not open {DOCX_FILE}: {e}\")\n",
    "\n",
    "lines = [p.text.strip() for p in paragraphs if p.text.strip()]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. regex patterns\n",
    "# ---------------------------------------------------------\n",
    "QT_HEADER   = re.compile(r\"=+Question Type (\\d+)=+\")\n",
    "SAMPLE_HDR  = re.compile(r\"Sample #(\\d+) for QT\\d+\", re.I)\n",
    "\n",
    "QUESTION_RE = re.compile(r\"^(?:Question:)?\\s*(.*?\\?)\\s*$\", re.I)\n",
    "CHOICE_RE   = re.compile(r\"^Choice\\s*\\d+\\s*[:)]\\s*(.+)$\", re.I)\n",
    "CORRECT_RE  = re.compile(r\"^Correct[_ ]answer\\s*[:)]\\s*([A-D])\\)\\s*(.+)$\", re.I)\n",
    "WORD_RE     = re.compile(r\"word[_ ]difficulty\\s*[:)]\\s*(\\d+)\", re.I)\n",
    "TASK_RE     = re.compile(r\"task[_ ]difficulty\\s*[:)]\\s*([EMH])\", re.I)\n",
    "\n",
    "EVAL_START  = re.compile(r\"Evaluation for .*?Total Score:\\s*(\\d+)/5\", re.I)\n",
    "METRIC_RE   = re.compile(\n",
    "    r\"(Clarity of Instruction|Accuracy of Correct Answer|Quality of Distractors|\"\n",
    "    r\"Word Difficulty|Task Difficulty)\\s*\\((\\d)\\)\\s*:\\s*(.+)$\",\n",
    "    re.I,\n",
    ")\n",
    "\n",
    "metric_cols = {\n",
    "    \"clarity of instruction\":   (\"eval_instruction_score\",  \"eval_instruction_exp\"),\n",
    "    \"accuracy of correct answer\":(\"eval_accuracy_score\",    \"eval_accuracy_exp\"),\n",
    "    \"quality of distractors\":   (\"eval_distractors_score\",  \"eval_distractors_exp\"),\n",
    "    \"word difficulty\":          (\"eval_word_diff_score\",    \"eval_word_diff_exp\"),\n",
    "    \"task difficulty\":          (\"eval_task_diff_score\",    \"eval_task_diff_exp\"),\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. streaming parse\n",
    "# ---------------------------------------------------------\n",
    "records   = []\n",
    "ctx       = defaultdict(lambda: None)\n",
    "\n",
    "def flush():\n",
    "    \"\"\"push current ctx into records & reset\"\"\"\n",
    "    if ctx.get(\"question\"):\n",
    "        # ensure every metric column exists\n",
    "        for score_col, exp_col in metric_cols.values():\n",
    "            ctx.setdefault(score_col, 0)\n",
    "            ctx.setdefault(exp_col, \"\")\n",
    "        records.append(ctx.copy())\n",
    "        ctx.clear()\n",
    "\n",
    "current_qt = None\n",
    "\n",
    "for ln in lines:\n",
    "    # ---- section headers ----\n",
    "    if m := QT_HEADER.match(ln):\n",
    "        flush()\n",
    "        current_qt = int(m.group(1))\n",
    "        continue\n",
    "    if m := SAMPLE_HDR.match(ln):\n",
    "        flush()\n",
    "        ctx[\"question_type\"] = current_qt\n",
    "        ctx[\"sample_number\"] = int(m.group(1))\n",
    "        continue\n",
    "\n",
    "    # ---- question / choices ----\n",
    "    if m := QUESTION_RE.match(ln):\n",
    "        ctx[\"question\"] = m.group(1).strip()\n",
    "        continue\n",
    "    if m := CHOICE_RE.match(ln):\n",
    "        key = f\"choice_{len([k for k in ctx if k.startswith('choice_')]) + 1}\"\n",
    "        ctx[key] = m.group(1).strip()\n",
    "        continue\n",
    "    if m := CORRECT_RE.match(ln):\n",
    "        ctx[\"correct_answer_letter\"] = m.group(1)\n",
    "        ctx[\"correct_answer_text\"]   = m.group(2).strip()\n",
    "        continue\n",
    "\n",
    "    # ---- misc attributes ----\n",
    "    if m := WORD_RE.search(ln):\n",
    "        ctx[\"word_difficulty\"] = int(m.group(1))\n",
    "    if m := TASK_RE.search(ln):\n",
    "        ctx[\"task_difficulty\"] = m.group(1).upper()\n",
    "\n",
    "    # ---- evaluation block ----\n",
    "    if m := EVAL_START.match(ln):\n",
    "        ctx[\"eval_total_score\"] = int(m.group(1))\n",
    "        # seed blank metric fields\n",
    "        for score_col, exp_col in metric_cols.values():\n",
    "            ctx[score_col] = 0\n",
    "            ctx[exp_col]   = \"\"\n",
    "        continue\n",
    "\n",
    "    # ---- individual metric lines ----\n",
    "    if m := METRIC_RE.match(ln):\n",
    "        label      = m.group(1).lower()\n",
    "        score      = int(m.group(2))\n",
    "        explanation= m.group(3).strip()\n",
    "        score_col, exp_col = metric_cols[label]\n",
    "        ctx[score_col] = score\n",
    "        ctx[exp_col]   = explanation\n",
    "        continue\n",
    "\n",
    "# final sample\n",
    "flush()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. normalise choice columns\n",
    "# ---------------------------------------------------------\n",
    "max_choices = max(int(k.split(\"_\")[1])\n",
    "                  for r in records for k in r if k.startswith(\"choice_\"))\n",
    "for r in records:\n",
    "    for i in range(1, max_choices + 1):\n",
    "        r.setdefault(f\"choice_{i}\", \"\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. write csv\n",
    "# ---------------------------------------------------------\n",
    "base_cols = [\"question_type\", \"sample_number\", \"question\"] + \\\n",
    "            [f\"choice_{i}\" for i in range(1, max_choices + 1)] + \\\n",
    "            [\"correct_answer_letter\", \"correct_answer_text\",\n",
    "             \"word_difficulty\", \"task_difficulty\", \"eval_total_score\"]\n",
    "\n",
    "metric_cols_flat = []\n",
    "for score_col, exp_col in metric_cols.values():\n",
    "    metric_cols_flat.extend([score_col, exp_col])\n",
    "\n",
    "fieldnames = base_cols + metric_cols_flat\n",
    "\n",
    "with open(OUT_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    csv.DictWriter(f, fieldnames=fieldnames).writeheader()\n",
    "    csv.DictWriter(f, fieldnames=fieldnames).writerows(records)\n",
    "\n",
    "print(f\"Wrote {len(records)} rows → {OUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985bc2a6-32ff-4980-9875-a8b221ae17f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation pipeline:  GPT‑4.5 rates GPT‑3.5‑generated MCQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69835f2c-1aed-4f58-88af-0ab38af0fb72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [03:05<00:00,  9.76s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gpt45_evaluations.csv'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# ---------- file paths ----------\n",
    "HUMAN_FILE = \"mcq_human_evals.csv\"\n",
    "ITEM_FILE  = \"Final_generated_questions_GPT_chain_of_thought_plus_sequential_rl_First20.csv\"\n",
    "OUT_FILE   = \"gpt45_evaluations.csv\"\n",
    "\n",
    "MODEL       = \"gpt-4.5-preview-2025-02-27\"\n",
    "TEMPERATURE = 0.0\n",
    "# DELAY_SEC   = 0.4           # polite pacing; tweak if you like\n",
    "\n",
    "# load_dotenv()\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# assert openai.api_key, \"Set OPENAI_API_KEY env‑var or .env!\"\n",
    "\n",
    "\n",
    "# load CSVs\n",
    "items_df = pd.read_csv(ITEM_FILE,  dtype=str)\n",
    "human_df = pd.read_csv(HUMAN_FILE, dtype=str)\n",
    "\n",
    "human_df[\"question_type\"] = human_df[\"question_type\"].astype(int)\n",
    "items_df[\"question_type\"] = items_df[\"question_type\"].astype(int)\n",
    "\n",
    "\n",
    "# %% Cell 2b — formatting helpers\n",
    "RUBRIC_TEXT = \"\"\"\n",
    "Rate the item on **five binary metrics**.  \n",
    "Give 1 = meets criterion, 0 = does not.\n",
    "\n",
    "1. Instruction Clarity  \n",
    "2. Accuracy of Correct Answer  \n",
    "3. Quality of Distractors  \n",
    "4. Word Difficulty Appropriateness  \n",
    "5. Task Difficulty Alignment  \n",
    "\n",
    "Respond **only** with JSON:\n",
    "\n",
    "{\n",
    "  \"instr_score\": 0/1, \"instr_exp\": \"...\",\n",
    "  \"acc_score\":   0/1, \"acc_exp\": \"...\",\n",
    "  \"dist_score\":  0/1, \"dist_exp\": \"...\",\n",
    "  \"word_score\":  0/1, \"word_exp\": \"...\",\n",
    "  \"task_score\":  0/1, \"task_exp\": \"...\",\n",
    "  \"total_score\": 0‑5\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def fmt_human_example(row) -> str:\n",
    "    choices = \"\\n\".join(\n",
    "        [f\"{chr(64+i)}) {row[f'choice_{i}']}\"\n",
    "         for i in range(1,5) if str(row.get(f\"choice_{i}\", \"\")).strip()]\n",
    "    )\n",
    "    lines = [\n",
    "        \"### Human‑rated Example\",\n",
    "        f\"Question: {row['question']}\",\n",
    "        choices,\n",
    "        f\"Correct Answer: {row['correct_answer_text']}\",\n",
    "        f\"word_difficulty={row['word_difficulty']}  task_difficulty={row['task_difficulty']}\",\n",
    "        \"Scores:\",\n",
    "        f\"- Instruction Clarity: {row['eval_instruction_score']} ({row['eval_instruction_exp']})\",\n",
    "        f\"- Accuracy of Correct Answer: {row['eval_accuracy_score']} ({row['eval_accuracy_exp']})\",\n",
    "        f\"- Quality of Distractors: {row['eval_distractors_score']} ({row['eval_distractors_exp']})\",\n",
    "        f\"- Word Difficulty Appropriateness: {row['eval_word_diff_score']} ({row['eval_word_diff_exp']})\",\n",
    "        f\"- Task Difficulty Alignment: {row['eval_task_diff_score']} ({row['eval_task_diff_exp']})\",\n",
    "        \"### End Example\\n\"\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def fmt_item(row) -> str:\n",
    "    choices = \"\\n\".join([f\"A) {row['choice_a']}\",\n",
    "                         f\"B) {row['choice_b']}\",\n",
    "                         f\"C) {row['choice_c']}\"])\n",
    "    return (\n",
    "        \"### Item to evaluate\\n\"\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"{choices}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']}  \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\"\n",
    "    )\n",
    "\n",
    "def make_prompt(qtype: int, item_row) -> list:\n",
    "    ex_block = \"\\n\".join(\n",
    "        fmt_human_example(r) for _, r\n",
    "        in human_df.query(\"question_type == @qtype\").iterrows()\n",
    "    )\n",
    "    user = (\n",
    "        f\"{ex_block}\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "        f\"{fmt_item(item_row)}\\n\"\n",
    "        f\"{RUBRIC_TEXT}\"\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a meticulous K‑12 morphology test reviewer.\"},\n",
    "        {\"role\": \"user\", \"content\": user}\n",
    "    ]\n",
    "\n",
    "# %% evaluate\n",
    "results = []\n",
    "\n",
    "for idx, item in tqdm(items_df.iterrows(), total=len(items_df)):\n",
    "    prompt_msgs = make_prompt(item.question_type, item)\n",
    "\n",
    "    try:\n",
    "        resp = openai.ChatCompletion.create(\n",
    "            model=MODEL,\n",
    "            temperature=TEMPERATURE,\n",
    "            messages=prompt_msgs\n",
    "        )\n",
    "        content = resp.choices[0].message[\"content\"].strip()\n",
    "        scores  = json.loads(content)          # fail here if JSON bad\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] row {idx} failed: {e}\")\n",
    "        scores = {k: None for k in [\n",
    "            \"instr_score\",\"instr_exp\",\"acc_score\",\"acc_exp\",\n",
    "            \"dist_score\",\"dist_exp\",\"word_score\",\"word_exp\",\n",
    "            \"task_score\",\"task_exp\",\"total_score\"\n",
    "        ]}\n",
    "    merged = item.to_dict()\n",
    "    merged.update(scores)\n",
    "    results.append(merged)\n",
    "    time.sleep(DELAY_SEC)\n",
    "\n",
    "eval_df = pd.DataFrame(results)\n",
    "eval_df.to_csv(OUT_FILE, index=False)\n",
    "OUT_FILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b16858-40b6-4748-871f-56e8765f3e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbcff56d-98b7-4de1-8b8f-5cf3a17ac4bb",
   "metadata": {},
   "source": [
    "### faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b726bcfb-d252-4847-afc9-64a56cd07ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202d97eb2b80448baf59fd163cefc8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ saved gpt45_evaluations_fast.csv\n"
     ]
    }
   ],
   "source": [
    "import os, json, asyncio\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import openai\n",
    "\n",
    "# ---------- file paths ----------\n",
    "HUMAN_FILE = \"mcq_human_evals.csv\"\n",
    "ITEM_FILE  = \"Final_generated_questions_GPT_chain_of_thought_plus_sequential_rl_First20.csv\"\n",
    "OUT_FILE   = \"gpt45_evaluations_fast.csv\"\n",
    "\n",
    "MODEL       = \"gpt-4.5-preview-2025-02-27\"\n",
    "TEMPERATURE = 0.0\n",
    "CONCURRENT  = 10          # parallel requests—adjust for your quota\n",
    "\n",
    "\n",
    "# %% Cell 2\n",
    "items_df = pd.read_csv(ITEM_FILE,  dtype=str)\n",
    "human_df = pd.read_csv(HUMAN_FILE, dtype=str)\n",
    "\n",
    "human_df[\"question_type\"] = human_df[\"question_type\"].astype(int)\n",
    "items_df[\"question_type\"] = items_df[\"question_type\"].astype(int)\n",
    "\n",
    "# ---------- helper to pretty‑print a single human example ----------\n",
    "def fmt_human_example(row) -> str:\n",
    "    choices = \"\\n\".join(\n",
    "        f\"{chr(64+i)}) {row[f'choice_{i}']}\"\n",
    "        for i in range(1,5)\n",
    "        if str(row.get(f\"choice_{i}\",\"\")).strip()\n",
    "    )\n",
    "    return (\n",
    "        \"### Human‑rated Example\\n\"\n",
    "        f\"Question: {row['question']}\\n{choices}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer_text']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\"\n",
    "        \"Scores:\\n\"\n",
    "        f\"- Instruction Clarity: {row['eval_instruction_score']} ({row['eval_instruction_exp']})\\n\"\n",
    "        f\"- Accuracy of Correct Answer: {row['eval_accuracy_score']} ({row['eval_accuracy_exp']})\\n\"\n",
    "        f\"- Quality of Distractors: {row['eval_distractors_score']} ({row['eval_distractors_exp']})\\n\"\n",
    "        f\"- Word Difficulty Appropriateness: {row['eval_word_diff_score']} ({row['eval_word_diff_exp']})\\n\"\n",
    "        f\"- Task Difficulty Alignment: {row['eval_task_diff_score']} ({row['eval_task_diff_exp']})\\n\"\n",
    "        \"### End Example\\n\"\n",
    "    )\n",
    "\n",
    "# ---------- build example block once per question_type ----------\n",
    "EXAMPLE_BLOCKS = {\n",
    "    qtype: \"\\n\".join(fmt_human_example(r) for _, r in grp.iterrows())\n",
    "    for qtype, grp in human_df.groupby(\"question_type\")\n",
    "}\n",
    "\n",
    "RUBRIC = \"\"\"\n",
    "Rate on five binary metrics (1 = meets criterion, 0 = does not) and reply **JSON only**:\n",
    "\n",
    "{\n",
    " \"instr_score\":0/1, \"instr_exp\":\"...\",\n",
    " \"acc_score\":0/1,   \"acc_exp\":\"...\",\n",
    " \"dist_score\":0/1,  \"dist_exp\":\"...\",\n",
    " \"word_score\":0/1,  \"word_exp\":\"...\",\n",
    " \"task_score\":0/1,  \"task_exp\":\"...\",\n",
    " \"total_score\":0-5\n",
    "}\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "sem = asyncio.Semaphore(CONCURRENT)   # throttle parallelism\n",
    "\n",
    "async def rate_item(idx, row):\n",
    "    user_prompt = (\n",
    "        f\"{EXAMPLE_BLOCKS[row['question_type']]}\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "        \"### Item to evaluate\\n\"\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"A) {row['choice_a']}\\n\"\n",
    "        f\"B) {row['choice_b']}\\n\"\n",
    "        f\"C) {row['choice_c']}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\\n\"\n",
    "        f\"{RUBRIC}\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a meticulous K‑12 morphology test reviewer.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    async with sem:                           # limit concurrent calls\n",
    "        try:\n",
    "            rsp = await openai.ChatCompletion.acreate(\n",
    "                model=MODEL,\n",
    "                temperature=TEMPERATURE,\n",
    "                messages=messages\n",
    "            )\n",
    "            data = json.loads(rsp.choices[0].message[\"content\"])\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] row {idx} failed: {e}\")\n",
    "            data = {k: None for k in\n",
    "                    [\"instr_score\",\"instr_exp\",\"acc_score\",\"acc_exp\",\n",
    "                     \"dist_score\",\"dist_exp\",\"word_score\",\"word_exp\",\n",
    "                     \"task_score\",\"task_exp\",\"total_score\"]}\n",
    "\n",
    "    merged = row.to_dict()\n",
    "    merged.update(data)\n",
    "    return merged\n",
    "\n",
    "\n",
    "async def main():\n",
    "    tasks   = [rate_item(i, r) for i, r in items_df.iterrows()]\n",
    "    results = [await t for t in tqdm(asyncio.as_completed(tasks),\n",
    "                                     total=len(tasks))]\n",
    "    pd.DataFrame(results).to_csv(OUT_FILE, index=False)\n",
    "    print(\"✓ saved\", OUT_FILE)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f348fd-2bc1-4560-9ea0-a73daecaf9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a759319-9e31-4e9d-9be3-75b531c37950",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a908634e8e3c4d5896b6d245f4919d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] row 7 failed: Expecting value: line 1 column 1 (char 0)\n",
      "✓ saved gpt45_evaluations_fast_o4_mini.csv\n"
     ]
    }
   ],
   "source": [
    "import os, json, asyncio\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import openai\n",
    "\n",
    "# ---------- file paths ----------\n",
    "HUMAN_FILE = \"mcq_human_evals.csv\"\n",
    "ITEM_FILE  = \"Final_generated_questions_GPT_chain_of_thought_plus_sequential_rl_First20.csv\"\n",
    "OUT_FILE   = \"gpt45_evaluations_fast_o4_mini.csv\"\n",
    "\n",
    "MODEL       = \"o4-mini\" # \"gpt-4.1-nano\" # \"gpt-4.5-preview-2025-02-27\"\n",
    "TEMPERATURE = 1.0\n",
    "CONCURRENT  = 10          # parallel requests—adjust for your quota\n",
    "\n",
    "\n",
    "# %% Cell 2\n",
    "items_df = pd.read_csv(ITEM_FILE,  dtype=str)\n",
    "human_df = pd.read_csv(HUMAN_FILE, dtype=str)\n",
    "\n",
    "human_df[\"question_type\"] = human_df[\"question_type\"].astype(int)\n",
    "items_df[\"question_type\"] = items_df[\"question_type\"].astype(int)\n",
    "\n",
    "# ---------- helper to pretty‑print a single human example ----------\n",
    "def fmt_human_example(row) -> str:\n",
    "    choices = \"\\n\".join(\n",
    "        f\"{chr(64+i)}) {row[f'choice_{i}']}\"\n",
    "        for i in range(1,5)\n",
    "        if str(row.get(f\"choice_{i}\",\"\")).strip()\n",
    "    )\n",
    "    return (\n",
    "        \"### Human‑rated Example\\n\"\n",
    "        f\"Question: {row['question']}\\n{choices}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer_text']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\"\n",
    "        \"Scores:\\n\"\n",
    "        f\"- Instruction Clarity: {row['eval_instruction_score']} ({row['eval_instruction_exp']})\\n\"\n",
    "        f\"- Accuracy of Correct Answer: {row['eval_accuracy_score']} ({row['eval_accuracy_exp']})\\n\"\n",
    "        f\"- Quality of Distractors: {row['eval_distractors_score']} ({row['eval_distractors_exp']})\\n\"\n",
    "        f\"- Word Difficulty Appropriateness: {row['eval_word_diff_score']} ({row['eval_word_diff_exp']})\\n\"\n",
    "        f\"- Task Difficulty Alignment: {row['eval_task_diff_score']} ({row['eval_task_diff_exp']})\\n\"\n",
    "        \"### End Example\\n\"\n",
    "    )\n",
    "\n",
    "# ---------- build example block once per question_type ----------\n",
    "EXAMPLE_BLOCKS = {\n",
    "    qtype: \"\\n\".join(fmt_human_example(r) for _, r in grp.iterrows())\n",
    "    for qtype, grp in human_df.groupby(\"question_type\")\n",
    "}\n",
    "\n",
    "RUBRIC = \"\"\"\n",
    "Rate on five binary metrics (1 = meets criterion, 0 = does not) and reply **JSON only**:\n",
    "\n",
    "{\n",
    " \"instr_score\":0/1, \"instr_exp\":\"...\",\n",
    " \"acc_score\":0/1,   \"acc_exp\":\"...\",\n",
    " \"dist_score\":0/1,  \"dist_exp\":\"...\",\n",
    " \"word_score\":0/1,  \"word_exp\":\"...\",\n",
    " \"task_score\":0/1,  \"task_exp\":\"...\",\n",
    " \"total_score\":0-5\n",
    "}\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "sem = asyncio.Semaphore(CONCURRENT)   # throttle parallelism\n",
    "\n",
    "async def rate_item(idx, row):\n",
    "    user_prompt = (\n",
    "        f\"{EXAMPLE_BLOCKS[row['question_type']]}\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "        \"### Item to evaluate\\n\"\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"A) {row['choice_a']}\\n\"\n",
    "        f\"B) {row['choice_b']}\\n\"\n",
    "        f\"C) {row['choice_c']}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\\n\"\n",
    "        f\"{RUBRIC}\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a meticulous K‑12 morphology test reviewer.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    async with sem:                           # limit concurrent calls\n",
    "        try:\n",
    "            rsp = await openai.ChatCompletion.acreate(\n",
    "                model=MODEL,\n",
    "                temperature=TEMPERATURE,\n",
    "                messages=messages\n",
    "            )\n",
    "            data = json.loads(rsp.choices[0].message[\"content\"])\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] row {idx} failed: {e}\")\n",
    "            data = {k: None for k in\n",
    "                    [\"instr_score\",\"instr_exp\",\"acc_score\",\"acc_exp\",\n",
    "                     \"dist_score\",\"dist_exp\",\"word_score\",\"word_exp\",\n",
    "                     \"task_score\",\"task_exp\",\"total_score\"]}\n",
    "\n",
    "    merged = row.to_dict()\n",
    "    merged.update(data)\n",
    "    return merged\n",
    "\n",
    "\n",
    "async def main():\n",
    "    tasks   = [rate_item(i, r) for i, r in items_df.iterrows()]\n",
    "    results = [await t for t in tqdm(asyncio.as_completed(tasks),\n",
    "                                     total=len(tasks))]\n",
    "    pd.DataFrame(results).to_csv(OUT_FILE, index=False)\n",
    "    print(\"✓ saved\", OUT_FILE)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b181f-8432-4d12-985c-1041660b5588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7e65124-b460-486b-93df-9315da16ec42",
   "metadata": {},
   "source": [
    "## concatenate and sort the generated question by GPT 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "deb2bfa7-3543-4e63-81dd-676e43bb8846",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → Final_generated_questions_GPT35_merged_sorted.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1.  read the two source files\n",
    "# -------------------------------------------------\n",
    "df1 = pd.read_csv(\"generated_questions_output.csv\")\n",
    "df2 = pd.read_csv(\"generated_questions_output_2_4.csv\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2.  concatenate rows\n",
    "# -------------------------------------------------\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3.  make task‑difficulty and prompting‑strategy ordered\n",
    "# -------------------------------------------------\n",
    "task_order = ['E', 'M', 'H']\n",
    "strategy_order = [\n",
    "    'zero_shot',\n",
    "    'few_shot',\n",
    "    'chain_of_thought',\n",
    "    'chain_of_thought_plus_role_chain',\n",
    "    'chain_of_thought_plus_sequential',\n",
    "    'chain_of_thought_plus_sequential_rl'\n",
    "]\n",
    "\n",
    "df['task_difficulty']      = pd.Categorical(df['task_difficulty'],\n",
    "                                            categories=task_order,\n",
    "                                            ordered=True)\n",
    "df['prompting_strategy']   = pd.Categorical(df['prompting_strategy'],\n",
    "                                            categories=strategy_order,\n",
    "                                            ordered=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4.  sort \n",
    "# -------------------------------------------------\n",
    "df_sorted = df.sort_values(\n",
    "    ['question_type', 'word_difficulty',\n",
    "     'task_difficulty', 'prompting_strategy']\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5.  write the merged, sorted file\n",
    "# -------------------------------------------------\n",
    "output_path = \"Final_generated_questions_GPT35_merged_sorted.csv\"\n",
    "df_sorted.to_csv(output_path, index=False)\n",
    "print(f\"Saved → {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7fb704-f27b-4f38-bd47-b2a4b17cbcee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a84fc04a-6366-4f2d-ac28-eccf2a9455a8",
   "metadata": {},
   "source": [
    "# Final Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7db0e431-8cfa-4582-904e-7837222fefdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 67\u001b[0m\n\u001b[1;32m     47\u001b[0m     choices \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m64\u001b[39m\u001b[38;5;241m+\u001b[39mi)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoice_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(row\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoice_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### Human‑rated Example\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mchoices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### End Example\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m     )\n\u001b[0;32m---> 67\u001b[0m EXAMPLE_BLOCKS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     68\u001b[0m     q: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(fmt_human_example(r) \u001b[38;5;28;01mfor\u001b[39;00m _, r \u001b[38;5;129;01min\u001b[39;00m g\u001b[38;5;241m.\u001b[39miterrows())\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m q, g \u001b[38;5;129;01min\u001b[39;00m human_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m }\n\u001b[1;32m     72\u001b[0m RUBRIC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124mRate on five binary metrics (1 = meets, 0 = does not) and reply JSON only:\u001b[39m\n\u001b[1;32m     74\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124m}\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# ════════════════════════════════════════════════════════════════════\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# 5. CSV STREAMING UTIL ──────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# ════════════════════════════════════════════════════════════════════\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[55], line 68\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     choices \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m64\u001b[39m\u001b[38;5;241m+\u001b[39mi)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoice_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(row\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoice_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### Human‑rated Example\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mchoices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### End Example\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m     )\n\u001b[1;32m     67\u001b[0m EXAMPLE_BLOCKS \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 68\u001b[0m     q: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(fmt_human_example(r) \u001b[38;5;28;01mfor\u001b[39;00m _, r \u001b[38;5;129;01min\u001b[39;00m g\u001b[38;5;241m.\u001b[39miterrows())\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m q, g \u001b[38;5;129;01min\u001b[39;00m human_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m }\n\u001b[1;32m     72\u001b[0m RUBRIC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124mRate on five binary metrics (1 = meets, 0 = does not) and reply JSON only:\u001b[39m\n\u001b[1;32m     74\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124m}\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# ════════════════════════════════════════════════════════════════════\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# 5. CSV STREAMING UTIL ──────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# ════════════════════════════════════════════════════════════════════\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[55], line 68\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     choices \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m64\u001b[39m\u001b[38;5;241m+\u001b[39mi)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoice_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(row\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoice_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### Human‑rated Example\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mchoices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### End Example\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m     )\n\u001b[1;32m     67\u001b[0m EXAMPLE_BLOCKS \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 68\u001b[0m     q: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(fmt_human_example(r) \u001b[38;5;28;01mfor\u001b[39;00m _, r \u001b[38;5;129;01min\u001b[39;00m g\u001b[38;5;241m.\u001b[39miterrows())\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m q, g \u001b[38;5;129;01min\u001b[39;00m human_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m }\n\u001b[1;32m     72\u001b[0m RUBRIC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124mRate on five binary metrics (1 = meets, 0 = does not) and reply JSON only:\u001b[39m\n\u001b[1;32m     74\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124m}\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# ════════════════════════════════════════════════════════════════════\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# 5. CSV STREAMING UTIL ──────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# ════════════════════════════════════════════════════════════════════\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[55], line 47\u001b[0m, in \u001b[0;36mfmt_human_example\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfmt_human_example\u001b[39m(row) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 47\u001b[0m     choices \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m64\u001b[39m\u001b[38;5;241m+\u001b[39mi)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoice_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(row\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoice_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### Human‑rated Example\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mchoices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### End Example\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[55], line 48\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfmt_human_example\u001b[39m(row) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     47\u001b[0m     choices \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m64\u001b[39m\u001b[38;5;241m+\u001b[39mi)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoice_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(row\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoice_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### Human‑rated Example\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mchoices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### End Example\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "# %% FILTERS I care about\n",
    "#\n",
    "# Leave a key absent (or give an empty list) to keep *all* rows for that column.\n",
    "#\n",
    "# FILTERS = {\n",
    "#     \"prompting_strategy\": [\"chain_of_thought_plus_role_chain\"],   # ← example\n",
    "#     \"question_type\":      [2],                               # any subset\n",
    "#     \"word_difficulty\":    [\"1\"],                              # strings → df is dtype=str\n",
    "#     \"task_difficulty\":    [\"E\"],                                   # 'E' 'M' 'H'\n",
    "# }\n",
    "FILTERS = {}\n",
    "\n",
    "\n",
    "import os, json, asyncio, pandas as pd, openai\n",
    "from tqdm.notebook import tqdm\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "HUMAN_FILE = \"mcq_human_evals.csv\"\n",
    "ITEM_FILE  = \"Final_generated_questions_GPT35_merged_sorted.csv\"   # <‑‑ the new merged file\n",
    "OUT_FILE   = \"gpto4mini_evaluations.csv\"\n",
    "\n",
    "MODEL       = \"o4-mini\"\n",
    "TEMPERATURE = 1.0\n",
    "CONCURRENT  = 10          # parallel requests\n",
    "\n",
    "# load_dotenv()\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ---- load data ----\n",
    "items_df = pd.read_csv(ITEM_FILE,  dtype=str)\n",
    "human_df = pd.read_csv(HUMAN_FILE, dtype=str)\n",
    "\n",
    "human_df[\"question_type\"] = human_df[\"question_type\"].astype(int)\n",
    "items_df[\"question_type\"] = items_df[\"question_type\"].astype(int)\n",
    "\n",
    "# ---- APPLY FILTERS -----------------------------------------\n",
    "for col, allowed in FILTERS.items():\n",
    "    if allowed:                      # ignore empty lists / None\n",
    "        items_df = items_df[items_df[col].isin([str(x) for x in allowed])]\n",
    "\n",
    "# print(\"Items after filtering:\", len(items_df))\n",
    "items_df = items_df.head(100)      # ← keep just the first 100 items\n",
    "\n",
    "\n",
    "# helpers & cached example blocks\n",
    "def fmt_human_example(row) -> str:\n",
    "    choices = \"\\n\".join(\n",
    "        f\"{chr(64+i)}) {row.get(f'choice_{i}','').strip()}\"\n",
    "        for i in range(1,5)\n",
    "        if str(row.get(f\"choice_{i}\",\"\")).strip()\n",
    "    )\n",
    "    return (\n",
    "        \"### Human‑rated Example\\n\"\n",
    "        f\"Question: {row['question']}\\n{choices}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer_text']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\"\n",
    "        \"Scores:\\n\"\n",
    "        f\"- Instruction Clarity: {row['eval_instruction_score']} ({row['eval_instruction_exp']})\\n\"\n",
    "        f\"- Accuracy of Correct Answer: {row['eval_accuracy_score']} ({row['eval_accuracy_exp']})\\n\"\n",
    "        f\"- Quality of Distractors: {row['eval_distractors_score']} ({row['eval_distractors_exp']})\\n\"\n",
    "        f\"- Word Difficulty Appropriateness: {row['eval_word_diff_score']} ({row['eval_word_diff_exp']})\\n\"\n",
    "        f\"- Task Difficulty Alignment: {row['eval_task_diff_score']} ({row['eval_task_diff_exp']})\\n\"\n",
    "        \"### End Example\\n\"\n",
    "    )\n",
    "\n",
    "EXAMPLE_BLOCKS = {\n",
    "    q: \"\\n\".join(fmt_human_example(r) for _, r in g.iterrows())\n",
    "    for q, g in human_df.groupby(\"question_type\")\n",
    "}\n",
    "\n",
    "RUBRIC = \"\"\"\n",
    "Rate on five binary metrics (1 = meets, 0 = does not) and reply JSON only:\n",
    "\n",
    "{\n",
    " \"instr_score\":0/1, \"instr_exp\":\"...\",\n",
    " \"acc_score\":0/1,   \"acc_exp\":\"...\",\n",
    " \"dist_score\":0/1,  \"dist_exp\":\"...\",\n",
    " \"word_score\":0/1,  \"word_exp\":\"...\",\n",
    " \"task_score\":0/1,  \"task_exp\":\"...\",\n",
    " \"total_score\":0-5\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "# 5. CSV STREAMING UTIL ──────────────────────────────────────────────\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "def append_row(row_dict: dict, path: str):\n",
    "    \"\"\"Append one dict row; create file + header if missing.\"\"\"\n",
    "    new_file = not os.path.exists(path)\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as fp:\n",
    "        w = csv.DictWriter(fp, fieldnames=row_dict.keys())\n",
    "        if new_file:\n",
    "            w.writeheader()\n",
    "        w.writerow(row_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "# 6. MODEL CALL ──────────────────────────────────────────────────────\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "sem = asyncio.Semaphore(CONCURRENT)\n",
    "\n",
    "async def rate_item(idx: int, row):\n",
    "    prompt = (\n",
    "        f\"{EXAMPLE_BLOCKS[row['question_type']]}\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "        \"### Item to evaluate\\n\"\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"A) {row['choice_a']}\\n\"\n",
    "        f\"B) {row['choice_b']}\\n\"\n",
    "        f\"C) {row['choice_c']}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\\n\"\n",
    "        f\"{RUBRIC}\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\n",
    "         \"content\":\"You are a meticulous K‑12 morphology test reviewer.\"},\n",
    "        {\"role\":\"user\",   \"content\":prompt}\n",
    "    ]\n",
    "\n",
    "    async with sem:\n",
    "        try:\n",
    "            rsp  = await openai.ChatCompletion.acreate(\n",
    "                model=MODEL, temperature=TEMPERATURE, messages=messages\n",
    "            )\n",
    "            data = json.loads(rsp.choices[0].message[\"content\"])\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] item {idx} failed → {e}\")\n",
    "            data = {k: None for k in\n",
    "                    [\"instr_score\",\"instr_exp\",\"acc_score\",\"acc_exp\",\n",
    "                     \"dist_score\",\"dist_exp\",\"word_score\",\"word_exp\",\n",
    "                     \"task_score\",\"task_exp\",\"total_score\"]}\n",
    "\n",
    "    merged = row.to_dict()\n",
    "    merged.update(data)\n",
    "    append_row(merged, OUT_FILE)          # ✨ stream to disk now\n",
    "    return merged\n",
    "\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "# 7. ORCHESTRATION ───────────────────────────────────────────────────\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "async def main():\n",
    "    # kick off all tasks\n",
    "    tasks = [asyncio.create_task(rate_item(i, r))\n",
    "             for i, r in items_df.iterrows()]\n",
    "\n",
    "    # progress bar as they finish (order is non‑deterministic)\n",
    "    for fut in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "        await fut\n",
    "\n",
    "    print(f\"✓ completed – results streamed to {OUT_FILE}\")\n",
    "\n",
    "# ── run\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92db0098-8be8-4e9e-86bf-3662f3a309c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fa04f1c-815d-409c-a7ba-69679668a46d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL NaN cells: 21712\n",
      "\n",
      "NaNs per column:\n",
      "psychometrician_reasoning    3509\n",
      "teacher_reasoning            3003\n",
      "student_reasoning            2994\n",
      "step_2                       2927\n",
      "step_3                       2926\n",
      "step_1                       2926\n",
      "chain_of_thought             2819\n",
      "generated_text                586\n",
      "choice_c                        6\n",
      "choice_b                        6\n",
      "choice_a                        5\n",
      "correct_answer                  4\n",
      "question                        1\n",
      "word_difficulty                 0\n",
      "prompting_strategy              0\n",
      "task_difficulty                 0\n",
      "question_type                   0\n",
      "dtype: int64\n",
      "\n",
      "Rows containing at least one NaN: 3510\n",
      "\n",
      "NaNs in choice columns only:\n",
      "choice_a    5\n",
      "choice_b    6\n",
      "choice_c    6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = \"Final_generated_questions_GPT35_merged_sorted.csv\"\n",
    "\n",
    "df = pd.read_csv(FILE, dtype=str)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1)  overall check: does the whole DataFrame contain NaNs?\n",
    "# -----------------------------------------------------------\n",
    "total_missing = df.isna().sum().sum()\n",
    "print(f\"TOTAL NaN cells: {total_missing}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2)  count NaNs per column\n",
    "# -----------------------------------------------------------\n",
    "print(\"\\nNaNs per column:\")\n",
    "print(df.isna().sum().sort_values(ascending=False))\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3)  inspect rows that have *any* NaN\n",
    "# -----------------------------------------------------------\n",
    "rows_with_nan = df[df.isna().any(axis=1)]\n",
    "print(f\"\\nRows containing at least one NaN: {len(rows_with_nan)}\")\n",
    "\n",
    "# ▶ if you want to see them:\n",
    "# print(rows_with_nan)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4)  focus on the choice columns (if that’s where the crash happened)\n",
    "# -----------------------------------------------------------\n",
    "choice_cols = [c for c in df.columns if c.lower().startswith(\"choice_\")]\n",
    "print(\"\\nNaNs in choice columns only:\")\n",
    "print(df[choice_cols].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f39ae401-3912-4f73-ba51-55467f6f5d84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 1513  →  missing: correct_answer\n",
      "row 2170  →  missing: choice_c, correct_answer\n",
      "row 2194  →  missing: choice_b\n",
      "row 2570  →  missing: choice_a, choice_b, choice_c\n",
      "row 2714  →  missing: question, choice_a, choice_b, choice_c, correct_answer\n",
      "row 2783  →  missing: correct_answer\n",
      "row 3073  →  missing: choice_a, choice_b, choice_c\n",
      "row 3252  →  missing: choice_a, choice_b, choice_c\n",
      "row 3416  →  missing: choice_a, choice_b, choice_c\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = \"Final_generated_questions_GPT35_merged_sorted.csv\"\n",
    "df   = pd.read_csv(FILE, dtype=str)\n",
    "\n",
    "# ­­­­­­­­­­­ columns you care about ­­­­­­­­­­\n",
    "focus_cols = [\n",
    "    \"question\",\n",
    "    \"choice_a\",\n",
    "    \"choice_b\",\n",
    "    \"choice_c\",\n",
    "    \"correct_answer\",\n",
    "    \"word_difficulty\",\n",
    "    \"task_difficulty\",\n",
    "    \"prompting_strategy\",\n",
    "    \"question_type\",\n",
    "]\n",
    "\n",
    "# 1) mask: True where any of the focus columns is NaN\n",
    "mask = df[focus_cols].isna().any(axis=1)\n",
    "\n",
    "# 2) rows that have a NaN in those columns\n",
    "bad_rows = df[mask]\n",
    "\n",
    "# 3) show the row indices and which column(s) are missing\n",
    "for idx, row in bad_rows.iterrows():\n",
    "    missing_cols = [c for c in focus_cols if pd.isna(row[c])]\n",
    "    print(f\"row {idx}  →  missing: {', '.join(missing_cols)}\")\n",
    "\n",
    "# 4) (optional) if you want to look at the full rows:\n",
    "# print(bad_rows[focus_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79429959-74a7-478e-a968-37b612115b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kept 3501 of 3510 rows\n",
      "✓ saved cleaned file → Final_generated_questions_GPT35_clean.csv\n"
     ]
    }
   ],
   "source": [
    "FILE_IN  = \"Final_generated_questions_GPT35_merged_sorted.csv\"\n",
    "FILE_OUT = \"Final_generated_questions_GPT35_clean.csv\"\n",
    "\n",
    "# load\n",
    "df = pd.read_csv(FILE_IN, dtype=str)\n",
    "\n",
    "# columns that must be complete\n",
    "req_cols = [\n",
    "    \"question\",\n",
    "    \"choice_a\",\n",
    "    \"choice_b\",\n",
    "    \"choice_c\",\n",
    "    \"correct_answer\",\n",
    "    \"word_difficulty\",\n",
    "    \"task_difficulty\",\n",
    "    \"prompting_strategy\",\n",
    "    \"question_type\",\n",
    "]\n",
    "\n",
    "# drop rows that have a NaN (or empty string) in any required column\n",
    "df_clean = (\n",
    "    df.dropna(subset=req_cols)          # remove true NaNs\n",
    "      .query(\" & \".join([f\"{c} != ''\" for c in req_cols]))   # remove empty strings\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"kept {len(df_clean)} of {len(df)} rows\")\n",
    "\n",
    "df_clean.to_csv(FILE_OUT, index=False)\n",
    "print(f\"✓ saved cleaned file → {FILE_OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f8425d-d526-43fa-9729-53b439456eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdd27017-6dd2-447d-a00a-ab03910c2477",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompting_strategy → unique values:\n",
      "['zero_shot' 'few_shot' 'chain_of_thought'\n",
      " 'chain_of_thought_plus_role_chain' 'chain_of_thought_plus_sequential'\n",
      " 'chain_of_thought_plus_sequential_rl']\n",
      "\n",
      "question_type (raw) → dtype: object\n",
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10'] ...\n",
      "\n",
      "word_difficulty → unique values:\n",
      "['1' '2' '3' '4' '5']\n",
      "\n",
      "task_difficulty → unique values:\n",
      "['E' 'M' 'H']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Final_generated_questions_GPT35_clean.csv\", dtype=str)\n",
    "\n",
    "print(\"prompting_strategy → unique values:\")\n",
    "print(df[\"prompting_strategy\"].unique())\n",
    "\n",
    "print(\"\\nquestion_type (raw) → dtype:\", df[\"question_type\"].dtype)\n",
    "print(df[\"question_type\"].unique()[:10], \"...\")\n",
    "\n",
    "print(\"\\nword_difficulty → unique values:\")\n",
    "print(df[\"word_difficulty\"].unique())\n",
    "\n",
    "print(\"\\ntask_difficulty → unique values:\")\n",
    "print(df[\"task_difficulty\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4a6c3b-f150-4568-9e67-362e44b21b74",
   "metadata": {},
   "source": [
    "### Final Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a1502ca8-b0a8-49bb-bb27-8c235c4056ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items after filtering: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fa5c98345e4ee8a869ac4901b82a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] row 6 failed: Expecting value: line 1 column 1 (char 0)\n",
      "[warn] row 24 failed: Expecting value: line 1 column 1 (char 0)\n",
      "[warn] row 52 failed: Expecting value: line 1 column 1 (char 0)\n",
      "[warn] row 84 failed: Expecting value: line 1 column 1 (char 0)\n",
      "✓ Completed – streamed to gpt_o4_mini_evaluations.csv\n"
     ]
    }
   ],
   "source": [
    "# %% FILTERS you care about\n",
    "#     – leave key absent or empty list to keep *all* rows for that column\n",
    "# FILTERS = {\n",
    "#     \"prompting_strategy\": [\"chain_of_thought_plus_role_chain\"],\n",
    "#     \"question_type\":      [2],\n",
    "#     \"word_difficulty\":    [\"1\"],\n",
    "#     \"task_difficulty\":    [\"E\"],          # 'E' | 'M' | 'H'\n",
    "# }\n",
    "FILTERS = {}\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Imports / config\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "import os, json, math, asyncio, pandas as pd, openai\n",
    "from tqdm.notebook import tqdm\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "HUMAN_FILE = \"mcq_human_evals.csv\"\n",
    "ITEM_FILE  = \"Final_generated_questions_GPT35_clean.csv\"\n",
    "OUT_FILE   = \"gpt_o4_mini_evaluations.csv\"\n",
    "\n",
    "MODEL       = \"o4-mini\"\n",
    "TEMPERATURE = 1.0\n",
    "CONCURRENT  = 10          # parallel requests\n",
    "\n",
    "# load_dotenv()                              # uncomment if you use .env\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Load & filter data\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "items_df = pd.read_csv(ITEM_FILE,  dtype=str)\n",
    "human_df = pd.read_csv(HUMAN_FILE, dtype=str)\n",
    "\n",
    "human_df[\"question_type\"] = human_df[\"question_type\"].astype(int)\n",
    "items_df[\"question_type\"] = items_df[\"question_type\"].astype(int)\n",
    "\n",
    "for col, allowed in FILTERS.items():\n",
    "    if allowed:                                   # skip empty lists / None\n",
    "        items_df = items_df[items_df[col].isin([str(x) for x in allowed])]\n",
    "\n",
    "items_df = items_df.head(100)        \n",
    "print(\"Items after filtering:\", len(items_df))\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Helper – robust human‑example formatter  (NaN‑safe)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "def fmt_human_example(row) -> str:\n",
    "    \"\"\"Return one nicely formatted human‑rated example, skipping NaN/empty choices.\"\"\"\n",
    "    choice_lines = []\n",
    "    for i in range(1, 5):\n",
    "        val = row.get(f\"choice_{i}\", \"\")\n",
    "        # skip if val is NaN or empty\n",
    "        if val is None or (isinstance(val, float) and math.isnan(val)):\n",
    "            continue\n",
    "        txt = str(val).strip()\n",
    "        if not txt or txt.lower() == \"nan\":\n",
    "            continue\n",
    "        choice_lines.append(f\"{chr(64+i)}) {txt}\")\n",
    "\n",
    "    choices = \"\\n\".join(choice_lines)\n",
    "\n",
    "    return (\n",
    "        \"### Human‑rated Example\\n\"\n",
    "        f\"Question: {row['question']}\\n{choices}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer_text']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\"\n",
    "        \"Scores:\\n\"\n",
    "        f\"- Instruction Clarity: {row['eval_instruction_score']} ({row['eval_instruction_exp']})\\n\"\n",
    "        f\"- Accuracy of Correct Answer: {row['eval_accuracy_score']} ({row['eval_accuracy_exp']})\\n\"\n",
    "        f\"- Quality of Distractors: {row['eval_distractors_score']} ({row['eval_distractors_exp']})\\n\"\n",
    "        f\"- Word Difficulty Appropriateness: {row['eval_word_diff_score']} ({row['eval_word_diff_exp']})\\n\"\n",
    "        f\"- Task Difficulty Alignment: {row['eval_task_diff_score']} ({row['eval_task_diff_exp']})\\n\"\n",
    "        \"### End Example\\n\"\n",
    "    )\n",
    "\n",
    "# cache the example block once per question_type\n",
    "EXAMPLE_BLOCKS = {\n",
    "    q: \"\\n\".join(fmt_human_example(r) for _, r in g.iterrows())\n",
    "    for q, g in human_df.groupby(\"question_type\")\n",
    "}\n",
    "\n",
    "RUBRIC = \"\"\"\n",
    "Rate on five binary metrics (1 = meets, 0 = does not) and reply JSON only:\n",
    "{\n",
    " \"instr_score\":0/1, \"instr_exp\":\"...\",\n",
    " \"acc_score\":0/1,   \"acc_exp\":\"...\",\n",
    " \"dist_score\":0/1,  \"dist_exp\":\"...\",\n",
    " \"word_score\":0/1,  \"word_exp\":\"...\",\n",
    " \"task_score\":0/1,  \"task_exp\":\"...\",\n",
    " \"total_score\":0-5\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  CSV streaming helper\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "def append_row(row_dict: dict, path: str):\n",
    "    \"\"\"Append one row to CSV; create file+header on first call.\"\"\"\n",
    "    first_write = not os.path.exists(path)\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as fp:\n",
    "        writer = csv.DictWriter(fp, fieldnames=row_dict.keys())\n",
    "        if first_write:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row_dict)\n",
    "\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  Async evaluator\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "sem = asyncio.Semaphore(CONCURRENT)\n",
    "\n",
    "async def rate_item(idx, row):\n",
    "    choice_a = str(row.get(\"choice_a\", \"\") or \"\").strip()\n",
    "    choice_b = str(row.get(\"choice_b\", \"\") or \"\").strip()\n",
    "    choice_c = str(row.get(\"choice_c\", \"\") or \"\").strip()\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"{EXAMPLE_BLOCKS[row['question_type']]}\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "        \"### Item to evaluate\\n\"\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"A) {choice_a}\\n\"\n",
    "        f\"B) {choice_b}\\n\"\n",
    "        f\"C) {choice_c}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\\n\"\n",
    "        f\"{RUBRIC}\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a meticulous K‑12 morphology test reviewer.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    async with sem:\n",
    "        try:\n",
    "            rsp  = await openai.ChatCompletion.acreate(\n",
    "                model=MODEL, temperature=TEMPERATURE, messages=messages\n",
    "            )\n",
    "            data = json.loads(rsp.choices[0].message[\"content\"])\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] row {idx} failed: {e}\")\n",
    "            data = {k: None for k in\n",
    "                    [\"instr_score\",\"instr_exp\",\"acc_score\",\"acc_exp\",\n",
    "                     \"dist_score\",\"dist_exp\",\"word_score\",\"word_exp\",\n",
    "                     \"task_score\",\"task_exp\",\"total_score\"]}\n",
    "\n",
    "    merged = row.to_dict()\n",
    "    merged.update(data)\n",
    "    append_row(merged, OUT_FILE)      # ✨ write immediately\n",
    "    return merged\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  Orchestration\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "async def main():\n",
    "    tasks = [asyncio.create_task(rate_item(i, r))\n",
    "             for i, r in items_df.iterrows()]\n",
    "\n",
    "    # live progress bar as rows finish (order is non‑deterministic)\n",
    "    for f in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "        await f\n",
    "\n",
    "    print(f\"✓ Completed – streamed to {OUT_FILE}\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611373b4-8eee-4f45-91d9-f17cbd1f17b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af9d357b-27a8-440b-81fb-dae56e62d74f",
   "metadata": {},
   "source": [
    "## replace the current sequential_rl strategy with 'Final_generated_questions_GPT_chain_of_thought_plus_sequential_rl.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c08b61ea-d093-43a5-8af0-9dbc453757d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ builtin rows: 2917 | new rows: 584 → combined: 3501 \n",
      "Saved to Final_generated_questions_GPT35_clean_UPDATED_new.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ── paths ───────────────────────────────────────────────────────────\n",
    "MAIN_CSV   = \"Final_generated_questions_GPT35_clean.csv\"\n",
    "UPDATE_CSV = \"Final_generated_questions_GPT_chain_of_thought_plus_sequential_rl_new.csv\"\n",
    "OUT_CSV    = \"Final_generated_questions_GPT35_clean_UPDATED_new.csv\"\n",
    "\n",
    "TARGET_STRATEGY = \"chain_of_thought_plus_sequential_rl\"\n",
    "\n",
    "# ── load both files ────────────────────────────────────────────────\n",
    "main_df   = pd.read_csv(MAIN_CSV,   dtype=str)\n",
    "update_df = pd.read_csv(UPDATE_CSV, dtype=str)\n",
    "\n",
    "# ── 1) drop all old rows for that strategy from the main data ──────\n",
    "main_df = main_df[ main_df[\"prompting_strategy\"] != TARGET_STRATEGY ]\n",
    "\n",
    "# ── 2) put the update data columns in the *same* order as main_df ──\n",
    "update_df = update_df.reindex(columns=main_df.columns)\n",
    "\n",
    "# ── 3) append the new rows and write out ───────────────────────────\n",
    "final_df = pd.concat([main_df, update_df], ignore_index=True)\n",
    "final_df.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(\"✓ builtin rows:\", len(main_df),\n",
    "      \"| new rows:\", len(update_df),\n",
    "      \"→ combined:\", len(final_df),\n",
    "      \"\\nSaved to\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d2cfcaa2-20ab-4d25-9be6-abf4ac11eb9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_order = ['E', 'M', 'H']\n",
    "strategy_order = [\n",
    "    'zero_shot',\n",
    "    'few_shot',\n",
    "    'chain_of_thought',\n",
    "    'chain_of_thought_plus_role_chain',\n",
    "    'chain_of_thought_plus_sequential',\n",
    "    'chain_of_thought_plus_sequential_rl'\n",
    "]\n",
    "\n",
    "final_df['task_difficulty']      = pd.Categorical(final_df['task_difficulty'],\n",
    "                                            categories=task_order,\n",
    "                                            ordered=True)\n",
    "final_df['prompting_strategy']   = pd.Categorical(final_df['prompting_strategy'],\n",
    "                                            categories=strategy_order,\n",
    "                                            ordered=True)\n",
    "\n",
    "\n",
    "final_df_sorted = final_df.sort_values(\n",
    "    ['question_type', 'word_difficulty',\n",
    "     'task_difficulty', 'prompting_strategy']\n",
    ")\n",
    "final_df_sorted.to_csv(OUT_CSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6785d1-0a12-40aa-b0df-c5ad53001c44",
   "metadata": {},
   "source": [
    "## FINAL FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7f812927-d7e3-49dc-be7a-0f2820a1db31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items after filtering: 3501\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b8cb7bdb9a4a469f31e056405f1e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed – streamed to gpt_41_mini_evaluations_of_gpt_35_UPDATED_new.csv\n"
     ]
    }
   ],
   "source": [
    "# %% FILTERS you care about\n",
    "#     – leave key absent or empty list to keep *all* rows for that column\n",
    "# FILTERS = {\n",
    "#     \"prompting_strategy\": [\"chain_of_thought_plus_role_chain\"],\n",
    "#     \"question_type\":      [2],\n",
    "#     \"word_difficulty\":    [\"1\"],\n",
    "#     \"task_difficulty\":    [\"E\"],          # 'E' | 'M' | 'H'\n",
    "# }\n",
    "FILTERS = {}\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Imports / config\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "import os, json, math, asyncio, pandas as pd, openai\n",
    "from tqdm.notebook import tqdm\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "HUMAN_FILE = \"mcq_human_evals.csv\"\n",
    "ITEM_FILE  = \"Final_generated_questions_GPT35_clean_UPDATED_new.csv\"\n",
    "OUT_FILE   = \"gpt_41_mini_evaluations_of_gpt_35_UPDATED_new.csv\"\n",
    "\n",
    "# - gpt-4.1-mini (created: 2025-04-10 20:49:33)\n",
    "#  - gpt-4.1-mini-2025-04-14 (created: 2025-04-10 20:39:07)\n",
    "\n",
    "MODEL       = \"gpt-4.1-mini\"\n",
    "TEMPERATURE = 0.0\n",
    "CONCURRENT  = 10          # parallel requests\n",
    "\n",
    "# load_dotenv()                              # uncomment if you use .env\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Load & filter data\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "items_df = pd.read_csv(ITEM_FILE,  dtype=str)\n",
    "human_df = pd.read_csv(HUMAN_FILE, dtype=str)\n",
    "\n",
    "human_df[\"question_type\"] = human_df[\"question_type\"].astype(int)\n",
    "items_df[\"question_type\"] = items_df[\"question_type\"].astype(int)\n",
    "\n",
    "for col, allowed in FILTERS.items():\n",
    "    if allowed:                                   # skip empty lists / None\n",
    "        items_df = items_df[items_df[col].isin([str(x) for x in allowed])]\n",
    "\n",
    "# items_df = items_df.head(100)        \n",
    "print(\"Items after filtering:\", len(items_df))\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Helper – robust human‑example formatter  (NaN‑safe)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "def fmt_human_example(row) -> str:\n",
    "    \"\"\"Return one nicely formatted human‑rated example, skipping NaN/empty choices.\"\"\"\n",
    "    choice_lines = []\n",
    "    for i in range(1, 5):\n",
    "        val = row.get(f\"choice_{i}\", \"\")\n",
    "        # skip if val is NaN or empty\n",
    "        if val is None or (isinstance(val, float) and math.isnan(val)):\n",
    "            continue\n",
    "        txt = str(val).strip()\n",
    "        if not txt or txt.lower() == \"nan\":\n",
    "            continue\n",
    "        choice_lines.append(f\"{chr(64+i)}) {txt}\")\n",
    "\n",
    "    choices = \"\\n\".join(choice_lines)\n",
    "\n",
    "    return (\n",
    "        \"### Human‑rated Example\\n\"\n",
    "        f\"Question: {row['question']}\\n{choices}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer_text']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\"\n",
    "        \"Scores:\\n\"\n",
    "        f\"- Instruction Clarity: {row['eval_instruction_score']} ({row['eval_instruction_exp']})\\n\"\n",
    "        f\"- Accuracy of Correct Answer: {row['eval_accuracy_score']} ({row['eval_accuracy_exp']})\\n\"\n",
    "        f\"- Quality of Distractors: {row['eval_distractors_score']} ({row['eval_distractors_exp']})\\n\"\n",
    "        f\"- Word Difficulty Appropriateness: {row['eval_word_diff_score']} ({row['eval_word_diff_exp']})\\n\"\n",
    "        f\"- Task Difficulty Alignment: {row['eval_task_diff_score']} ({row['eval_task_diff_exp']})\\n\"\n",
    "        \"### End Example\\n\"\n",
    "    )\n",
    "\n",
    "# cache the example block once per question_type\n",
    "EXAMPLE_BLOCKS = {\n",
    "    q: \"\\n\".join(fmt_human_example(r) for _, r in g.iterrows())\n",
    "    for q, g in human_df.groupby(\"question_type\")\n",
    "}\n",
    "\n",
    "RUBRIC = \"\"\"\n",
    "Rate on five binary metrics (1 = meets, 0 = does not) and reply JSON only:\n",
    "{\n",
    " \"instr_score\":0/1, \"instr_exp\":\"...\",\n",
    " \"acc_score\":0/1,   \"acc_exp\":\"...\",\n",
    " \"dist_score\":0/1,  \"dist_exp\":\"...\",\n",
    " \"word_score\":0/1,  \"word_exp\":\"...\",\n",
    " \"task_score\":0/1,  \"task_exp\":\"...\",\n",
    " \"total_score\":0-5\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  CSV streaming helper\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "def append_row(row_dict: dict, path: str):\n",
    "    \"\"\"Append one row to CSV; create file+header on first call.\"\"\"\n",
    "    first_write = not os.path.exists(path)\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as fp:\n",
    "        writer = csv.DictWriter(fp, fieldnames=row_dict.keys())\n",
    "        if first_write:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row_dict)\n",
    "\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  Async evaluator\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "sem = asyncio.Semaphore(CONCURRENT)\n",
    "\n",
    "async def rate_item(idx, row):\n",
    "    choice_a = str(row.get(\"choice_a\", \"\") or \"\").strip()\n",
    "    choice_b = str(row.get(\"choice_b\", \"\") or \"\").strip()\n",
    "    choice_c = str(row.get(\"choice_c\", \"\") or \"\").strip()\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"{EXAMPLE_BLOCKS[row['question_type']]}\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "        \"### Item to evaluate\\n\"\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"A) {choice_a}\\n\"\n",
    "        f\"B) {choice_b}\\n\"\n",
    "        f\"C) {choice_c}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\\n\"\n",
    "        f\"{RUBRIC}\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a meticulous K‑12 morphology test reviewer.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    async with sem:\n",
    "        try:\n",
    "            rsp  = await openai.ChatCompletion.acreate(\n",
    "                model=MODEL, temperature=TEMPERATURE, messages=messages\n",
    "            )\n",
    "            data = json.loads(rsp.choices[0].message[\"content\"])\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] row {idx} failed: {e}\")\n",
    "            data = {k: None for k in\n",
    "                    [\"instr_score\",\"instr_exp\",\"acc_score\",\"acc_exp\",\n",
    "                     \"dist_score\",\"dist_exp\",\"word_score\",\"word_exp\",\n",
    "                     \"task_score\",\"task_exp\",\"total_score\"]}\n",
    "\n",
    "    merged = row.to_dict()\n",
    "    merged.update(data)\n",
    "    append_row(merged, OUT_FILE)      # ✨ write immediately\n",
    "    return merged\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  Orchestration\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "async def main():\n",
    "    tasks = [asyncio.create_task(rate_item(i, r))\n",
    "             for i, r in items_df.iterrows()]\n",
    "\n",
    "    # live progress bar as rows finish (order is non‑deterministic)\n",
    "    for f in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "        await f\n",
    "\n",
    "    print(f\"✓ Completed – streamed to {OUT_FILE}\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "14af5149-b770-4581-9be4-8c4a103aa05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓  Saved /blue/babajani.a/babak.ahmadi/NLP_Dorr/Project/MA/gpt_41_mini_evaluations_of_gpt_35_UPDATED_new.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "# --- filenames ---------------------------------------------------------------\n",
    "csv_file   = pathlib.Path(\"gpt_41_mini_evaluations_of_gpt_35_UPDATED_new.csv\")\n",
    "excel_file = csv_file.with_suffix(\".xlsx\")         # same name, .xlsx extension\n",
    "\n",
    "# --- convert -----------------------------------------------------------------\n",
    "df = pd.read_csv(csv_file, encoding=\"utf-8\")        # adjust encoding if needed\n",
    "df.to_excel(excel_file, index=False)\n",
    "\n",
    "print(f\"✓  Saved {excel_file.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff8e7e-6ab7-4007-98f8-40c7c5f60c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fce02394-cf83-4dce-a022-1e077cadd77b",
   "metadata": {},
   "source": [
    "## evaluate the items generated by Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c07fd517-b7d2-4f41-b05c-89bad26ea76e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items after filtering: 2143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7727b1e070b54667ab975aeffa78c721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] row 253 failed: Request timed out\n",
      "✓ Completed – streamed to gpt_41_mini_evaluations_of_gemma.csv\n"
     ]
    }
   ],
   "source": [
    "# %% FILTERS you care about\n",
    "#     – leave key absent or empty list to keep *all* rows for that column\n",
    "# FILTERS = {\n",
    "#     \"prompting_strategy\": [\"chain_of_thought_plus_role_chain\"],\n",
    "#     \"question_type\":      [2],\n",
    "#     \"word_difficulty\":    [\"1\"],\n",
    "#     \"task_difficulty\":    [\"E\"],          # 'E' | 'M' | 'H'\n",
    "# }\n",
    "FILTERS = {}\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Imports / config\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "import os, json, math, asyncio, pandas as pd, openai\n",
    "from tqdm.notebook import tqdm\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "HUMAN_FILE = \"mcq_human_evals.csv\"\n",
    "ITEM_FILE  = \"Final_generated_questions_Gemma.csv\"\n",
    "OUT_FILE   = \"gpt_41_mini_evaluations_of_gemma.csv\"\n",
    "\n",
    "# - gpt-4.1-mini (created: 2025-04-10 20:49:33)\n",
    "#  - gpt-4.1-mini-2025-04-14 (created: 2025-04-10 20:39:07)\n",
    "\n",
    "MODEL       = \"gpt-4.1-mini\"\n",
    "TEMPERATURE = 0.0\n",
    "CONCURRENT  = 10          # parallel requests\n",
    "\n",
    "# load_dotenv()                              # uncomment if you use .env\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Load & filter data\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "items_df = pd.read_csv(ITEM_FILE,  dtype=str)\n",
    "human_df = pd.read_csv(HUMAN_FILE, dtype=str)\n",
    "\n",
    "# ── rename columns in items_df so the rest of the script works unchanged ─────\n",
    "items_df = items_df.rename(columns={\n",
    "    \"Prompting_strategy\": \"prompting_strategy\",\n",
    "    \"Question_Type\":      \"question_type\",\n",
    "    \"Word_Difficulty\":    \"word_difficulty\",\n",
    "    \"Task_Difficulty\":    \"task_difficulty\",\n",
    "    \"Question\":           \"question\",\n",
    "    \"Correct_Answer\":     \"correct_answer\",\n",
    "    \"Choice_1\":           \"choice_a\",\n",
    "    \"Choice_2\":           \"choice_b\",\n",
    "    \"Choice_3\":           \"choice_c\",\n",
    "    # keep any other columns (e.g., 'Text') as‑is\n",
    "})\n",
    "\n",
    "human_df[\"question_type\"] = human_df[\"question_type\"].astype(int)\n",
    "items_df[\"question_type\"] = items_df[\"question_type\"].astype(int)\n",
    "\n",
    "for col, allowed in FILTERS.items():\n",
    "    if allowed:                                   # skip empty lists / None\n",
    "        items_df = items_df[items_df[col].isin([str(x) for x in allowed])]\n",
    "\n",
    "# items_df = items_df.head(10)        \n",
    "print(\"Items after filtering:\", len(items_df))\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Helper – robust human‑example formatter  (NaN‑safe)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "def fmt_human_example(row) -> str:\n",
    "    \"\"\"Return one nicely formatted human‑rated example, skipping NaN/empty choices.\"\"\"\n",
    "    choice_lines = []\n",
    "    for i in range(1, 5):\n",
    "        val = row.get(f\"choice_{i}\", \"\")\n",
    "        # skip if val is NaN or empty\n",
    "        if val is None or (isinstance(val, float) and math.isnan(val)):\n",
    "            continue\n",
    "        txt = str(val).strip()\n",
    "        if not txt or txt.lower() == \"nan\":\n",
    "            continue\n",
    "        choice_lines.append(f\"{chr(64+i)}) {txt}\")\n",
    "\n",
    "    choices = \"\\n\".join(choice_lines)\n",
    "\n",
    "    return (\n",
    "        \"### Human‑rated Example\\n\"\n",
    "        f\"Question: {row['question']}\\n{choices}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer_text']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\"\n",
    "        \"Scores:\\n\"\n",
    "        f\"- Instruction Clarity: {row['eval_instruction_score']} ({row['eval_instruction_exp']})\\n\"\n",
    "        f\"- Accuracy of Correct Answer: {row['eval_accuracy_score']} ({row['eval_accuracy_exp']})\\n\"\n",
    "        f\"- Quality of Distractors: {row['eval_distractors_score']} ({row['eval_distractors_exp']})\\n\"\n",
    "        f\"- Word Difficulty Appropriateness: {row['eval_word_diff_score']} ({row['eval_word_diff_exp']})\\n\"\n",
    "        f\"- Task Difficulty Alignment: {row['eval_task_diff_score']} ({row['eval_task_diff_exp']})\\n\"\n",
    "        \"### End Example\\n\"\n",
    "    )\n",
    "\n",
    "# cache the example block once per question_type\n",
    "EXAMPLE_BLOCKS = {\n",
    "    q: \"\\n\".join(fmt_human_example(r) for _, r in g.iterrows())\n",
    "    for q, g in human_df.groupby(\"question_type\")\n",
    "}\n",
    "\n",
    "RUBRIC = \"\"\"\n",
    "Rate on five binary metrics (1 = meets, 0 = does not) and reply JSON only:\n",
    "{\n",
    " \"instr_score\":0/1, \"instr_exp\":\"...\",\n",
    " \"acc_score\":0/1,   \"acc_exp\":\"...\",\n",
    " \"dist_score\":0/1,  \"dist_exp\":\"...\",\n",
    " \"word_score\":0/1,  \"word_exp\":\"...\",\n",
    " \"task_score\":0/1,  \"task_exp\":\"...\",\n",
    " \"total_score\":0-5\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  CSV streaming helper\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "def append_row(row_dict: dict, path: str):\n",
    "    \"\"\"Append one row to CSV; create file+header on first call.\"\"\"\n",
    "    first_write = not os.path.exists(path)\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as fp:\n",
    "        writer = csv.DictWriter(fp, fieldnames=row_dict.keys())\n",
    "        if first_write:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row_dict)\n",
    "\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  Async evaluator\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "sem = asyncio.Semaphore(CONCURRENT)\n",
    "\n",
    "async def rate_item(idx, row):\n",
    "    choice_a = str(row.get(\"choice_a\", \"\") or \"\").strip()\n",
    "    choice_b = str(row.get(\"choice_b\", \"\") or \"\").strip()\n",
    "    choice_c = str(row.get(\"choice_c\", \"\") or \"\").strip()\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"{EXAMPLE_BLOCKS[row['question_type']]}\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "        \"### Item to evaluate\\n\"\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"A) {choice_a}\\n\"\n",
    "        f\"B) {choice_b}\\n\"\n",
    "        f\"C) {choice_c}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\\n\"\n",
    "        f\"{RUBRIC}\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a meticulous K‑12 morphology test reviewer.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    async with sem:\n",
    "        try:\n",
    "            rsp  = await openai.ChatCompletion.acreate(\n",
    "                model=MODEL, temperature=TEMPERATURE, messages=messages\n",
    "            )\n",
    "            data = json.loads(rsp.choices[0].message[\"content\"])\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] row {idx} failed: {e}\")\n",
    "            data = {k: None for k in\n",
    "                    [\"instr_score\",\"instr_exp\",\"acc_score\",\"acc_exp\",\n",
    "                     \"dist_score\",\"dist_exp\",\"word_score\",\"word_exp\",\n",
    "                     \"task_score\",\"task_exp\",\"total_score\"]}\n",
    "\n",
    "    merged = row.to_dict()\n",
    "    merged.update(data)\n",
    "    append_row(merged, OUT_FILE)      #  write immediately\n",
    "    return merged\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  Orchestration\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "async def main():\n",
    "    tasks = [asyncio.create_task(rate_item(i, r))\n",
    "             for i, r in items_df.iterrows()]\n",
    "\n",
    "    # live progress bar as rows finish (order is non‑deterministic)\n",
    "    for f in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "        await f\n",
    "\n",
    "    print(f\"✓ Completed – streamed to {OUT_FILE}\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c32b5a78-bbba-4152-96a7-8b08cfe16f33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓  Saved /blue/babajani.a/babak.ahmadi/NLP_Dorr/Project/MA/gpt_41_mini_evaluations_of_gemma.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --- filenames ---------------------------------------------------------------\n",
    "csv_file   = pathlib.Path(\"gpt_41_mini_evaluations_of_gemma.csv\")\n",
    "excel_file = csv_file.with_suffix(\".xlsx\")         # same name, .xlsx extension\n",
    "\n",
    "# --- convert -----------------------------------------------------------------\n",
    "df = pd.read_csv(csv_file, encoding=\"utf-8\")        # adjust encoding if needed\n",
    "df.to_excel(excel_file, index=False)\n",
    "\n",
    "print(f\"✓  Saved {excel_file.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e73bfca1-3e06-4e88-8126-04295847fd16",
   "metadata": {
    "tags": []
   },
   "source": [
    "# save in excel, instead of csv\n",
    "\n",
    "# %% FILTERS you care about\n",
    "#     – leave key absent or empty list to keep *all* rows for that column\n",
    "# FILTERS = {\n",
    "#     \"prompting_strategy\": [\"chain_of_thought_plus_role_chain\"],\n",
    "#     \"question_type\":      [2],\n",
    "#     \"word_difficulty\":    [\"1\"],\n",
    "#     \"task_difficulty\":    [\"E\"],          # 'E' | 'M' | 'H'\n",
    "# }\n",
    "FILTERS = {}\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Imports / config\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "import os, json, math, asyncio, pandas as pd, openai\n",
    "from tqdm.notebook import tqdm\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "HUMAN_FILE = \"mcq_human_evals.csv\"\n",
    "ITEM_FILE  = \"Final_generated_questions_Gemma.csv\"\n",
    "OUT_FILE = \"gpt_41_mini_evaluations_of_gemma.xlsx\"     # <- .xlsx, not .csv\n",
    "\n",
    "\n",
    "# - gpt-4.1-mini (created: 2025-04-10 20:49:33)\n",
    "#  - gpt-4.1-mini-2025-04-14 (created: 2025-04-10 20:39:07)\n",
    "\n",
    "MODEL       = \"gpt-4.1-mini\"\n",
    "TEMPERATURE = 0.0\n",
    "CONCURRENT  = 10          # parallel requests\n",
    "\n",
    "# load_dotenv()                              # uncomment if you use .env\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Load & filter data\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "items_df = pd.read_csv(ITEM_FILE,  dtype=str)\n",
    "human_df = pd.read_csv(HUMAN_FILE, dtype=str)\n",
    "\n",
    "# ── rename columns in items_df so the rest of the script works unchanged ─────\n",
    "items_df = items_df.rename(columns={\n",
    "    \"Prompting_strategy\": \"prompting_strategy\",\n",
    "    \"Question_Type\":      \"question_type\",\n",
    "    \"Word_Difficulty\":    \"word_difficulty\",\n",
    "    \"Task_Difficulty\":    \"task_difficulty\",\n",
    "    \"Question\":           \"question\",\n",
    "    \"Correct_Answer\":     \"correct_answer\",\n",
    "    \"Choice_1\":           \"choice_a\",\n",
    "    \"Choice_2\":           \"choice_b\",\n",
    "    \"Choice_3\":           \"choice_c\",\n",
    "    # keep any other columns (e.g., 'Text') as‑is\n",
    "})\n",
    "\n",
    "human_df[\"question_type\"] = human_df[\"question_type\"].astype(int)\n",
    "items_df[\"question_type\"] = items_df[\"question_type\"].astype(int)\n",
    "\n",
    "for col, allowed in FILTERS.items():\n",
    "    if allowed:                                   # skip empty lists / None\n",
    "        items_df = items_df[items_df[col].isin([str(x) for x in allowed])]\n",
    "\n",
    "items_df = items_df.head(10)        \n",
    "print(\"Items after filtering:\", len(items_df))\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Helper – robust human‑example formatter  (NaN‑safe)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "def fmt_human_example(row) -> str:\n",
    "    \"\"\"Return one nicely formatted human‑rated example, skipping NaN/empty choices.\"\"\"\n",
    "    choice_lines = []\n",
    "    for i in range(1, 5):\n",
    "        val = row.get(f\"choice_{i}\", \"\")\n",
    "        # skip if val is NaN or empty\n",
    "        if val is None or (isinstance(val, float) and math.isnan(val)):\n",
    "            continue\n",
    "        txt = str(val).strip()\n",
    "        if not txt or txt.lower() == \"nan\":\n",
    "            continue\n",
    "        choice_lines.append(f\"{chr(64+i)}) {txt}\")\n",
    "\n",
    "    choices = \"\\n\".join(choice_lines)\n",
    "\n",
    "    return (\n",
    "        \"### Human‑rated Example\\n\"\n",
    "        f\"Question: {row['question']}\\n{choices}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer_text']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\"\n",
    "        \"Scores:\\n\"\n",
    "        f\"- Instruction Clarity: {row['eval_instruction_score']} ({row['eval_instruction_exp']})\\n\"\n",
    "        f\"- Accuracy of Correct Answer: {row['eval_accuracy_score']} ({row['eval_accuracy_exp']})\\n\"\n",
    "        f\"- Quality of Distractors: {row['eval_distractors_score']} ({row['eval_distractors_exp']})\\n\"\n",
    "        f\"- Word Difficulty Appropriateness: {row['eval_word_diff_score']} ({row['eval_word_diff_exp']})\\n\"\n",
    "        f\"- Task Difficulty Alignment: {row['eval_task_diff_score']} ({row['eval_task_diff_exp']})\\n\"\n",
    "        \"### End Example\\n\"\n",
    "    )\n",
    "\n",
    "# cache the example block once per question_type\n",
    "EXAMPLE_BLOCKS = {\n",
    "    q: \"\\n\".join(fmt_human_example(r) for _, r in g.iterrows())\n",
    "    for q, g in human_df.groupby(\"question_type\")\n",
    "}\n",
    "\n",
    "RUBRIC = \"\"\"\n",
    "Rate on five binary metrics (1 = meets, 0 = does not) and reply JSON only:\n",
    "{\n",
    " \"instr_score\":0/1, \"instr_exp\":\"...\",\n",
    " \"acc_score\":0/1,   \"acc_exp\":\"...\",\n",
    " \"dist_score\":0/1,  \"dist_exp\":\"...\",\n",
    " \"word_score\":0/1,  \"word_exp\":\"...\",\n",
    " \"task_score\":0/1,  \"task_exp\":\"...\",\n",
    " \"total_score\":0-5\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  Excel‑streaming helper  (openpyxl)\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "from openpyxl import Workbook, load_workbook\n",
    "\n",
    "def append_row(row_dict: dict, path: str, sheet_name: str = \"data\"):\n",
    "    \"\"\"\n",
    "    Append one dict row to an .xlsx workbook.\n",
    "    • Creates the file + sheet + header on first call.\n",
    "    • Re‑uses header order on every subsequent call.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        wb = Workbook()\n",
    "        ws = wb.active\n",
    "        ws.title = sheet_name\n",
    "        ws.append(list(row_dict.keys()))          # header row\n",
    "        wb.save(path)\n",
    "\n",
    "    wb = load_workbook(path)\n",
    "    ws = wb[sheet_name]\n",
    "\n",
    "    header = [cell.value for cell in ws[1]]       # ← get the text, not a Cell\n",
    "    ws.append([row_dict.get(col, \"\") for col in header])\n",
    "    wb.save(path)\n",
    "\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  Async evaluator\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "sem = asyncio.Semaphore(CONCURRENT)\n",
    "\n",
    "async def rate_item(idx, row):\n",
    "    choice_a = str(row.get(\"choice_a\", \"\") or \"\").strip()\n",
    "    choice_b = str(row.get(\"choice_b\", \"\") or \"\").strip()\n",
    "    choice_c = str(row.get(\"choice_c\", \"\") or \"\").strip()\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"{EXAMPLE_BLOCKS[row['question_type']]}\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "        \"### Item to evaluate\\n\"\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"A) {choice_a}\\n\"\n",
    "        f\"B) {choice_b}\\n\"\n",
    "        f\"C) {choice_c}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\\n\"\n",
    "        f\"{RUBRIC}\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a meticulous K‑12 morphology test reviewer.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    async with sem:\n",
    "        try:\n",
    "            rsp  = await openai.ChatCompletion.acreate(\n",
    "                model=MODEL, temperature=TEMPERATURE, messages=messages\n",
    "            )\n",
    "            data = json.loads(rsp.choices[0].message[\"content\"])\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] row {idx} failed: {e}\")\n",
    "            data = {k: None for k in\n",
    "                    [\"instr_score\",\"instr_exp\",\"acc_score\",\"acc_exp\",\n",
    "                     \"dist_score\",\"dist_exp\",\"word_score\",\"word_exp\",\n",
    "                     \"task_score\",\"task_exp\",\"total_score\"]}\n",
    "\n",
    "    merged = row.to_dict()\n",
    "    merged.update(data)\n",
    "    append_row(merged, OUT_FILE)      #  write immediately\n",
    "    return merged\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  Orchestration\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "async def main():\n",
    "    tasks = [asyncio.create_task(rate_item(i, r))\n",
    "             for i, r in items_df.iterrows()]\n",
    "\n",
    "    # live progress bar as rows finish (order is non‑deterministic)\n",
    "    for f in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "        await f\n",
    "\n",
    "    print(f\"✓ Completed – streamed to {OUT_FILE}\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4678f3-07bc-4a87-aa59-8d453caeacfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-1.3",
   "language": "python",
   "name": "nlp-1.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
