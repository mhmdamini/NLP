{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "146647ca-e731-4e9e-ab93-b694014e62ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HF_TOKEN = \"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\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e38fe-ed4f-41fa-b37a-5e9ee23017bc",
   "metadata": {},
   "source": [
    "## Train the Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7533f7f-b50f-4b73-99b7-c42f5e8692a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "# Define HF token globally so it's accessible everywhere\n",
    "HF_TOKEN = \"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\"\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN  # Set environment variable for token\n",
    "\n",
    "class MorphologyDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        \n",
    "        # Structured prompt format\n",
    "        instruction = (\n",
    "            \"You are answering a morphology question. \"\n",
    "            \"Begin your answer with 'Answer: Choice X' for multiple choice or 'Answer: [text]' for open-ended questions. \"\n",
    "            \"Provide a clear explanation afterward.\"\n",
    "        )\n",
    "        \n",
    "        task_type = \"Multiple choice\" if pd.notna(item['Choice_1']) else \"Open-ended\"\n",
    "        \n",
    "        # Input format with clear separation\n",
    "        input_text = (\n",
    "            f\"{instruction}\\n\\n\"\n",
    "            f\"# Question Information\\n\"\n",
    "            f\"- Type: {task_type}\\n\"\n",
    "            f\"- Task: {item['Task']}\\n\"\n",
    "            f\"- Category: {item['Category']}\\n\"\n",
    "            f\"- Word: {item['Word']}\\n\"\n",
    "            f\"- Question: {item['Instruction']}\\n\"\n",
    "        )\n",
    "        \n",
    "        if pd.notna(item['Choice_1']):\n",
    "            choices = []\n",
    "            choice_num = 1\n",
    "            input_text += \"\\n# Available Choices\\n\"\n",
    "            while True:\n",
    "                choice_key = f'Choice_{choice_num}'\n",
    "                if choice_key not in item or pd.isna(item[choice_key]):\n",
    "                    break\n",
    "                input_text += f\"- Choice {choice_num}: {item[choice_key]}\\n\"\n",
    "                choice_num += 1\n",
    "        \n",
    "        # Clear delimiter between input and expected output\n",
    "        input_text += \"\\n# Your Answer:\\n\"\n",
    "        \n",
    "        # Target output format\n",
    "        if pd.notna(item['Choice_1']):\n",
    "            correct_choice = item[f'Choice_{item[\"Correct_Answer\"]}']\n",
    "            target_text = (\n",
    "                f\"Answer: Choice {item['Correct_Answer']}. \"\n",
    "                f\"{correct_choice} is correct because it demonstrates the {item['Category'].lower()} \"\n",
    "                f\"concept. In the word '{item['Word']}', we can identify the {item['Task'].lower()} \"\n",
    "                f\"through proper morphological analysis. This is a key concept in understanding \"\n",
    "                f\"how words are formed and structured in English.\"\n",
    "            )\n",
    "        else:\n",
    "            target_text = (\n",
    "                f\"Answer: {str(item['Correct_Answer'])}. \"\n",
    "                f\"This demonstrates the {item['Category'].lower()} concept in '{item['Word']}'. \"\n",
    "                f\"When analyzing how this word {item['Task'].lower()}, we can see the morphological \"\n",
    "                f\"principles at work. This helps us understand the structure and formation of words.\"\n",
    "            )\n",
    "\n",
    "        # Combine input and target with EOS token\n",
    "        full_text = f\"{input_text}{target_text}</s>\"\n",
    "        \n",
    "        # Create encodings\n",
    "        encodings = self.tokenizer(\n",
    "            full_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Create labels with masked loss for prompt\n",
    "        input_only = self.tokenizer(\n",
    "            input_text,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_length = input_only['input_ids'].shape[1]\n",
    "        labels = encodings['input_ids'].clone()\n",
    "        \n",
    "        # Set prompt part to -100 to ignore in loss calculation\n",
    "        labels[:, :input_length] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "            'labels': labels.squeeze()\n",
    "        }\n",
    "\n",
    "def prepare_data(csv_path):\n",
    "    \"\"\"Prepare and split the data for training with data augmentation\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Convert necessary columns to string\n",
    "    for col in ['Correct_Answer', 'Word_Difficulty', 'Task_Difficulty']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Simple data augmentation: create small variations in questions\n",
    "    augmented_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        augmented_data.append(row.to_dict())  # Original row\n",
    "        \n",
    "        # Only augment if it's a multiple choice question\n",
    "        if pd.notna(row.get('Choice_1', pd.NA)):\n",
    "            # Variation 1: Slightly different instruction wording\n",
    "            variation = row.to_dict()\n",
    "            orig_instruction = variation['Instruction']\n",
    "            \n",
    "            if \"what is\" in orig_instruction.lower():\n",
    "                variation['Instruction'] = orig_instruction.lower().replace(\"what is\", \"identify\").capitalize()\n",
    "                augmented_data.append(variation)\n",
    "            elif \"identify\" in orig_instruction.lower():\n",
    "                variation['Instruction'] = orig_instruction.lower().replace(\"identify\", \"what is\").capitalize()\n",
    "                augmented_data.append(variation)\n",
    "    \n",
    "    # Convert back to DataFrame\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    \n",
    "    # Split data with stratification\n",
    "    try:\n",
    "        train_df, val_df = train_test_split(\n",
    "            augmented_df, \n",
    "            test_size=0.15,\n",
    "            random_state=42,\n",
    "            stratify=augmented_df[['Category', 'Task']].apply(lambda x: f\"{x['Category']}_{x['Task']}\", axis=1)\n",
    "        )\n",
    "    except ValueError:\n",
    "        # Fallback to stratifying by just Category\n",
    "        train_df, val_df = train_test_split(\n",
    "            augmented_df, \n",
    "            test_size=0.15, \n",
    "            random_state=42, \n",
    "            stratify=augmented_df['Category']\n",
    "        )\n",
    "    \n",
    "    print(f\"Original data size: {len(df)}\")\n",
    "    print(f\"Augmented data size: {len(augmented_df)}\")\n",
    "    print(f\"Training set size: {len(train_df)}, Validation set size: {len(val_df)}\")\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "def monitor_gpu_memory(message):\n",
    "    \"\"\"Helper function to monitor GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "        max_allocated = torch.cuda.max_memory_allocated() / (1024 ** 3)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024 ** 3)\n",
    "        print(f\"{message}: Allocated: {allocated:.2f} GB, Max: {max_allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "def save_config_locally(model_name, output_dir):\n",
    "    \"\"\"Save model config locally to avoid authentication issues during training\"\"\"\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    import shutil\n",
    "    \n",
    "    # Create directory structure\n",
    "    base_path = os.path.join(output_dir, \"base_model_config\")\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Download config file\n",
    "        config_path = hf_hub_download(\n",
    "            repo_id=model_name,\n",
    "            filename=\"config.json\",\n",
    "            token=HF_TOKEN\n",
    "        )\n",
    "        \n",
    "        # Copy to our directory\n",
    "        shutil.copy(config_path, os.path.join(base_path, \"config.json\"))\n",
    "        print(f\"Config saved locally to {base_path}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving config locally: {e}\")\n",
    "        return False\n",
    "\n",
    "def train_gemma_model(train_df, val_df, model_name=\"google/gemma-2b-it\", output_dir=\"gemma_morphology\", use_lora=True):\n",
    "    \"\"\"Train Gemma model with advanced techniques\"\"\"\n",
    "    # Clean memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    monitor_gpu_memory(\"Initial GPU state\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save config locally to avoid authentication issues\n",
    "    save_config_locally(model_name, output_dir)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "    \n",
    "    # Save tokenizer locally to avoid authentication issues\n",
    "    tokenizer_save_path = os.path.join(output_dir, \"tokenizer\")\n",
    "    tokenizer.save_pretrained(tokenizer_save_path)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load model with memory optimizations\n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=HF_TOKEN,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float32,  # Always use float32 to avoid gradient issues\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    monitor_gpu_memory(\"After model loading\")\n",
    "    \n",
    "    # Apply LoRA for more efficient fine-tuning if requested\n",
    "    if use_lora:\n",
    "        print(\"Applying LoRA adapters...\")\n",
    "        # Configure LoRA\n",
    "        lora_config = LoraConfig(\n",
    "            r=16,  # rank\n",
    "            lora_alpha=32,  # scaling factor\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM\n",
    "        )\n",
    "        \n",
    "        # Apply LoRA to model\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "    else:\n",
    "        # Enable gradient checkpointing for full fine-tuning\n",
    "        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    \n",
    "    monitor_gpu_memory(\"After model adaptation\")\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"Creating datasets...\")\n",
    "    train_dataset = MorphologyDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = MorphologyDataset(val_df, tokenizer, max_length=512)\n",
    "    \n",
    "    monitor_gpu_memory(\"After dataset creation\")\n",
    "\n",
    "    # Training arguments\n",
    "    batch_size = 2 if use_lora else 1\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=12,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_ratio=0.1,\n",
    "        learning_rate=3e-5 if use_lora else 2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=30,\n",
    "        save_steps=30,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        gradient_accumulation_steps=4 if use_lora else 8,\n",
    "        fp16=False,  # DISABLE fp16 to avoid gradient issues\n",
    "        bf16=False,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        optim=\"adamw_torch\",\n",
    "        gradient_checkpointing=not use_lora,  # Enable for full fine-tuning only\n",
    "        ddp_find_unused_parameters=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\",  # Disable reporting to save memory\n",
    "        run_name=f\"gemma_morphology_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "        hub_token=HF_TOKEN,  # Add token for Hugging Face API calls\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "    \n",
    "    monitor_gpu_memory(\"Before trainer initialization\")\n",
    "\n",
    "    # Initialize trainer with callbacks\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Stop if not improving\n",
    "    )\n",
    "    \n",
    "    monitor_gpu_memory(\"After trainer initialization\")\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    print(\"Saving model...\")\n",
    "    model_save_path = os.path.join(output_dir, \"final_model\")\n",
    "    \n",
    "    if use_lora:\n",
    "        # For LoRA, we save the adapter\n",
    "        model.save_pretrained(model_save_path, token=HF_TOKEN)\n",
    "    else:\n",
    "        # For full model, save everything\n",
    "        trainer.save_model(model_save_path)\n",
    "    \n",
    "    # Save tokenizer from the local copy to avoid authentication issues\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    \n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def test_model(model, tokenizer, test_questions):\n",
    "    \"\"\"Test the model on a set of questions\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for question in test_questions:\n",
    "        # Structured prompt format\n",
    "        instruction = (\n",
    "            \"You are answering a morphology question. \"\n",
    "            \"Begin your answer with 'Answer: Choice X' for multiple choice or 'Answer: [text]' for open-ended questions. \"\n",
    "            \"Provide a clear explanation afterward.\"\n",
    "        )\n",
    "        \n",
    "        # Format input\n",
    "        input_text = (\n",
    "            f\"{instruction}\\n\\n\"\n",
    "            f\"# Question Information\\n\"\n",
    "            f\"- Type: Multiple choice\\n\"\n",
    "            f\"- Task: {question['Task']}\\n\"\n",
    "            f\"- Category: {question['Category']}\\n\"\n",
    "            f\"- Word: {question['Word']}\\n\"\n",
    "            f\"- Question: {question['Instruction']}\\n\"\n",
    "            f\"\\n# Available Choices\\n\"\n",
    "        )\n",
    "        \n",
    "        # Add choices\n",
    "        for i, choice in enumerate(question['Choices'], 1):\n",
    "            input_text += f\"- Choice {i}: {choice}\\n\"\n",
    "        \n",
    "        # Add answer prompt\n",
    "        input_text += \"\\n# Your Answer:\\n\"\n",
    "        \n",
    "        # Generation settings\n",
    "        generation_config = {\n",
    "            'max_new_tokens': 200,\n",
    "            'do_sample': True,\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.92,\n",
    "            'top_k': 50,\n",
    "            'repetition_penalty': 1.2,\n",
    "        }\n",
    "        \n",
    "        # Generate answer\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                **generation_config\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract answer part\n",
    "        if input_text in generated_text:\n",
    "            answer = generated_text[len(input_text):].strip()\n",
    "        else:\n",
    "            answer = generated_text\n",
    "            \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'input': input_text,\n",
    "            'generated': answer\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Configure paths\n",
    "    data_path = 'MC_data_MA2.csv'\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_dir = f\"gemma_morphology\"#_{timestamp}\"\n",
    "    \n",
    "    # Display GPU information\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"\\nPreparing data...\")\n",
    "    train_df, val_df = prepare_data(data_path)\n",
    "    \n",
    "    # Define whether to use LoRA (recommended for better memory efficiency)\n",
    "    use_lora = True  # Set to False for full fine-tuning if you have enough GPU memory\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nTraining Gemma model {'with LoRA' if use_lora else ''}...\")\n",
    "    model, tokenizer = train_gemma_model(train_df, val_df, output_dir=output_dir, use_lora=use_lora)\n",
    "    \n",
    "    # Test questions\n",
    "    test_questions = [\n",
    "        {\n",
    "            \"Task\": \"Identify\",\n",
    "            \"Category\": \"Derivation\",\n",
    "            \"Word\": \"happiness\",\n",
    "            \"Instruction\": \"What is the base word and suffix in 'happiness'?\",\n",
    "            \"Choices\": [\n",
    "                \"base: happy, suffix: -ness\",\n",
    "                \"base: happ, suffix: -iness\",\n",
    "                \"base: happi, suffix: -ness\",\n",
    "                \"base: hap, suffix: -piness\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Task\": \"Analyze\",\n",
    "            \"Category\": \"Compounding\",\n",
    "            \"Word\": \"blackboard\",\n",
    "            \"Instruction\": \"Identify the type of compound word in 'blackboard'.\",\n",
    "            \"Choices\": [\n",
    "                \"Endocentric compound\",\n",
    "                \"Exocentric compound\",\n",
    "                \"Copulative compound\",\n",
    "                \"Appositional compound\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    return model, tokenizer\n",
    "    # Test the model\n",
    "    print(\"\\nTesting model on sample questions...\")\n",
    "    results = test_model(model, tokenizer, test_questions)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nSample Generated Answers:\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(f\"Question: {result['question']['Instruction']}\")\n",
    "        print(f\"Word: {result['question']['Word']}\")\n",
    "        print(f\"Generated Answer: {result['generated']}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Clean memory before starting\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    main()\n",
    "#model, tokenizer= train_gemma()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610e0be-f4f8-48bb-9e14-84ab13693ec1",
   "metadata": {},
   "source": [
    "## Load the trained Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc3e114-4a74-47f9-aed2-6b8a43c229a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HF_TOKEN = \"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\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import openai\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d77705f-a787-47d1-abc5-58b37a184d60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8cf3c7689b74de8a9b4e0ccf81a9a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapters...\n",
      "Loading tokenizer...\n",
      "Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14008ce65457415a945819474d98067e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapters...\n",
      "\n",
      "=== Question #1 === Question Type #1 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #1 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #1 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #1 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #1 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #1 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #2 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #2 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #2 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #2 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #2 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #2 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #3 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #3 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #3 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #3 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #3 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #3 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #4 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #4 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #4 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #4 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #4 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #4 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #5 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #5 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #5 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #5 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #5 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #5 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #6 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #6 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #6 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #6 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #6 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #6 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #7 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #7 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #7 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #7 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #7 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #7 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #8 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #8 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #8 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #8 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #8 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #8 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #9 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #9 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #9 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #9 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #9 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #9 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #10 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #10 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #10 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #10 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #10 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #10 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #11 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #11 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #11 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #11 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #11 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #11 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #12 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #12 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #12 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #12 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #12 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #12 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #13 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #13 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #13 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #13 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #13 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #13 === few_shot ===word difficulty:3======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #1 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #1 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #1 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #1 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #1 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #1 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #2 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #2 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #2 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #2 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #2 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #2 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #3 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #3 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #3 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #3 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #3 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #3 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #4 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #4 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #4 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #4 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #4 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #4 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #5 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #5 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #5 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #5 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #5 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #5 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #6 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #6 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #6 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #6 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #6 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #6 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #7 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #7 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #7 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #7 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #7 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #7 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #8 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #8 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #8 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #8 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #8 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #8 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #9 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #9 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #9 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #9 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #9 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #9 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #10 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #10 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #10 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #10 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #10 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #10 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #11 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #11 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #11 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #11 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #11 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #11 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #12 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #12 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #12 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #12 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #12 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #12 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #13 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #13 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #13 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #13 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #13 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #13 === few_shot ===word difficulty:4======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #1 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #1 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #1 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #1 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #1 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #1 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #2 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #2 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #2 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #2 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #2 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #2 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #3 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #3 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #3 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #3 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #3 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #3 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #4 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #4 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #4 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #4 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #4 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #4 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #5 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #5 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #5 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #5 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #5 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #5 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #6 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #6 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #6 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #6 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #6 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #6 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #7 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #7 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #7 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #7 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #7 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #7 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #8 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #8 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #8 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #8 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #8 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #8 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #9 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #9 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #9 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #9 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #9 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #9 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #10 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #10 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #10 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #10 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #10 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #10 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #11 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #11 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #11 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #11 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #11 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #11 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #12 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #12 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #12 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #12 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #12 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #12 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #1 === Question Type #13 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #2 === Question Type #13 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #3 === Question Type #13 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #4 === Question Type #13 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #5 === Question Type #13 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "=== Question #6 === Question Type #13 === few_shot ===word difficulty:5======task difficulty: Hard\n",
      "\n",
      "Saved questions for strategy 'few_shot' to Final_Generated_Questions_Gemma_Part10_few_shot.csv\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 3. Load pretrained model first word utility\n",
    "###############################################################################\n",
    "\n",
    "def load_gemma_model(model_path, base_model_name=\"last_gemma_morphology_20250404_014910/final_model\", use_lora=True):\n",
    "    \"\"\"Load the trained model and tokenizer\"\"\"\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    print(\"Loading base model...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        token=HF_TOKEN,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    if use_lora:\n",
    "        print(\"Loading LoRA adapters...\")\n",
    "        from peft import PeftModel\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    else:\n",
    "        model = base_model\n",
    "\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 1. Configure OpenAI\n",
    "###############################################################################\n",
    "# Replace this with a safer method (e.g., environment variable) in production\n",
    "# openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "def test_api_access():\n",
    "    \"\"\"\n",
    "    Attempts to list OpenAI models to confirm that the API key is valid.\n",
    "    Prints a success or failure message, along with a list of available models if successful.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        models = openai.Model.list()\n",
    "        # print(\"Access to OpenAI API successful! Available models:\")\n",
    "        # for model in models['data']:\n",
    "        #     print(f\" - {model['id']}\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to access the OpenAI API:\")\n",
    "        print(e)\n",
    "\n",
    "###############################################################################\n",
    "# 2. Core LLM function (using OpenAI GPT models)\n",
    "###############################################################################\n",
    "\n",
    "def load_gemma_model(model_path, base_model_name=\"gemma_morphology/final_model\", use_lora=True):\n",
    "    \"\"\"Load the trained model and tokenizer\"\"\"\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    print(\"Loading base model...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        token=HF_TOKEN,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    if use_lora:\n",
    "        print(\"Loading LoRA adapters...\")\n",
    "        from peft import PeftModel\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    else:\n",
    "        model = base_model\n",
    "\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_gpt(prompt, model, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Generate text using the pre-trained Gemma model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generation parameters\n",
    "        generation_config = {\n",
    "            'max_new_tokens': 200,\n",
    "            'do_sample': True,\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.92,\n",
    "            'top_k': 50,\n",
    "            'repetition_penalty': 1.2,\n",
    "            'pad_token_id': tokenizer.pad_token_id,\n",
    "            'eos_token_id': tokenizer.eos_token_id,\n",
    "        }\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                **generation_config\n",
    "            )\n",
    "\n",
    "        # Decode and return\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Remove the prompt from the generated text\n",
    "        if prompt in generated_text:\n",
    "            generated_text = generated_text[len(prompt):].strip()\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_gemma: {e}\")\n",
    "        return None\n",
    "    #blue/babajani.a/babak.ahmadi/NLP_Dorr/Project/MA/MC_data_MMA.csv\n",
    "    \n",
    "model_path=\"/blue/babajani.a/babak.ahmadi/NLP_Dorr/Project/MA/last_gemma_morphology_20250403_172610/final_model\"\n",
    "#blue/babajani_directory/babak.ahmadi/NLP_Dorr/Project/MA/gemma_morphology_20250403_164502/final_model\"\n",
    "    #model_path=\"/blue/cai6307/EduGen/gemma_few_shot_20250327_162345/final_model\"\n",
    "    #model_path = \"gemma_morphology_20250402_025445/final_model\"  # Update this path\n",
    "model, tokenizer = load_gemma_model(model_path)\n",
    "\n",
    "def generate_gpat(prompt, model_name=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Sends a prompt to the OpenAI GPT (ChatCompletion) and returns the response text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message[\"content\"]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with the GPT call: {e}\")\n",
    "        return None\n",
    "\n",
    "###############################################################################\n",
    "# 3. Extract first word utility\n",
    "###############################################################################\n",
    "def extract_word_from_response(response_text):\n",
    "    \"\"\"\n",
    "    Extracts and returns the first word-like token from the response_text.\n",
    "    Returns None if no valid word is found or if response_text is None.\n",
    "    \"\"\"\n",
    "    if response_text is None:\n",
    "        return None\n",
    "    cleaned_text = re.sub(r\"[^\\w'-]\", \" \", response_text)\n",
    "    word = re.search(r'\\b\\w+\\b', cleaned_text)\n",
    "    return word.group(0) if word else None\n",
    "\n",
    "###############################################################################\n",
    "# 4. Few-shot example formatter\n",
    "###############################################################################\n",
    "def prompt_few_shot(data, Question_Type, num_examples):\n",
    "    \"\"\"\n",
    "    Filters the dataset by Question_Type, samples up to num_examples,\n",
    "    and formats them as example references to be appended to the main prompt.\n",
    "    \"\"\"\n",
    "    filtered_df = data[data.iloc[:, 0] == Question_Type]\n",
    "    if filtered_df.empty:\n",
    "        return \"\"\n",
    "    else:\n",
    "        examples = filtered_df.sample(n=min(num_examples, len(filtered_df)))\n",
    "        formatted_examples = \"There are few examples, please do not use them on the generated questions. \\n\"\n",
    "        for _, row in examples.iterrows():\n",
    "            correct_choice = row[f\"Choice_{row['Correct_Answer']}\"]\n",
    "            formatted_examples += f\"For Example:\\n\"\n",
    "            formatted_examples += f\"Question: {row['Instruction']}\\n\"\n",
    "            formatted_examples += f\"A) {row['Choice_1']}\\n\"\n",
    "            formatted_examples += f\"B) {row['Choice_2']}\\n\"\n",
    "            formatted_examples += f\"C) {row['Choice_3']}\\n\"\n",
    "            formatted_examples += f\"Correct Answer: {correct_choice}\\n\"\n",
    "            formatted_examples += (\n",
    "                f\"Explanation: Task difficulty of this question is {row['Task_Difficulty']}, \"\n",
    "                f\"and word difficulty of this question is {row['Word_Difficulty']}\\n\\n\"\n",
    "                f\"This is few_shot examples, generate different questions from these examples\\n\\n\"\n",
    "            )\n",
    "        return formatted_examples\n",
    "\n",
    "###############################################################################\n",
    "# 5. Chain-of-thought (single-prompt)\n",
    "###############################################################################\n",
    "def prompt_chain_of_thought():\n",
    "    \"\"\"\n",
    "    Appends a general chain-of-thought instruction.\n",
    "    No CSV examples are usedjust an instruction telling the model\n",
    "    to 'think aloud' before finalizing the question.\n",
    "    \"\"\"\n",
    "    # Ensuring we keep the final question in a parseable format\n",
    "    chain_instruction = f\"\"\"\n",
    "--- Chain of Thought ---\n",
    "Please think aloud, and provide your reasoning before providing the final 3-choice question.\n",
    "Include your reasoning in the final output as well.\n",
    "\n",
    "Finally, PRESENT the final question in this format:\n",
    "Question: [your question]\n",
    "A) [option A]\n",
    "B) [option B]\n",
    "C) [option C]\n",
    "Correct Answer: [the correct choice]\n",
    "\"\"\"\n",
    "    return chain_instruction\n",
    "\n",
    "###############################################################################\n",
    "# 6A. \"Fake\" single-prompt chain_of_thought_plus_sequential\n",
    "###############################################################################\n",
    "def prompt_chain_of_thought_plus_sequential(question_type, word_difficulty, task_difficulty):\n",
    "    \"\"\"\n",
    "    A multi-step chain-of-thought approach. The final prompt instructs the model\n",
    "    to create the MCQ in 3 steps (selecting words, drafting a question, adding distractors),\n",
    "    each time showing its chain-of-thought reasoning.\n",
    "    \"\"\"\n",
    "    multi_step_instructions = f\"\"\"\n",
    "--- Chain of Thought + Sequential Steps ---\n",
    "\n",
    "We want a 3-choice question for question_type={question_type}.\n",
    "Word difficulty = {word_difficulty}, Task difficulty = {task_difficulty}.\n",
    "\n",
    "Please follow these steps in your final output (all in one go):\n",
    "\n",
    "Step 1: List three suitable Grade 3-5 words that illustrate the morphological concept\n",
    "         (prefix, suffix, root, etc. depending on question_type).\n",
    "         Show reasoning why each word is appropriate.\n",
    "         Then select exactly ONE of them.\n",
    "\n",
    "Step 2: Using the single selected word, generate a DRAFT 3-choice question.\n",
    "        Provide chain-of-thought: i.e., explain your reasoning for how the question is framed.\n",
    "\n",
    "Step 3: Add one correct answer choice and two distractors.\n",
    "        Provide chain-of-thought for how each distractor might trick the student,\n",
    "        and confirm which is correct.\n",
    "\n",
    "Finally, PRESENT the final question in this format:\n",
    "Question: [your question]\n",
    "A) [option A]\n",
    "B) [option B]\n",
    "C) [option C]\n",
    "Correct Answer: [the correct choice]\n",
    "\n",
    "Be explicit with your chain-of-thought reasoning for each step,\n",
    "but ensure the final output ends with the standard question format shown above.\n",
    "    \"\"\"\n",
    "    return multi_step_instructions\n",
    "\n",
    "\n",
    "def parse_chosen_word(response_text):\n",
    "    \"\"\"\n",
    "    Try to parse the chosen word from the multi-step response text.\n",
    "    We look for specific cues like 'Final word:' or 'Chosen word:' or\n",
    "    text like 'I would choose \"XYZ\".'\n",
    "    If none of these are found, we fall back to a naive approach.\n",
    "    \"\"\"\n",
    "    if not response_text:\n",
    "        return None\n",
    "\n",
    "    # 0) Look for something like: Final word choice: XYZ\n",
    "    match = re.search(r'(?i)final word choice:\\s*([A-Za-z\\'\\-]+)', response_text)\n",
    "    if match:\n",
    "        return match.group(1)    \n",
    "        \n",
    "    # 1) Look for something like: Final word: XYZ\n",
    "    match = re.search(r'(?i)final word:\\s*([A-Za-z\\'\\-]+)', response_text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    # 2) Look for something like: Chosen word: XYZ\n",
    "    match = re.search(r'(?i)chosen word:\\s*([A-Za-z\\'\\-]+)', response_text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    # 3) Look for: I would choose \"XYZ\"\n",
    "    match = re.search(r'(?i)i would choose\\s+\"([^\"]+)\"', response_text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    # 4) Look for: I choose \"XYZ\"\n",
    "    match = re.search(r'(?i)i choose\\s+\"([^\"]+)\"', response_text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    # 5) As a last resort, do a naive approach: first alphabetic word.\n",
    "    cleaned_text = re.sub(r\"[^\\w'-]\", \" \", response_text)\n",
    "    first_word = re.search(r'\\b[a-zA-Z\\'\\-]+\\b', cleaned_text)\n",
    "    return first_word.group(0) if first_word else None\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6B. REAL multi-step approach: chain_of_thought_plus_sequential_rl\n",
    "###############################################################################\n",
    "def prompt_chain_of_thought_plus_sequential_rl(question_type, word_difficulty, task_difficulty,model, tokenizer,forbidden_list=None, **kwargs):\n",
    "    \"\"\"\n",
    "    TRUE multi-step approach that calls GPT multiple times.\n",
    "\n",
    "    Step 0 (New): We prepend an instruction prompt based on question_type.\n",
    "    Step 1: We ask GPT for 3 suitable words + chain-of-thought, then parse out\n",
    "            the chosen word.\n",
    "    Step 2: We feed that chosen word to GPT, ask for a draft 3-choice question\n",
    "            (with chain-of-thought).\n",
    "    Step 3: We ask GPT to add distractors & finalize the parseable question.\n",
    "\n",
    "    Returns (final_text, the_word).\n",
    "    \"\"\"\n",
    "        \n",
    "    word_list = kwargs.get(\"word_list\", None)\n",
    "    if question_type in [4, 5] and word_list is None:\n",
    "        raise ValueError(\"word_list is required for question_type 4 and 5\")\n",
    "\n",
    "\n",
    "    if forbidden_list is None:\n",
    "        forbidden_list = []\n",
    "        \n",
    "    # ---------------------------\n",
    "    # Define all question prompts\n",
    "    # ---------------------------\n",
    "    prompt_instruction_qt1 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about prefixes. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to identify the prefix in a chosen word and provide \"\n",
    "        f\"two incorrect choices along with the correct answer.\"\n",
    "    )\n",
    "    prompt_instruction_qt2 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about suffixes. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to identify the suffix in the chosen word \"\n",
    "        f\"and provide two incorrect choices along with the correct answer. \"\n",
    "    )\n",
    "    prompt_instruction_qt3 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about root words. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to identify the root word in the chosen word \"\n",
    "        f\"and provide two incorrect choices along with the correct answer. \"\n",
    "    )\n",
    "    prompt_instruction_qt4 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about morphemes. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to identify the word that does NOT share \"\n",
    "        f\"the same prefix as the others from the given words {word_list}. \"\n",
    "    )\n",
    "    prompt_instruction_qt5 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about morphemes. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to identify the word that does NOT share \"\n",
    "        f\"the same suffix as the others from the given words {word_list}. \"\n",
    "    )\n",
    "    prompt_instruction_qt6 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about word transformations. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to transform the chosen word to a new meaning, \"\n",
    "        f\"with two incorrect choices and one correct answer. \"\n",
    "    )\n",
    "    prompt_instruction_qt7 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about affixed words. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to select the correct meaning of the chosen word  \"\n",
    "        f\"from three answer choices. \"\n",
    "    )\n",
    "    # prompt_restriction used in qt8 is assumed defined elsewhere or can be set to an empty string if not needed.\n",
    "    prompt_instruction_qt8 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about spelling based on morpheme meaning\"\n",
    "        f\"{{prompt_restriction}} \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should include a word with a suffix, provide two misspelled variations \"\n",
    "        f\"and one correct spelling.\"\n",
    "    )\n",
    "    prompt_instruction_qt9 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning to break affixed words into parts. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to break the chosen word into its correct parts \"\n",
    "        f\"(prefix, root, suffix) and provide two incorrect choices along with the correct answer.\"\n",
    "    )\n",
    "    prompt_instruction_qt10 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about prefixes. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to select the correct definition of the prefix \"\n",
    "        f\"in the chosen word from three answer choices.\"\n",
    "    )\n",
    "    prompt_instruction_qt11 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about root words in affixed words. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to select the correct definition of the root word \"\n",
    "        f\"in the chosen word from three answer choices.\"\n",
    "    )\n",
    "    prompt_instruction_qt12 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about suffixes. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to select the correct definition or function of the suffix \"\n",
    "        f\"in the chosen word from three answer choices.\"\n",
    "    )\n",
    "    prompt_instruction_qt13 = (\n",
    "        f\"We want to generate a 3-choice question for a student learning about morphologically complex words. \"\n",
    "        f\"The word difficulty must be {word_difficulty} and task difficulty must be {task_difficulty}. \"\n",
    "        f\"Be informed that ultimately, the question should ask the student to select the correct definition of the chosen word \"\n",
    "        f\"based on its morphemes.\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Map question_type to the corresponding prompt instruction\n",
    "    # -------------------------------------------------------\n",
    "    instructions_map = {\n",
    "        1: prompt_instruction_qt1,\n",
    "        2: prompt_instruction_qt2,\n",
    "        3: prompt_instruction_qt3,\n",
    "        4: prompt_instruction_qt4,\n",
    "        5: prompt_instruction_qt5,\n",
    "        6: prompt_instruction_qt6,\n",
    "        7: prompt_instruction_qt7,\n",
    "        8: prompt_instruction_qt8,\n",
    "        9: prompt_instruction_qt9,\n",
    "        10: prompt_instruction_qt10,\n",
    "        11: prompt_instruction_qt11,\n",
    "        12: prompt_instruction_qt12,\n",
    "        13: prompt_instruction_qt13\n",
    "    }\n",
    "\n",
    "    # ---------------------------\n",
    "    # Retrieve the question prompt\n",
    "    # ---------------------------\n",
    "    question_prompt_instruction = instructions_map.get(\n",
    "        question_type,\n",
    "        f\"[No prompt defined for question_type={question_type}]\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ---------------------------\n",
    "    # Step 1: Pick Words\n",
    "    # ---------------------------\n",
    "    # We prepend the relevant instruction prompt here\n",
    "    if question_type not in [4, 5]:\n",
    "        step1_prompt = f\"\"\"\n",
    "    {question_prompt_instruction}\n",
    "\n",
    "    Step 1 (Pick Words):\n",
    "    Question Type = {question_type}\n",
    "    Word Difficulty = {word_difficulty}, Task Difficulty = {task_difficulty}\n",
    "\n",
    "    Please list three suitable Grade 3-5 words that fit the morphological concept\n",
    "    (prefix, suffix, root, etc.) for this Question Type: {question_type}. Have in mind that this word is going to be used for this instruction {question_prompt_instruction}. \n",
    "    Do NOT generate the whole question yet. In this step just generate and choose appropriate word considering what the question is about. Explain your chain-of-thought for each choice (why is it appropriate?).\n",
    "    Then select exactly ONE of the three as the final word, and provide reasoning for it (think aloud).\n",
    "\n",
    "    You should only consider word that does NOT share the same prefix as the others from the given words {word_list}. Have in mind that this word is going to be used for this instruction {question_prompt_instruction}. \n",
    "    Do NOT generate the whole question yet. In this step just generate and choose appropriate word considering what the question is about. Explain your chain-of-thought for each choice (why is it appropriate?).\n",
    "    Then select exactly ONE of the three as the final word, and provide reasoning for it (think aloud).\n",
    "    \"\"\"\n",
    "        step1_result = generate_gpt(step1_prompt, model, tokenizer)\n",
    "        print(f\"==> Step 1 result: {step1_result}\\n\")\n",
    "        # Just do a naive parse: find \"Chosen Word:\" or something\n",
    "        # If there's no consistent structure, we could guess or rely on a simpler approach\n",
    "        chosen_word = parse_chosen_word(step1_result)\n",
    "\n",
    "        # If we can't parse the chosen word, fallback\n",
    "        if not chosen_word:\n",
    "            chosen_word = \"mysteryWord\"\n",
    "\n",
    "        # If the chosen word is already forbidden, optionally attempt a few more tries:\n",
    "        attempts = 0\n",
    "        while chosen_word and chosen_word.lower() in [fw.lower() for fw in forbidden_list]:\n",
    "            attempts += 1\n",
    "            if attempts > 3:\n",
    "                # If GPT keeps repeating words, just force a placeholder.\n",
    "                # print(f\"Chosen word '{chosen_word}' was in forbidden_list. Change this word.\")\n",
    "                temp_prompt = f\"Chosen word '{chosen_word}' was in forbidden_list: {forbidden_list}. Change this word such that it is not in the forbidden list.\"\n",
    "                step1_result = generate_gpt(temp_prompt, model, tokenizer)\n",
    "                chosen_word = parse_chosen_word(step1_result)\n",
    "                break\n",
    "            print(f\"Chosen word '{chosen_word}' was in forbidden_list; re-asking GPT for a new word.\")\n",
    "            step1_result = generate_gpt(step1_prompt, model, tokenizer)\n",
    "            chosen_word = parse_chosen_word(step1_result)\n",
    "            if not chosen_word:\n",
    "                chosen_word = \"mysteryWord\"\n",
    "\n",
    "    else:\n",
    "        step1_prompt = f\"\"\"\n",
    "    {question_prompt_instruction}\n",
    "\n",
    "    Step 1 (Pick Words):\n",
    "    Question Type = {question_type}\n",
    "    Word Difficulty = {word_difficulty}, Task Difficulty = {task_difficulty}\n",
    "\n",
    "    Have in mind that everything should be suitable for Grade 3-5 words that fit the morphological concept\n",
    "    (prefix, suffix, root, etc.) for this Question Type: {question_type}. \n",
    "    You should only consider words that one of them does NOT share the same prefix as the others from the given words {word_list}. \n",
    "    Have in mind that this word is going to be used for this instruction {question_prompt_instruction}. \n",
    "    Do NOT generate the whole question yet. In this step just consider choosing the appropriate word considering what the question is about. \n",
    "    Explain your chain-of-thought for each choice (why is it appropriate?). \n",
    "    SKIP THIS STEP, AND MOVE FORWARD WITH STEP 2.\n",
    "    \"\"\"\n",
    "        step1_result = generate_gpt(step1_prompt, model, tokenizer)\n",
    "        print(f\"==> Step 1 result: {step1_result}\\n\")\n",
    "        # Just do a naive parse: find \"Chosen Word:\" or something\n",
    "        # If there's no consistent structure, we could guess or rely on a simpler approach\n",
    "        chosen_word = parse_chosen_word(step1_result)\n",
    "\n",
    "        # If we can't parse the chosen word, fallback\n",
    "        if not chosen_word:\n",
    "            chosen_word = \"mysteryWord\"\n",
    "\n",
    "        # If the chosen word is already forbidden, optionally attempt a few more tries:\n",
    "        attempts = 0\n",
    "        while chosen_word and chosen_word.lower() in [fw.lower() for fw in forbidden_list]:\n",
    "            attempts += 1\n",
    "            if attempts > 3:\n",
    "                # If GPT keeps repeating words, just force a placeholder.\n",
    "                # print(f\"Chosen word '{chosen_word}' was in forbidden_list. Change this word.\")\n",
    "                temp_prompt = f\"Chosen word '{chosen_word}' was in forbidden_list: {forbidden_list}. Change this word such that it is not in the forbidden list.\"\n",
    "                step1_result = generate_gpt(temp_prompt, model, tokenizer)\n",
    "                chosen_word = parse_chosen_word(step1_result)\n",
    "                break\n",
    "            print(f\"Chosen word '{chosen_word}' was in forbidden_list; re-asking GPT for a new word.\")\n",
    "            step1_result = generate_gpt(step1_prompt, model, tokenizer)\n",
    "            chosen_word = parse_chosen_word(step1_result)\n",
    "            if not chosen_word:\n",
    "                chosen_word = \"mysteryWord\"\n",
    "    # ---------------------------\n",
    "    # Step 2: Draft question\n",
    "    # ---------------------------\n",
    "    step2_prompt = f\"\"\"\n",
    "Step 2 (Draft Question):\n",
    "We have chosen the word: {chosen_word}.\n",
    "\n",
    "Now draft a 3-choice question based on the following instruction: {question_prompt_instruction}. Provide a chain-of-thought explaining\n",
    "how you formed the question, and provide the correct answer.\n",
    "\n",
    "Do NOT finalize the answer choices yet. Give the question text\n",
    "and placeholders for A/B/C. For instance:\n",
    "\"Question: ... A) ... B) ... C) ...\"\n",
    "\n",
    "\"\"\"\n",
    "    step2_result = generate_gpt(step2_prompt, model, tokenizer)\n",
    "    # We'll accept step2_result as a partial question\n",
    "    print(f\"==> Step 2 result: {step2_result}\\n\")\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Step 3: Add distractors & finalize\n",
    "    # ---------------------------\n",
    "    step3_prompt = f\"\"\"\n",
    "Step 3 (Add Choices & Finalize):\n",
    "Based on the draft question and the correct answer you provided in the previous step:\n",
    "\n",
    "{step2_result}\n",
    "\n",
    "Update the TWO distractors with reasoning, considering the specified task difficulty of: {task_difficulty}. Provide chain-of-thought\n",
    "about how each distractor might trick the student, and confirm the correct answer.\n",
    "\n",
    "Finally, shuffle the answer choices and present the final question in a parseable format:\n",
    "Question: ...\n",
    "A) ...\n",
    "B) ...\n",
    "C) ...\n",
    "Correct Answer: ...\n",
    "Do NOT provide the reasonings in the above parseable format. Write them here, after everything is done.\n",
    "\"\"\"\n",
    "    step3_result = generate_gpt(step3_prompt, model, tokenizer)\n",
    "    print(f\"==> Step 3 result: {step3_result}\\n\")\n",
    "    # The final text to parse is step3_result\n",
    "    final_text = step3_result\n",
    "    return final_text, chosen_word\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 7. NEW Strategy: chain_of_thought_plus_role_chain\n",
    "###############################################################################\n",
    "def prompt_chain_of_thought_plus_role_chain(question_type, word_difficulty, task_difficulty):\n",
    "    \n",
    "\n",
    "    role_instructions = f\"\"\"\n",
    "--- 3-Role Reasoning for a Grade 3-5 3-choice question ---\n",
    "\n",
    "We want a 3-choice question for:\n",
    "  question_type = {question_type}\n",
    "  word_difficulty = {word_difficulty}\n",
    "  task_difficulty = {task_difficulty}\n",
    "\n",
    "You should act in all of the following three roles, one by one, and think aloud in each of them (provide reasonings):\n",
    "==========\n",
    "Roles & Instructions:\n",
    "==========\n",
    "\n",
    "(1) Teacher Role\n",
    "  - Act as a Grade 35 teacher.\n",
    "  - Propose a question that is suitable for a grade 3-5 student, focusing on the morphological concept \n",
    "    (prefix, suffix, root, etc.) relevant to question_type={question_type}.\n",
    "  - Provide your chain-of-thought on how the question was formed (provide reasoning, or think aloud), \n",
    "    ensuring it's neither too trivial nor too advanced for grades 35.\n",
    "  - Then pass along your question and partial choices to the next role.\n",
    "\n",
    "(2) Student Role\n",
    "  - Act as a Grade 35 student.\n",
    "  - Read what the Teacher proposed. \n",
    "  - Comment if the question is confusing, or if any distractor is obviously incorrect.\n",
    "  - Provide your chain-of-thought as a student (provide reasoning, or think aloud).\n",
    "  - Then pass along your outputs to the next role.\n",
    "\n",
    "(3) Technometrician Role\n",
    "  - Act as a test-design specialist focusing on morphological objectives (prefix, suffix, root, etc.) \n",
    "    and checking alignment with word_difficulty={word_difficulty} & task_difficulty={task_difficulty}.\n",
    "  - Evaluate the question from both Teacher and Student roles:\n",
    "    - Are we accurately testing the morphological skill for question_type={question_type}?\n",
    "    - Are the difficulty levels appropriate?\n",
    "  - Provide final refinements if needed. Be strict if you can make the question or the distractors more aligned with what is asked.\n",
    "  - Then present the finalized question in a parseable format, exactly as follows:\n",
    "\n",
    "      Final Question: [refined question text here]\n",
    "      A) [choice A]\n",
    "      B) [choice B]\n",
    "      C) [choice C]\n",
    "      Correct Answer: [the correct choice]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return role_instructions\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 8. Prompt Generators for Each Question Type\n",
    "###############################################################################\n",
    "def generate_prefix_prompt(word_difficulty, task_difficulty, data, prompting, words, model, tokenizer):  # question_type = 1\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has a prefix{word_exclusion} \"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    # -- 1) GPT call to pick a word\n",
    "    result = generate_gpt(gen_prompt, model, tokenizer)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    # -- 2) The main MCQ prompt (Zero-shot style)\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about prefixes. \"\n",
    "        f\"The word is '{word}', with word difficulty {word_difficulty} and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to identify the prefix in '{word}' and provide \"\n",
    "        f\"two incorrect choices along with the correct answer. Please specify the correct answer.\"\n",
    "    )\n",
    "        \n",
    "    # -- 3) Append optional prompting strategy\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 1, 8)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(1, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        # Call the RL version, which returns final_text and chosen_word\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            1, word_difficulty, task_difficulty,model, tokenizer, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(1, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        # default zero-shot\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_suffix_prompt(word_difficulty, task_difficulty, data, prompting, words, model, tokenizer):  # question_type = 2\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has a suffix{word_exclusion} \"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt, model, tokenizer)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about suffixes. \"\n",
    "        f\"The word is '{word}', with word difficulty {word_difficulty} and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to identify the suffix in '{word}' and provide two \"\n",
    "        f\"incorrect choices along with the correct answer. Please specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 2, 8)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(2, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            2, word_difficulty, task_difficulty,model, tokenizer, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(2, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_root_word_prompt(word_difficulty, task_difficulty, data, prompting, words, model, tokenizer):  # question_type = 3\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has a prefix or suffix{word_exclusion} \"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt, model, tokenizer)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about root words. \"\n",
    "        f\"The word is '{word}', with word difficulty {word_difficulty} and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to identify the root word in '{word}' and provide two \"\n",
    "        f\"incorrect choices along with the correct answer. Clearly specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 3, 8)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(3, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            3, word_difficulty, task_difficulty, model, tokenizer,forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(3, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_common_prefix_prompt(word_list, word_difficulty, task_difficulty, data, prompting, model, tokenizer):  # question_type = 4\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about morphemes. \"\n",
    "        f\"The words given are {word_list}, with word difficulty {word_difficulty} and \"\n",
    "        f\"task difficulty {task_difficulty}. The question should ask the student to identify the word \"\n",
    "        f\"that does NOT share the same prefix as the others. Clearly specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 4, 8)\n",
    "        return prompt, word_list\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word_list\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(4, word_difficulty, task_difficulty)\n",
    "        return prompt, word_list\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            4, word_difficulty, task_difficulty,model, tokenizer, forbidden_list=[\"\"], word_list=word_list\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(4, word_difficulty, task_difficulty)\n",
    "        return prompt, word_list\n",
    "    else:\n",
    "        return prompt, word_list\n",
    "\n",
    "\n",
    "def generate_common_suffix_prompt(word_list, word_difficulty, task_difficulty, data, prompting, model, tokenizer):  # question_type = 5\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about morphemes. \"\n",
    "        f\"The words given are {word_list}, with word difficulty {word_difficulty} and \"\n",
    "        f\"task difficulty {task_difficulty}. The question should ask the student to identify the word \"\n",
    "        f\"that does NOT share the same suffix as the others. Clearly specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 5, 8)\n",
    "        return prompt, word_list\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word_list\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(5, word_difficulty, task_difficulty)\n",
    "        return prompt, word_list\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            5, word_difficulty, task_difficulty,model, tokenizer, forbidden_list=[\"\"], word_list=word_list\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(5, word_difficulty, task_difficulty)\n",
    "        return prompt, word_list\n",
    "    else:\n",
    "        return prompt, word_list\n",
    "\n",
    "\n",
    "def generate_word_transformation_prompt(word_difficulty, task_difficulty, data, prompting, words, model, tokenizer):  # question_type = 6\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has variations with different meanings{word_exclusion} \"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt, model, tokenizer)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about word transformations. \"\n",
    "        f\"The word is '{word}', word difficulty {word_difficulty}, and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to transform '{word}' to a new meaning, with two incorrect \"\n",
    "        f\"choices and one correct answer. Clearly specify correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 6, 8)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(6, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            6, word_difficulty, task_difficulty, model, tokenizer, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(6, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_word_meaning_prompt(word_difficulty, task_difficulty, data, prompting, words, model, tokenizer):  # question_type = 7\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has a different meaning with a same prefix or suffix{word_exclusion} \"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt, model, tokenizer)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about affixed words. \"\n",
    "        f\"The word is '{word}', with word difficulty {word_difficulty} and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to select the correct meaning of '{word}' from three answer choices. \"\n",
    "        f\"Clearly specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 7, 8)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(7, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            7, word_difficulty, task_difficulty, model, tokenizer, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(7, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_spelling_prompt(word_difficulty, task_difficulty, data, prompting, prompts, model, tokenizer):  # question_type = 8\n",
    "    if prompts:\n",
    "        prompt_restriction = (\n",
    "            f\" The question and the word should not be similar to any of the previously generated questions: \"\n",
    "            f\"{', '.join(prompts)}.\"\n",
    "        )\n",
    "    else:\n",
    "        prompt_restriction = \"\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about spelling based on morpheme meaning\"\n",
    "        f\"{prompt_restriction} The question should include a word with a suffix.\"\n",
    "        f\"Provide two misspelled variations and one correct spelling. The question should have word difficulty \"\n",
    "        f\"{word_difficulty} and task difficulty {task_difficulty}. Clearly specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 8, 8)\n",
    "        return prompt, prompt\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, prompt\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(8, word_difficulty, task_difficulty)\n",
    "        return prompt, prompt\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            8, word_difficulty, task_difficulty, model, tokenizer, forbidden_list=prompts\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(8, word_difficulty, task_difficulty)\n",
    "        return prompt, prompt\n",
    "    else:\n",
    "        return prompt, prompt\n",
    "\n",
    "\n",
    "def generate_affixed_word_breakdown_prompt(word_difficulty, task_difficulty, data, prompting, words, model, tokenizer):  # question_type = 9\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has at least three parts and at most four parts{word_exclusion} \"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt, model, tokenizer)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning to break affixed words into parts. \"\n",
    "        f\"The word is '{word}', with word difficulty {word_difficulty} and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to break '{word}' into its correct parts (prefix, root, suffix) \"\n",
    "        f\"and provide two incorrect choices along with the correct answer. Please specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 9, 8)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(9, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            9, word_difficulty, task_difficulty, model, tokenizer, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(9, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_prefix_definition_prompt(word_difficulty, task_difficulty, data, prompting, words, model, tokenizer):  # question_type = 10\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has a prefix{word_exclusion} \"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt, model, tokenizer)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about prefixes. \"\n",
    "        f\"The word is '{word}', with word difficulty {word_difficulty} and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to select the correct definition of the prefix in '{word}' from \"\n",
    "        f\"three answer choices. Please specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 10, 8)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(10, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            10, word_difficulty, task_difficulty, model, tokenizer, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(10, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_root_word_definition_prompt(word_difficulty, task_difficulty, data, prompting, words, model, tokenizer):  # question_type = 11\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has a prefix or suffix{word_exclusion} \"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt, model, tokenizer)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about root words in affixed words. \"\n",
    "        f\"The word is '{word}', with word difficulty {word_difficulty} and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to select the correct definition of the root word in '{word}' from \"\n",
    "        f\"three answer choices. Please specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 11, 8)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(11, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            11, word_difficulty, task_difficulty,model, tokenizer, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(11, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_suffix_definition_prompt(word_difficulty, task_difficulty, data, prompting, words, model, tokenizer):  # question_type = 12\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word that has a suffix{word_exclusion}\"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt, model, tokenizer)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about suffixes. \"\n",
    "        f\"The word is '{word}', with word difficulty {word_difficulty} and task difficulty {task_difficulty}. \"\n",
    "        f\"The question should ask the student to select the correct definition or function of the suffix in '{word}' \"\n",
    "        f\"from three answer choices. Please specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 12, 8)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(12, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            12, word_difficulty, task_difficulty,  model, tokenizer, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(12, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "\n",
    "def generate_morphologically_complex_word_definition_prompt(word_difficulty, task_difficulty, data, prompting, words, model, tokenizer):  # question_type = 13\n",
    "    if words:\n",
    "        word_exclusion = f\" and it must NOT be any of these words (case insensitive): {', '.join(words)}.\"\n",
    "    else:\n",
    "        word_exclusion = \"\"\n",
    "    gen_prompt = (\n",
    "        f\"Please generate an English word whose morpheme has a distinct meaning, can take a suffix or prefix,{word_exclusion}\"\n",
    "        f\"Its level of difficulty for grade 3-5 is {word_difficulty} out of 5.\\n\"\n",
    "        f\"Return your response in this exact format:\\n\"\n",
    "        f\"WORD: [your word]\\n\"\n",
    "        f\"EXPLANATION: [brief explanation why this word is appropriate]\"\n",
    "    )\n",
    "    result = generate_gpt(gen_prompt, model, tokenizer)\n",
    "    word_match = re.search(r\"WORD:\\s*(\\w+)\", result or \"\")\n",
    "    word = word_match.group(1) if word_match else None\n",
    "\n",
    "    prompt = (\n",
    "        f\"Generate a 3-choice question for a student learning about morphologically complex words. \"\n",
    "        f\"The word is '{word}'. With word difficulty {word_difficulty} and task difficulty {task_difficulty}, \"\n",
    "        f\"the question should ask the student to select the correct definition of '{word}' based on its morphemes. \"\n",
    "        f\"Please specify the correct answer.\"\n",
    "    )\n",
    "    if prompting == 'few_shot':\n",
    "        prompt += prompt_few_shot(data, 13, 8)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought':\n",
    "        prompt += prompt_chain_of_thought()\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential':\n",
    "        prompt += prompt_chain_of_thought_plus_sequential(13, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    elif prompting == 'chain_of_thought_plus_sequential_rl':\n",
    "        final_text, chosen_word = prompt_chain_of_thought_plus_sequential_rl(\n",
    "            13, word_difficulty, task_difficulty, model, tokenizer, forbidden_list=words\n",
    "        )\n",
    "        return final_text, chosen_word\n",
    "    elif prompting == 'chain_of_thought_plus_role_chain':\n",
    "        prompt += prompt_chain_of_thought_plus_role_chain(13, word_difficulty, task_difficulty)\n",
    "        return prompt, word\n",
    "    else:\n",
    "        return prompt, word\n",
    "\n",
    "###############################################################################\n",
    "# 9. Dispatcher for Zero/Few/Chain-of-thought/Chain-of-thought-plus-sequential/\n",
    "#    Chain-of-thought-plus-role-chain\n",
    "###############################################################################\n",
    "def load_lists(file):\n",
    "    import json\n",
    "    with open(file,'r') as f:\n",
    "        list1=json.load(f)\n",
    "    return list1\n",
    "def prompt_generator(question_type, word_difficulty, task_difficulty, data, prompting, forbidden_list,model, tokenizer):\n",
    "    list1=load_lists('/blue/babajani.a/babak.ahmadi/NLP_Dorr/Project/MA/JsonLists/list1.json')\n",
    "    list2=load_lists('/blue/babajani.a/babak.ahmadi/NLP_Dorr/Project/MA/JsonLists/list2.json')\n",
    "    if question_type == 1:\n",
    "        return generate_prefix_prompt(word_difficulty, task_difficulty, data, prompting,forbidden_list,model, tokenizer)\n",
    "    elif question_type == 2:\n",
    "        return generate_suffix_prompt(word_difficulty, task_difficulty, data, prompting, forbidden_list, model, tokenizer)\n",
    "    elif question_type == 3:\n",
    "        return generate_root_word_prompt(word_difficulty, task_difficulty, data, prompting, forbidden_list, model, tokenizer)\n",
    "    elif question_type == 4:\n",
    "        allowed = [sublist for sublist in list1 if sublist not in forbidden_list]\n",
    "        random_word_list = random.choice(allowed)\n",
    "        return generate_common_prefix_prompt(random_word_list, word_difficulty, task_difficulty, data, prompting, model, tokenizer)\n",
    "    elif question_type == 5:\n",
    "        allowed = [sublist for sublist in list2 if sublist not in forbidden_list]\n",
    "        random_word_list = random.choice(allowed)\n",
    "        return generate_common_suffix_prompt(random_word_list, word_difficulty, task_difficulty, data, prompting, model, tokenizer)\n",
    "    elif question_type == 6:\n",
    "        return generate_word_transformation_prompt(word_difficulty, task_difficulty, data, prompting, forbidden_list, model, tokenizer)\n",
    "    elif question_type == 7:\n",
    "        return generate_word_meaning_prompt(word_difficulty, task_difficulty, data, prompting,forbidden_list, model, tokenizer)\n",
    "    elif question_type == 8:\n",
    "        return generate_spelling_prompt(word_difficulty, task_difficulty, data, prompting,forbidden_list, model, tokenizer)\n",
    "    elif question_type == 9:\n",
    "        return generate_affixed_word_breakdown_prompt(word_difficulty, task_difficulty, data, prompting,forbidden_list, model, tokenizer)\n",
    "    elif question_type == 10:\n",
    "        return generate_prefix_definition_prompt(word_difficulty, task_difficulty, data, prompting,forbidden_list, model, tokenizer)\n",
    "    elif question_type == 11:\n",
    "        return generate_root_word_definition_prompt(word_difficulty, task_difficulty, data, prompting,forbidden_list, model, tokenizer)\n",
    "    elif question_type == 12:\n",
    "        return generate_suffix_definition_prompt(word_difficulty, task_difficulty, data, prompting, forbidden_list, model, tokenizer)\n",
    "    elif question_type == 13:\n",
    "        return generate_morphologically_complex_word_definition_prompt(word_difficulty, task_difficulty, data, prompting,forbidden_list, model, tokenizer)\n",
    "    else:\n",
    "        # Catch-all for unspecified question_type\n",
    "        return generate_definition_prompt(word_difficulty, task_difficulty, data, prompting, forbidden_list, model, tokenizer)\n",
    "\n",
    "###############################################################################\n",
    "# 10. A more comprehensive question parser\n",
    "###############################################################################\n",
    "def parse_question_output(text):\n",
    "    \"\"\"\n",
    "    A comprehensive parser for different strategies.\n",
    "\n",
    "    Returns a dict with:\n",
    "      {\n",
    "        'question': str or None,\n",
    "        'choice_a': str or None,\n",
    "        'choice_b': str or None,\n",
    "        'choice_c': str or None,\n",
    "        'correct_answer': str or None,\n",
    "        'chain_of_thought': str or None,\n",
    "        'teacher_reasoning': str or None,\n",
    "        'student_reasoning': str or None,\n",
    "        'psychometrician_reasoning': str or None,\n",
    "        'step_1': str or None,\n",
    "        'step_2': str or None,\n",
    "        'step_3': str or None\n",
    "      }\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    parsed = {\n",
    "        'question': None,\n",
    "        'choice_a': None,\n",
    "        'choice_b': None,\n",
    "        'choice_c': None,\n",
    "        'correct_answer': None,\n",
    "        'chain_of_thought': None,\n",
    "        'teacher_reasoning': None,\n",
    "        'student_reasoning': None,\n",
    "        'psychometrician_reasoning': None,\n",
    "        'step_1': None,\n",
    "        'step_2': None,\n",
    "        'step_3': None\n",
    "    }\n",
    "\n",
    "    # 1) Extract teacher/student/psychometrician if present\n",
    "    teacher_match = re.search(\n",
    "        r\"(?:Teacher\\s*:\\s*)(.*?)(?=\\n\\s*(?:Student\\s*:|Psychometrician\\s*:|Final Question\\s*:|Question\\s*:|$))\",\n",
    "        text, re.DOTALL\n",
    "    )\n",
    "    if teacher_match:\n",
    "        parsed['teacher_reasoning'] = teacher_match.group(1).strip()\n",
    "\n",
    "    student_match = re.search(\n",
    "        r\"(?:Student\\s*:\\s*)(.*?)(?=\\n\\s*(?:Teacher\\s*:|Psychometrician\\s*:|Final Question\\s*:|Question\\s*:|$))\",\n",
    "        text, re.DOTALL\n",
    "    )\n",
    "    if student_match:\n",
    "        parsed['student_reasoning'] = student_match.group(1).strip()\n",
    "\n",
    "    psych_match = re.search(\n",
    "        r\"(?:Psychometrician\\s*:\\s*|Technometrician\\s*:\\s*)(.*?)(?=\\n\\s*(?:Teacher\\s*:|Student\\s*:|Final Question\\s*:|Question\\s*:|$))\",\n",
    "        text, re.DOTALL\n",
    "    )\n",
    "    if psych_match:\n",
    "        parsed['psychometrician_reasoning'] = psych_match.group(1).strip()\n",
    "\n",
    "    # 2) Extract chain-of-thought if \"Chain of Thought\" block\n",
    "    cot_match = re.search(\n",
    "        r\"--- Chain of Thought\\s*---(.*?)(?=\\n---|\\nQuestion|\\nStep|\\Z)\",\n",
    "        text, re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    if cot_match:\n",
    "        parsed['chain_of_thought'] = cot_match.group(1).strip()\n",
    "\n",
    "    # 3) Extract step_1 / step_2 / step_3 if present\n",
    "    step1 = re.search(r\"(?:Step\\s*1\\s*\\(.*?\\)\\s*:|Step\\s*1\\s*:)(.*?)(?=Step\\s*2|$)\", text, re.DOTALL)\n",
    "    if step1:\n",
    "        parsed['step_1'] = step1.group(1).strip()\n",
    "\n",
    "    step2 = re.search(r\"(?:Step\\s*2\\s*\\(.*?\\)\\s*:|Step\\s*2\\s*:)(.*?)(?=Step\\s*3|$)\", text, re.DOTALL)\n",
    "    if step2:\n",
    "        parsed['step_2'] = step2.group(1).strip()\n",
    "\n",
    "    step3 = re.search(r\"(?:Step\\s*3\\s*\\(.*?\\)\\s*:|Step\\s*3\\s*:)(.*?)(?=(Step\\s*4|Question\\s*:|Final Question\\s*:|$))\", \n",
    "                      text, re.DOTALL)\n",
    "    if step3:\n",
    "        parsed['step_3'] = step3.group(1).strip()\n",
    "\n",
    "    # 4) Extract final question (look for \"Final Question:\" or \"Question:\")\n",
    "    final_q_match = re.search(r\"Final Question\\s*:\\s*(.*)\", text, re.DOTALL | re.IGNORECASE)\n",
    "    if final_q_match:\n",
    "        # We'll parse out until we find A) or end\n",
    "        remainder = final_q_match.group(1)\n",
    "        # The question portion ends where \"A)\" might begin\n",
    "        splitted = re.split(r\"\\nA\\)|\\nA\\) \", remainder, 1)\n",
    "        parsed['question'] = splitted[0].strip()\n",
    "    else:\n",
    "        # fallback to \"Question:\"\n",
    "        q_match = re.search(r\"Question\\s*:\\s*(.*)\", text, re.DOTALL | re.IGNORECASE)\n",
    "        if q_match:\n",
    "            remainder = q_match.group(1)\n",
    "            splitted = re.split(r\"\\nA\\)|\\nA\\) \", remainder, 1)\n",
    "            parsed['question'] = splitted[0].strip() if splitted else remainder.strip()\n",
    "\n",
    "    # 5) Extract choices A), B), C)\n",
    "    # We do multi-line capture    \n",
    "    a_match = re.search(r\"A\\)\\s*(.*?)(?=\\n[B-Z]\\)|\\nCorrect Answer:|\\Z)\", text, re.DOTALL)\n",
    "    b_match = re.search(r\"B\\)\\s*(.*?)(?=\\n[C-Z]\\)|\\nCorrect Answer:|\\Z)\", text, re.DOTALL)\n",
    "    c_match = re.search(r\"C\\)\\s*(.*?)(?=\\n[D-Z]\\)|\\nCorrect Answer:|\\Z)\", text, re.DOTALL)\n",
    "    \n",
    "    if a_match:\n",
    "        parsed['choice_a'] = a_match.group(1).strip()\n",
    "    # b_match = re.search(r\"B\\)\\s*(.*?)(?=\\n[C-Z]\\)|\\Z)\", text, re.DOTALL)\n",
    "    if b_match:\n",
    "        parsed['choice_b'] = b_match.group(1).strip()\n",
    "    # c_match = re.search(r\"C\\)\\s*(.*?)(?=\\n[D-Z]\\)|\\Z)\", text, re.DOTALL)\n",
    "    if c_match:\n",
    "        parsed['choice_c'] = c_match.group(1).strip()\n",
    "\n",
    "    # 6) Extract \"Correct Answer:\"\n",
    "    correct_match = re.search(r\"Correct\\s*Answer\\s*:\\s*(.*)\", text, re.IGNORECASE)\n",
    "    if correct_match:\n",
    "        ans = correct_match.group(1).strip()\n",
    "        # Remove leading \"A) \", \"B) \", or \"C) \" if present:\n",
    "        ans_no_label = re.sub(r'^[ABC]\\)\\s*', '', ans, flags=re.IGNORECASE).strip()\n",
    "        parsed['correct_answer'] = ans_no_label\n",
    "\n",
    "    return parsed\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"/blue/babajani.a/babak.ahmadi/NLP_Dorr/Project/MA/gemma_morphology/final_model\"\n",
    "    model, tokenizer = load_gemma_model(model_path)\n",
    "    data_file = 'MC_data_MA2.csv'\n",
    "    \n",
    "    strategies = ['chain_of_thought_plus_role_chain','chain_of_thought_plus_sequential_rl', 'chain_of_thought',\n",
    "        'chain_of_thought_plus_sequential',\n",
    "                  'few_shot','zero_shot'\n",
    "    ]\n",
    "\n",
    "    data = pd.read_csv(data_file, encoding='utf-8')\n",
    "    NUM_QUESTIONS = 6\n",
    "    word_difficulties = [1,2,3,4,5]\n",
    "    task_difficulties = ['Eeasy','Medium', 'Hard']\n",
    "    Question_Type_array = np.array(data[\"Question_Type\"].unique())\n",
    "    questio_id = np.sort([int(float(x)) for x in Question_Type_array])\n",
    "\n",
    "    for strategy in strategies:\n",
    "        questions_data = []\n",
    "        prev_prompts = {q_type: [] for q_type in questio_id}\n",
    "\n",
    "        for word_difficulty in word_difficulties:\n",
    "            for task_difficulty in task_difficulties:\n",
    "                for question_type in questio_id:\n",
    "                    for i in range(NUM_QUESTIONS):\n",
    "                        print(f\"\\n=== Question #{i+1} === Question Type #{question_type} === {strategy} ===word difficulty:{word_difficulty}======task difficulty: {task_difficulty}\")\n",
    "                        prompt_or_final_text, new_word = prompt_generator(\n",
    "                            question_type,\n",
    "                            word_difficulty,\n",
    "                            task_difficulty,\n",
    "                            data,\n",
    "                            strategy,\n",
    "                            prev_prompts[question_type],\n",
    "                            model, tokenizer\n",
    "                        )\n",
    "                        generated_text = generate_gpt(prompt_or_final_text, model, tokenizer)\n",
    "\n",
    "                        if generated_text:\n",
    "                            parsed = parse_question_output(generated_text)\n",
    "                            if parsed:\n",
    "                                parsed['question_type'] = question_type\n",
    "                                parsed['word_difficulty'] = word_difficulty\n",
    "                                parsed['task_difficulty'] = task_difficulty\n",
    "                                parsed['whole_text']=generated_text\n",
    "                                questions_data.append(parsed)\n",
    "\n",
    "                        if new_word:\n",
    "                            prev_prompts[question_type].append(new_word)\n",
    "\n",
    "                        #if generated_text:\n",
    "                        #    print(\"\\nGenerated Text from GEMMA Model:\")\n",
    "                        #    #print(generated_text)\n",
    "                        #else:\n",
    "                            #print(\"No text returned by the model.\")\n",
    "\n",
    "        # Save after finishing one strategy\n",
    "        df = pd.DataFrame(questions_data)\n",
    "        filename = f'Final_Generated_Questions_Gemma_Part10_{strategy}.csv'\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"\\nSaved questions for strategy '{strategy}' to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc98e8-ae6d-4981-9dbb-250b6d9574ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
