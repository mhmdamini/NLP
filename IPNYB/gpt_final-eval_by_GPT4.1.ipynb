{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a0a02b7-92ba-4467-9dc0-7757b41639da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "\n",
    "openai.api_key = \"sk-proj-PSXJ5xydTMPUZcHkLIuq\" # This is not a real api_key and you should replace it with your api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7091aa12-827c-4fa8-b0d7-e6cad16ef4be",
   "metadata": {},
   "source": [
    "## Models to choose from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e0b8d34-b5f2-4999-b570-cf926f36a782",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access to OpenAI API successful! Available models (sorted by date):\n",
      " - gpt-4.1-nano (created: 2025-04-10 21:48:27)\n",
      " - gpt-4.1-nano-2025-04-14 (created: 2025-04-10 21:37:05)\n",
      " - gpt-4.1-mini (created: 2025-04-10 20:49:33)\n",
      " - gpt-4.1-mini-2025-04-14 (created: 2025-04-10 20:39:07)\n",
      " - gpt-4.1 (created: 2025-04-10 20:22:22)\n",
      " - gpt-4.1-2025-04-14 (created: 2025-04-10 20:09:06)\n",
      " - o4-mini (created: 2025-04-09 19:02:31)\n",
      " - o4-mini-2025-04-16 (created: 2025-04-08 17:31:46)\n",
      " - gpt-4o-mini-tts (created: 2025-03-19 17:05:59)\n",
      " - gpt-4o-mini-transcribe (created: 2025-03-15 19:56:36)\n",
      " - gpt-4o-transcribe (created: 2025-03-15 19:54:23)\n",
      " - gpt-4o-mini-search-preview (created: 2025-03-07 23:46:01)\n",
      " - gpt-4o-mini-search-preview-2025-03-11 (created: 2025-03-07 23:40:58)\n",
      " - gpt-4o-search-preview (created: 2025-03-07 23:05:20)\n",
      " - gpt-4o-search-preview-2025-03-11 (created: 2025-03-07 22:56:10)\n",
      " - gpt-4.5-preview-2025-02-27 (created: 2025-02-27 02:28:24)\n",
      " - gpt-4.5-preview (created: 2025-02-27 02:24:19)\n",
      " - gpt-4o-2024-11-20 (created: 2025-02-12 03:39:03)\n",
      " - o3-mini-2025-01-31 (created: 2025-01-27 20:36:40)\n",
      " - o3-mini (created: 2025-01-17 20:39:43)\n",
      " - gpt-4o-mini-audio-preview (created: 2024-12-16 22:17:04)\n",
      " - gpt-4o-mini-realtime-preview (created: 2024-12-16 22:16:20)\n",
      " - o1 (created: 2024-12-16 19:03:36)\n",
      " - o1-2024-12-17 (created: 2024-12-16 05:29:36)\n",
      " - gpt-4o-mini-audio-preview-2024-12-17 (created: 2024-12-13 18:52:00)\n",
      " - gpt-4o-mini-realtime-preview-2024-12-17 (created: 2024-12-13 17:56:41)\n",
      " - gpt-4o-audio-preview-2024-12-17 (created: 2024-12-12 20:10:39)\n",
      " - gpt-4o-realtime-preview-2024-12-17 (created: 2024-12-11 19:30:30)\n",
      " - omni-moderation-2024-09-26 (created: 2024-11-27 19:07:46)\n",
      " - omni-moderation-latest (created: 2024-11-15 16:47:45)\n",
      " - gpt-4o-realtime-preview (created: 2024-09-30 01:33:18)\n",
      " - gpt-4o-audio-preview (created: 2024-09-27 18:07:23)\n",
      " - gpt-4o-audio-preview-2024-10-01 (created: 2024-09-26 22:17:22)\n",
      " - gpt-4o-realtime-preview-2024-10-01 (created: 2024-09-23 22:49:26)\n",
      " - o1-mini (created: 2024-09-06 18:56:48)\n",
      " - o1-mini-2024-09-12 (created: 2024-09-06 18:56:19)\n",
      " - o1-preview (created: 2024-09-06 18:54:57)\n",
      " - o1-preview-2024-09-12 (created: 2024-09-06 18:54:25)\n",
      " - chatgpt-4o-latest (created: 2024-08-13 02:12:11)\n",
      " - gpt-4o-2024-08-06 (created: 2024-08-04 23:38:39)\n",
      " - gpt-4o-mini (created: 2024-07-16 23:32:21)\n",
      " - gpt-4o-mini-2024-07-18 (created: 2024-07-16 23:31:57)\n",
      " - gpt-4o-2024-05-13 (created: 2024-05-10 19:08:52)\n",
      " - gpt-4o (created: 2024-05-10 18:50:49)\n",
      " - gpt-4-turbo-2024-04-09 (created: 2024-04-08 18:41:17)\n",
      " - gpt-4-turbo (created: 2024-04-05 23:57:21)\n",
      " - gpt-3.5-turbo-0125 (created: 2024-01-23 22:19:18)\n",
      " - gpt-4-turbo-preview (created: 2024-01-23 19:22:57)\n",
      " - gpt-4-0125-preview (created: 2024-01-23 19:20:12)\n",
      " - text-embedding-3-large (created: 2024-01-22 19:53:00)\n",
      " - text-embedding-3-small (created: 2024-01-22 18:43:17)\n",
      " - tts-1-hd-1106 (created: 2023-11-03 23:18:53)\n",
      " - tts-1-1106 (created: 2023-11-03 23:14:01)\n",
      " - tts-1-hd (created: 2023-11-03 21:13:35)\n",
      " - gpt-3.5-turbo-1106 (created: 2023-11-02 21:15:48)\n",
      " - gpt-4-1106-preview (created: 2023-11-02 20:33:26)\n",
      " - dall-e-2 (created: 2023-11-01 00:22:57)\n",
      " - dall-e-3 (created: 2023-10-31 20:46:29)\n",
      " - gpt-3.5-turbo-instruct-0914 (created: 2023-09-07 21:34:32)\n",
      " - gpt-3.5-turbo-instruct (created: 2023-08-24 18:23:47)\n",
      " - babbage-002 (created: 2023-08-21 16:16:55)\n",
      " - davinci-002 (created: 2023-08-21 16:11:41)\n",
      " - gpt-4 (created: 2023-06-27 16:13:31)\n",
      " - gpt-4-0613 (created: 2023-06-12 16:54:56)\n",
      " - gpt-3.5-turbo-16k (created: 2023-05-10 22:35:02)\n",
      " - tts-1 (created: 2023-04-19 21:49:11)\n",
      " - gpt-3.5-turbo (created: 2023-02-28 18:56:42)\n",
      " - whisper-1 (created: 2023-02-27 21:13:04)\n",
      " - text-embedding-ada-002 (created: 2022-12-16 19:01:39)\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from datetime import datetime\n",
    "\n",
    "def test_api_access():\n",
    "    \"\"\"\n",
    "    Attempts to list OpenAI models to confirm that the API key is valid.\n",
    "    Prints a success or failure message, along with a list of available models sorted by creation date.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        models = openai.Model.list()\n",
    "        sorted_models = sorted(\n",
    "            models['data'], key=lambda m: m['created'], reverse=True  # Newest first\n",
    "        )\n",
    "        print(\"Access to OpenAI API successful! Available models (sorted by date):\")\n",
    "        for model in sorted_models:\n",
    "            created_time = datetime.utcfromtimestamp(model['created']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(f\" - {model['id']} (created: {created_time})\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to access the OpenAI API:\")\n",
    "        print(e)\n",
    "\n",
    "test_api_access()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a561834-1bde-40d5-9be3-7b77ea95a3b8",
   "metadata": {},
   "source": [
    "## Parse the human expert evaluation into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f11d3d6-e2f7-49de-a1ac-b6c1d74304f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 63 rows → mcq_human_evals.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parse “Human Evaluation Metrics.docx” and write mcq_human_evals.csv.\n",
    "\n",
    "• One row per annotated sample.\n",
    "• Captures question data, human scores, AND the explanatory\n",
    "  sentences for each of the five evaluation dimensions.\n",
    "\"\"\"\n",
    "\n",
    "import re, csv, sys\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from docx import Document          # pip install python-docx\n",
    "\n",
    "\n",
    "DOCX_FILE = \"Human Evaluation Metrics.docx\"\n",
    "OUT_FILE  = \"mcq_human_evals.csv\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Pull non‑empty lines out of the .docx\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    paragraphs = Document(DOCX_FILE).paragraphs\n",
    "except Exception as e:\n",
    "    sys.exit(f\"Could not open {DOCX_FILE}: {e}\")\n",
    "\n",
    "lines = [p.text.strip() for p in paragraphs if p.text.strip()]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. regex patterns\n",
    "# ---------------------------------------------------------\n",
    "QT_HEADER   = re.compile(r\"=+Question Type (\\d+)=+\")\n",
    "SAMPLE_HDR  = re.compile(r\"Sample #(\\d+) for QT\\d+\", re.I)\n",
    "\n",
    "QUESTION_RE = re.compile(r\"^(?:Question:)?\\s*(.*?\\?)\\s*$\", re.I)\n",
    "CHOICE_RE   = re.compile(r\"^Choice\\s*\\d+\\s*[:)]\\s*(.+)$\", re.I)\n",
    "CORRECT_RE  = re.compile(r\"^Correct[_ ]answer\\s*[:)]\\s*([A-D])\\)\\s*(.+)$\", re.I)\n",
    "WORD_RE     = re.compile(r\"word[_ ]difficulty\\s*[:)]\\s*(\\d+)\", re.I)\n",
    "TASK_RE     = re.compile(r\"task[_ ]difficulty\\s*[:)]\\s*([EMH])\", re.I)\n",
    "\n",
    "EVAL_START  = re.compile(r\"Evaluation for .*?Total Score:\\s*(\\d+)/5\", re.I)\n",
    "METRIC_RE   = re.compile(\n",
    "    r\"(Clarity of Instruction|Accuracy of Correct Answer|Quality of Distractors|\"\n",
    "    r\"Word Difficulty|Task Difficulty)\\s*\\((\\d)\\)\\s*:\\s*(.+)$\",\n",
    "    re.I,\n",
    ")\n",
    "\n",
    "metric_cols = {\n",
    "    \"clarity of instruction\":   (\"eval_instruction_score\",  \"eval_instruction_exp\"),\n",
    "    \"accuracy of correct answer\":(\"eval_accuracy_score\",    \"eval_accuracy_exp\"),\n",
    "    \"quality of distractors\":   (\"eval_distractors_score\",  \"eval_distractors_exp\"),\n",
    "    \"word difficulty\":          (\"eval_word_diff_score\",    \"eval_word_diff_exp\"),\n",
    "    \"task difficulty\":          (\"eval_task_diff_score\",    \"eval_task_diff_exp\"),\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. streaming parse\n",
    "# ---------------------------------------------------------\n",
    "records   = []\n",
    "ctx       = defaultdict(lambda: None)\n",
    "\n",
    "def flush():\n",
    "    \"\"\"push current ctx into records & reset\"\"\"\n",
    "    if ctx.get(\"question\"):\n",
    "        # ensure every metric column exists\n",
    "        for score_col, exp_col in metric_cols.values():\n",
    "            ctx.setdefault(score_col, 0)\n",
    "            ctx.setdefault(exp_col, \"\")\n",
    "        records.append(ctx.copy())\n",
    "        ctx.clear()\n",
    "\n",
    "current_qt = None\n",
    "\n",
    "for ln in lines:\n",
    "    # ---- section headers ----\n",
    "    if m := QT_HEADER.match(ln):\n",
    "        flush()\n",
    "        current_qt = int(m.group(1))\n",
    "        continue\n",
    "    if m := SAMPLE_HDR.match(ln):\n",
    "        flush()\n",
    "        ctx[\"question_type\"] = current_qt\n",
    "        ctx[\"sample_number\"] = int(m.group(1))\n",
    "        continue\n",
    "\n",
    "    # ---- question / choices ----\n",
    "    if m := QUESTION_RE.match(ln):\n",
    "        ctx[\"question\"] = m.group(1).strip()\n",
    "        continue\n",
    "    if m := CHOICE_RE.match(ln):\n",
    "        key = f\"choice_{len([k for k in ctx if k.startswith('choice_')]) + 1}\"\n",
    "        ctx[key] = m.group(1).strip()\n",
    "        continue\n",
    "    if m := CORRECT_RE.match(ln):\n",
    "        ctx[\"correct_answer_letter\"] = m.group(1)\n",
    "        ctx[\"correct_answer_text\"]   = m.group(2).strip()\n",
    "        continue\n",
    "\n",
    "    # ---- misc attributes ----\n",
    "    if m := WORD_RE.search(ln):\n",
    "        ctx[\"word_difficulty\"] = int(m.group(1))\n",
    "    if m := TASK_RE.search(ln):\n",
    "        ctx[\"task_difficulty\"] = m.group(1).upper()\n",
    "\n",
    "    # ---- evaluation block ----\n",
    "    if m := EVAL_START.match(ln):\n",
    "        ctx[\"eval_total_score\"] = int(m.group(1))\n",
    "        # seed blank metric fields\n",
    "        for score_col, exp_col in metric_cols.values():\n",
    "            ctx[score_col] = 0\n",
    "            ctx[exp_col]   = \"\"\n",
    "        continue\n",
    "\n",
    "    # ---- individual metric lines ----\n",
    "    if m := METRIC_RE.match(ln):\n",
    "        label      = m.group(1).lower()\n",
    "        score      = int(m.group(2))\n",
    "        explanation= m.group(3).strip()\n",
    "        score_col, exp_col = metric_cols[label]\n",
    "        ctx[score_col] = score\n",
    "        ctx[exp_col]   = explanation\n",
    "        continue\n",
    "\n",
    "# final sample\n",
    "flush()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. normalise choice columns\n",
    "# ---------------------------------------------------------\n",
    "max_choices = max(int(k.split(\"_\")[1])\n",
    "                  for r in records for k in r if k.startswith(\"choice_\"))\n",
    "for r in records:\n",
    "    for i in range(1, max_choices + 1):\n",
    "        r.setdefault(f\"choice_{i}\", \"\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. write csv\n",
    "# ---------------------------------------------------------\n",
    "base_cols = [\"question_type\", \"sample_number\", \"question\"] + \\\n",
    "            [f\"choice_{i}\" for i in range(1, max_choices + 1)] + \\\n",
    "            [\"correct_answer_letter\", \"correct_answer_text\",\n",
    "             \"word_difficulty\", \"task_difficulty\", \"eval_total_score\"]\n",
    "\n",
    "metric_cols_flat = []\n",
    "for score_col, exp_col in metric_cols.values():\n",
    "    metric_cols_flat.extend([score_col, exp_col])\n",
    "\n",
    "fieldnames = base_cols + metric_cols_flat\n",
    "\n",
    "with open(OUT_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    csv.DictWriter(f, fieldnames=fieldnames).writeheader()\n",
    "    csv.DictWriter(f, fieldnames=fieldnames).writerows(records)\n",
    "\n",
    "print(f\"Wrote {len(records)} rows → {OUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985bc2a6-32ff-4980-9875-a8b221ae17f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation pipeline:  GPT‑4.5 rates GPT‑3.5‑generated MCQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69835f2c-1aed-4f58-88af-0ab38af0fb72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [03:05<00:00,  9.76s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gpt45_evaluations.csv'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# ---------- file paths ----------\n",
    "HUMAN_FILE = \"mcq_human_evals.csv\"\n",
    "ITEM_FILE  = \"Final_generated_questions_GPT_chain_of_thought_plus_sequential_rl_First20.csv\"\n",
    "OUT_FILE   = \"gpt45_evaluations.csv\"\n",
    "\n",
    "MODEL       = \"gpt-4.5-preview-2025-02-27\"\n",
    "TEMPERATURE = 0.0\n",
    "# DELAY_SEC   = 0.4           # polite pacing; tweak if you like\n",
    "\n",
    "# load_dotenv()\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# assert openai.api_key, \"Set OPENAI_API_KEY env‑var or .env!\"\n",
    "\n",
    "\n",
    "# load CSVs\n",
    "items_df = pd.read_csv(ITEM_FILE,  dtype=str)\n",
    "human_df = pd.read_csv(HUMAN_FILE, dtype=str)\n",
    "\n",
    "human_df[\"question_type\"] = human_df[\"question_type\"].astype(int)\n",
    "items_df[\"question_type\"] = items_df[\"question_type\"].astype(int)\n",
    "\n",
    "\n",
    "# %% Cell 2b — formatting helpers\n",
    "RUBRIC_TEXT = \"\"\"\n",
    "Rate the item on **five binary metrics**.  \n",
    "Give 1 = meets criterion, 0 = does not.\n",
    "\n",
    "1. Instruction Clarity  \n",
    "2. Accuracy of Correct Answer  \n",
    "3. Quality of Distractors  \n",
    "4. Word Difficulty Appropriateness  \n",
    "5. Task Difficulty Alignment  \n",
    "\n",
    "Respond **only** with JSON:\n",
    "\n",
    "{\n",
    "  \"instr_score\": 0/1, \"instr_exp\": \"...\",\n",
    "  \"acc_score\":   0/1, \"acc_exp\": \"...\",\n",
    "  \"dist_score\":  0/1, \"dist_exp\": \"...\",\n",
    "  \"word_score\":  0/1, \"word_exp\": \"...\",\n",
    "  \"task_score\":  0/1, \"task_exp\": \"...\",\n",
    "  \"total_score\": 0‑5\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def fmt_human_example(row) -> str:\n",
    "    choices = \"\\n\".join(\n",
    "        [f\"{chr(64+i)}) {row[f'choice_{i}']}\"\n",
    "         for i in range(1,5) if str(row.get(f\"choice_{i}\", \"\")).strip()]\n",
    "    )\n",
    "    lines = [\n",
    "        \"### Human‑rated Example\",\n",
    "        f\"Question: {row['question']}\",\n",
    "        choices,\n",
    "        f\"Correct Answer: {row['correct_answer_text']}\",\n",
    "        f\"word_difficulty={row['word_difficulty']}  task_difficulty={row['task_difficulty']}\",\n",
    "        \"Scores:\",\n",
    "        f\"- Instruction Clarity: {row['eval_instruction_score']} ({row['eval_instruction_exp']})\",\n",
    "        f\"- Accuracy of Correct Answer: {row['eval_accuracy_score']} ({row['eval_accuracy_exp']})\",\n",
    "        f\"- Quality of Distractors: {row['eval_distractors_score']} ({row['eval_distractors_exp']})\",\n",
    "        f\"- Word Difficulty Appropriateness: {row['eval_word_diff_score']} ({row['eval_word_diff_exp']})\",\n",
    "        f\"- Task Difficulty Alignment: {row['eval_task_diff_score']} ({row['eval_task_diff_exp']})\",\n",
    "        \"### End Example\\n\"\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def fmt_item(row) -> str:\n",
    "    choices = \"\\n\".join([f\"A) {row['choice_a']}\",\n",
    "                         f\"B) {row['choice_b']}\",\n",
    "                         f\"C) {row['choice_c']}\"])\n",
    "    return (\n",
    "        \"### Item to evaluate\\n\"\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"{choices}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']}  \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\"\n",
    "    )\n",
    "\n",
    "def make_prompt(qtype: int, item_row) -> list:\n",
    "    ex_block = \"\\n\".join(\n",
    "        fmt_human_example(r) for _, r\n",
    "        in human_df.query(\"question_type == @qtype\").iterrows()\n",
    "    )\n",
    "    user = (\n",
    "        f\"{ex_block}\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "        f\"{fmt_item(item_row)}\\n\"\n",
    "        f\"{RUBRIC_TEXT}\"\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a meticulous K‑12 morphology test reviewer.\"},\n",
    "        {\"role\": \"user\", \"content\": user}\n",
    "    ]\n",
    "\n",
    "# %% evaluate\n",
    "results = []\n",
    "\n",
    "for idx, item in tqdm(items_df.iterrows(), total=len(items_df)):\n",
    "    prompt_msgs = make_prompt(item.question_type, item)\n",
    "\n",
    "    try:\n",
    "        resp = openai.ChatCompletion.create(\n",
    "            model=MODEL,\n",
    "            temperature=TEMPERATURE,\n",
    "            messages=prompt_msgs\n",
    "        )\n",
    "        content = resp.choices[0].message[\"content\"].strip()\n",
    "        scores  = json.loads(content)          # fail here if JSON bad\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] row {idx} failed: {e}\")\n",
    "        scores = {k: None for k in [\n",
    "            \"instr_score\",\"instr_exp\",\"acc_score\",\"acc_exp\",\n",
    "            \"dist_score\",\"dist_exp\",\"word_score\",\"word_exp\",\n",
    "            \"task_score\",\"task_exp\",\"total_score\"\n",
    "        ]}\n",
    "    merged = item.to_dict()\n",
    "    merged.update(scores)\n",
    "    results.append(merged)\n",
    "    time.sleep(DELAY_SEC)\n",
    "\n",
    "eval_df = pd.DataFrame(results)\n",
    "eval_df.to_csv(OUT_FILE, index=False)\n",
    "OUT_FILE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcff56d-98b7-4de1-8b8f-5cf3a17ac4bb",
   "metadata": {},
   "source": [
    "### faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b726bcfb-d252-4847-afc9-64a56cd07ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202d97eb2b80448baf59fd163cefc8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ saved gpt45_evaluations_fast.csv\n"
     ]
    }
   ],
   "source": [
    "import os, json, asyncio\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import openai\n",
    "\n",
    "# ---------- file paths ----------\n",
    "HUMAN_FILE = \"mcq_human_evals.csv\"\n",
    "ITEM_FILE  = \"Final_generated_questions_GPT_chain_of_thought_plus_sequential_rl_First20.csv\"\n",
    "OUT_FILE   = \"gpt45_evaluations_fast.csv\"\n",
    "\n",
    "MODEL       = \"gpt-4.5-preview-2025-02-27\"\n",
    "TEMPERATURE = 0.0\n",
    "CONCURRENT  = 10          # parallel requests—adjust for your quota\n",
    "\n",
    "\n",
    "# %% Cell 2\n",
    "items_df = pd.read_csv(ITEM_FILE,  dtype=str)\n",
    "human_df = pd.read_csv(HUMAN_FILE, dtype=str)\n",
    "\n",
    "human_df[\"question_type\"] = human_df[\"question_type\"].astype(int)\n",
    "items_df[\"question_type\"] = items_df[\"question_type\"].astype(int)\n",
    "\n",
    "# ---------- helper to pretty‑print a single human example ----------\n",
    "def fmt_human_example(row) -> str:\n",
    "    choices = \"\\n\".join(\n",
    "        f\"{chr(64+i)}) {row[f'choice_{i}']}\"\n",
    "        for i in range(1,5)\n",
    "        if str(row.get(f\"choice_{i}\",\"\")).strip()\n",
    "    )\n",
    "    return (\n",
    "        \"### Human‑rated Example\\n\"\n",
    "        f\"Question: {row['question']}\\n{choices}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer_text']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\"\n",
    "        \"Scores:\\n\"\n",
    "        f\"- Instruction Clarity: {row['eval_instruction_score']} ({row['eval_instruction_exp']})\\n\"\n",
    "        f\"- Accuracy of Correct Answer: {row['eval_accuracy_score']} ({row['eval_accuracy_exp']})\\n\"\n",
    "        f\"- Quality of Distractors: {row['eval_distractors_score']} ({row['eval_distractors_exp']})\\n\"\n",
    "        f\"- Word Difficulty Appropriateness: {row['eval_word_diff_score']} ({row['eval_word_diff_exp']})\\n\"\n",
    "        f\"- Task Difficulty Alignment: {row['eval_task_diff_score']} ({row['eval_task_diff_exp']})\\n\"\n",
    "        \"### End Example\\n\"\n",
    "    )\n",
    "\n",
    "# ---------- build example block once per question_type ----------\n",
    "EXAMPLE_BLOCKS = {\n",
    "    qtype: \"\\n\".join(fmt_human_example(r) for _, r in grp.iterrows())\n",
    "    for qtype, grp in human_df.groupby(\"question_type\")\n",
    "}\n",
    "\n",
    "RUBRIC = \"\"\"\n",
    "Rate on five binary metrics (1 = meets criterion, 0 = does not) and reply **JSON only**:\n",
    "\n",
    "{\n",
    " \"instr_score\":0/1, \"instr_exp\":\"...\",\n",
    " \"acc_score\":0/1,   \"acc_exp\":\"...\",\n",
    " \"dist_score\":0/1,  \"dist_exp\":\"...\",\n",
    " \"word_score\":0/1,  \"word_exp\":\"...\",\n",
    " \"task_score\":0/1,  \"task_exp\":\"...\",\n",
    " \"total_score\":0-5\n",
    "}\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "sem = asyncio.Semaphore(CONCURRENT)   # throttle parallelism\n",
    "\n",
    "async def rate_item(idx, row):\n",
    "    user_prompt = (\n",
    "        f\"{EXAMPLE_BLOCKS[row['question_type']]}\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "        \"### Item to evaluate\\n\"\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"A) {row['choice_a']}\\n\"\n",
    "        f\"B) {row['choice_b']}\\n\"\n",
    "        f\"C) {row['choice_c']}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\\n\"\n",
    "        f\"{RUBRIC}\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a meticulous K‑12 morphology test reviewer.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    async with sem:                           # limit concurrent calls\n",
    "        try:\n",
    "            rsp = await openai.ChatCompletion.acreate(\n",
    "                model=MODEL,\n",
    "                temperature=TEMPERATURE,\n",
    "                messages=messages\n",
    "            )\n",
    "            data = json.loads(rsp.choices[0].message[\"content\"])\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] row {idx} failed: {e}\")\n",
    "            data = {k: None for k in\n",
    "                    [\"instr_score\",\"instr_exp\",\"acc_score\",\"acc_exp\",\n",
    "                     \"dist_score\",\"dist_exp\",\"word_score\",\"word_exp\",\n",
    "                     \"task_score\",\"task_exp\",\"total_score\"]}\n",
    "\n",
    "    merged = row.to_dict()\n",
    "    merged.update(data)\n",
    "    return merged\n",
    "\n",
    "\n",
    "async def main():\n",
    "    tasks   = [rate_item(i, r) for i, r in items_df.iterrows()]\n",
    "    results = [await t for t in tqdm(asyncio.as_completed(tasks),\n",
    "                                     total=len(tasks))]\n",
    "    pd.DataFrame(results).to_csv(OUT_FILE, index=False)\n",
    "    print(\"✓ saved\", OUT_FILE)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a759319-9e31-4e9d-9be3-75b531c37950",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a908634e8e3c4d5896b6d245f4919d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] row 7 failed: Expecting value: line 1 column 1 (char 0)\n",
      "✓ saved gpt45_evaluations_fast_o4_mini.csv\n"
     ]
    }
   ],
   "source": [
    "import os, json, asyncio\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import openai\n",
    "\n",
    "# ---------- file paths ----------\n",
    "HUMAN_FILE = \"mcq_human_evals.csv\"\n",
    "ITEM_FILE  = \"Final_generated_questions_GPT_chain_of_thought_plus_sequential_rl_First20.csv\"\n",
    "OUT_FILE   = \"gpt45_evaluations_fast_o4_mini.csv\"\n",
    "\n",
    "MODEL       = \"o4-mini\" # \"gpt-4.1-nano\" # \"gpt-4.5-preview-2025-02-27\"\n",
    "TEMPERATURE = 1.0\n",
    "CONCURRENT  = 10          # parallel requests—adjust for your quota\n",
    "\n",
    "\n",
    "# %% Cell 2\n",
    "items_df = pd.read_csv(ITEM_FILE,  dtype=str)\n",
    "human_df = pd.read_csv(HUMAN_FILE, dtype=str)\n",
    "\n",
    "human_df[\"question_type\"] = human_df[\"question_type\"].astype(int)\n",
    "items_df[\"question_type\"] = items_df[\"question_type\"].astype(int)\n",
    "\n",
    "# ---------- helper to pretty‑print a single human example ----------\n",
    "def fmt_human_example(row) -> str:\n",
    "    choices = \"\\n\".join(\n",
    "        f\"{chr(64+i)}) {row[f'choice_{i}']}\"\n",
    "        for i in range(1,5)\n",
    "        if str(row.get(f\"choice_{i}\",\"\")).strip()\n",
    "    )\n",
    "    return (\n",
    "        \"### Human‑rated Example\\n\"\n",
    "        f\"Question: {row['question']}\\n{choices}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer_text']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\"\n",
    "        \"Scores:\\n\"\n",
    "        f\"- Instruction Clarity: {row['eval_instruction_score']} ({row['eval_instruction_exp']})\\n\"\n",
    "        f\"- Accuracy of Correct Answer: {row['eval_accuracy_score']} ({row['eval_accuracy_exp']})\\n\"\n",
    "        f\"- Quality of Distractors: {row['eval_distractors_score']} ({row['eval_distractors_exp']})\\n\"\n",
    "        f\"- Word Difficulty Appropriateness: {row['eval_word_diff_score']} ({row['eval_word_diff_exp']})\\n\"\n",
    "        f\"- Task Difficulty Alignment: {row['eval_task_diff_score']} ({row['eval_task_diff_exp']})\\n\"\n",
    "        \"### End Example\\n\"\n",
    "    )\n",
    "\n",
    "# ---------- build example block once per question_type ----------\n",
    "EXAMPLE_BLOCKS = {\n",
    "    qtype: \"\\n\".join(fmt_human_example(r) for _, r in grp.iterrows())\n",
    "    for qtype, grp in human_df.groupby(\"question_type\")\n",
    "}\n",
    "\n",
    "RUBRIC = \"\"\"\n",
    "Rate on five binary metrics (1 = meets criterion, 0 = does not) and reply **JSON only**:\n",
    "\n",
    "{\n",
    " \"instr_score\":0/1, \"instr_exp\":\"...\",\n",
    " \"acc_score\":0/1,   \"acc_exp\":\"...\",\n",
    " \"dist_score\":0/1,  \"dist_exp\":\"...\",\n",
    " \"word_score\":0/1,  \"word_exp\":\"...\",\n",
    " \"task_score\":0/1,  \"task_exp\":\"...\",\n",
    " \"total_score\":0-5\n",
    "}\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "sem = asyncio.Semaphore(CONCURRENT)   # throttle parallelism\n",
    "\n",
    "async def rate_item(idx, row):\n",
    "    user_prompt = (\n",
    "        f\"{EXAMPLE_BLOCKS[row['question_type']]}\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "        \"### Item to evaluate\\n\"\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"A) {row['choice_a']}\\n\"\n",
    "        f\"B) {row['choice_b']}\\n\"\n",
    "        f\"C) {row['choice_c']}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\\n\"\n",
    "        f\"{RUBRIC}\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a meticulous K‑12 morphology test reviewer.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    async with sem:                           # limit concurrent calls\n",
    "        try:\n",
    "            rsp = await openai.ChatCompletion.acreate(\n",
    "                model=MODEL,\n",
    "                temperature=TEMPERATURE,\n",
    "                messages=messages\n",
    "            )\n",
    "            data = json.loads(rsp.choices[0].message[\"content\"])\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] row {idx} failed: {e}\")\n",
    "            data = {k: None for k in\n",
    "                    [\"instr_score\",\"instr_exp\",\"acc_score\",\"acc_exp\",\n",
    "                     \"dist_score\",\"dist_exp\",\"word_score\",\"word_exp\",\n",
    "                     \"task_score\",\"task_exp\",\"total_score\"]}\n",
    "\n",
    "    merged = row.to_dict()\n",
    "    merged.update(data)\n",
    "    return merged\n",
    "\n",
    "\n",
    "async def main():\n",
    "    tasks   = [rate_item(i, r) for i, r in items_df.iterrows()]\n",
    "    results = [await t for t in tqdm(asyncio.as_completed(tasks),\n",
    "                                     total=len(tasks))]\n",
    "    pd.DataFrame(results).to_csv(OUT_FILE, index=False)\n",
    "    print(\"✓ saved\", OUT_FILE)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e65124-b460-486b-93df-9315da16ec42",
   "metadata": {},
   "source": [
    "## concatenate and sort the generated question by GPT 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "deb2bfa7-3543-4e63-81dd-676e43bb8846",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → Final_generated_questions_GPT35_merged_sorted.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1.  read the two source files\n",
    "# -------------------------------------------------\n",
    "df1 = pd.read_csv(\"generated_questions_output.csv\")\n",
    "df2 = pd.read_csv(\"generated_questions_output_2_4.csv\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2.  concatenate rows\n",
    "# -------------------------------------------------\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3.  make task‑difficulty and prompting‑strategy ordered\n",
    "# -------------------------------------------------\n",
    "task_order = ['E', 'M', 'H']\n",
    "strategy_order = [\n",
    "    'zero_shot',\n",
    "    'few_shot',\n",
    "    'chain_of_thought',\n",
    "    'chain_of_thought_plus_role_chain',\n",
    "    'chain_of_thought_plus_sequential',\n",
    "    'chain_of_thought_plus_sequential_rl'\n",
    "]\n",
    "\n",
    "df['task_difficulty']      = pd.Categorical(df['task_difficulty'],\n",
    "                                            categories=task_order,\n",
    "                                            ordered=True)\n",
    "df['prompting_strategy']   = pd.Categorical(df['prompting_strategy'],\n",
    "                                            categories=strategy_order,\n",
    "                                            ordered=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4.  sort \n",
    "# -------------------------------------------------\n",
    "df_sorted = df.sort_values(\n",
    "    ['question_type', 'word_difficulty',\n",
    "     'task_difficulty', 'prompting_strategy']\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5.  write the merged, sorted file\n",
    "# -------------------------------------------------\n",
    "output_path = \"Final_generated_questions_GPT35_merged_sorted.csv\"\n",
    "df_sorted.to_csv(output_path, index=False)\n",
    "print(f\"Saved → {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84fc04a-6366-4f2d-ac28-eccf2a9455a8",
   "metadata": {},
   "source": [
    "# Final Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fa04f1c-815d-409c-a7ba-69679668a46d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL NaN cells: 21712\n",
      "\n",
      "NaNs per column:\n",
      "psychometrician_reasoning    3509\n",
      "teacher_reasoning            3003\n",
      "student_reasoning            2994\n",
      "step_2                       2927\n",
      "step_3                       2926\n",
      "step_1                       2926\n",
      "chain_of_thought             2819\n",
      "generated_text                586\n",
      "choice_c                        6\n",
      "choice_b                        6\n",
      "choice_a                        5\n",
      "correct_answer                  4\n",
      "question                        1\n",
      "word_difficulty                 0\n",
      "prompting_strategy              0\n",
      "task_difficulty                 0\n",
      "question_type                   0\n",
      "dtype: int64\n",
      "\n",
      "Rows containing at least one NaN: 3510\n",
      "\n",
      "NaNs in choice columns only:\n",
      "choice_a    5\n",
      "choice_b    6\n",
      "choice_c    6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = \"Final_generated_questions_GPT35_merged_sorted.csv\"\n",
    "\n",
    "df = pd.read_csv(FILE, dtype=str)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1)  overall check: does the whole DataFrame contain NaNs?\n",
    "# -----------------------------------------------------------\n",
    "total_missing = df.isna().sum().sum()\n",
    "print(f\"TOTAL NaN cells: {total_missing}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2)  count NaNs per column\n",
    "# -----------------------------------------------------------\n",
    "print(\"\\nNaNs per column:\")\n",
    "print(df.isna().sum().sort_values(ascending=False))\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3)  inspect rows that have *any* NaN\n",
    "# -----------------------------------------------------------\n",
    "rows_with_nan = df[df.isna().any(axis=1)]\n",
    "print(f\"\\nRows containing at least one NaN: {len(rows_with_nan)}\")\n",
    "\n",
    "# ▶ if you want to see them:\n",
    "# print(rows_with_nan)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4)  focus on the choice columns (if that’s where the crash happened)\n",
    "# -----------------------------------------------------------\n",
    "choice_cols = [c for c in df.columns if c.lower().startswith(\"choice_\")]\n",
    "print(\"\\nNaNs in choice columns only:\")\n",
    "print(df[choice_cols].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f39ae401-3912-4f73-ba51-55467f6f5d84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 1513  →  missing: correct_answer\n",
      "row 2170  →  missing: choice_c, correct_answer\n",
      "row 2194  →  missing: choice_b\n",
      "row 2570  →  missing: choice_a, choice_b, choice_c\n",
      "row 2714  →  missing: question, choice_a, choice_b, choice_c, correct_answer\n",
      "row 2783  →  missing: correct_answer\n",
      "row 3073  →  missing: choice_a, choice_b, choice_c\n",
      "row 3252  →  missing: choice_a, choice_b, choice_c\n",
      "row 3416  →  missing: choice_a, choice_b, choice_c\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = \"Final_generated_questions_GPT35_merged_sorted.csv\"\n",
    "df   = pd.read_csv(FILE, dtype=str)\n",
    "\n",
    "# ­­­­­­­­­­­ columns you care about ­­­­­­­­­­\n",
    "focus_cols = [\n",
    "    \"question\",\n",
    "    \"choice_a\",\n",
    "    \"choice_b\",\n",
    "    \"choice_c\",\n",
    "    \"correct_answer\",\n",
    "    \"word_difficulty\",\n",
    "    \"task_difficulty\",\n",
    "    \"prompting_strategy\",\n",
    "    \"question_type\",\n",
    "]\n",
    "\n",
    "# 1) mask: True where any of the focus columns is NaN\n",
    "mask = df[focus_cols].isna().any(axis=1)\n",
    "\n",
    "# 2) rows that have a NaN in those columns\n",
    "bad_rows = df[mask]\n",
    "\n",
    "# 3) show the row indices and which column(s) are missing\n",
    "for idx, row in bad_rows.iterrows():\n",
    "    missing_cols = [c for c in focus_cols if pd.isna(row[c])]\n",
    "    print(f\"row {idx}  →  missing: {', '.join(missing_cols)}\")\n",
    "\n",
    "# 4) (optional) if you want to look at the full rows:\n",
    "# print(bad_rows[focus_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79429959-74a7-478e-a968-37b612115b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kept 3501 of 3510 rows\n",
      "✓ saved cleaned file → Final_generated_questions_GPT35_clean.csv\n"
     ]
    }
   ],
   "source": [
    "FILE_IN  = \"Final_generated_questions_GPT35_merged_sorted.csv\"\n",
    "FILE_OUT = \"Final_generated_questions_GPT35_clean.csv\"\n",
    "\n",
    "# load\n",
    "df = pd.read_csv(FILE_IN, dtype=str)\n",
    "\n",
    "# columns that must be complete\n",
    "req_cols = [\n",
    "    \"question\",\n",
    "    \"choice_a\",\n",
    "    \"choice_b\",\n",
    "    \"choice_c\",\n",
    "    \"correct_answer\",\n",
    "    \"word_difficulty\",\n",
    "    \"task_difficulty\",\n",
    "    \"prompting_strategy\",\n",
    "    \"question_type\",\n",
    "]\n",
    "\n",
    "# drop rows that have a NaN (or empty string) in any required column\n",
    "df_clean = (\n",
    "    df.dropna(subset=req_cols)          # remove true NaNs\n",
    "      .query(\" & \".join([f\"{c} != ''\" for c in req_cols]))   # remove empty strings\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"kept {len(df_clean)} of {len(df)} rows\")\n",
    "\n",
    "df_clean.to_csv(FILE_OUT, index=False)\n",
    "print(f\"✓ saved cleaned file → {FILE_OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdd27017-6dd2-447d-a00a-ab03910c2477",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompting_strategy → unique values:\n",
      "['zero_shot' 'few_shot' 'chain_of_thought'\n",
      " 'chain_of_thought_plus_role_chain' 'chain_of_thought_plus_sequential'\n",
      " 'chain_of_thought_plus_sequential_rl']\n",
      "\n",
      "question_type (raw) → dtype: object\n",
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10'] ...\n",
      "\n",
      "word_difficulty → unique values:\n",
      "['1' '2' '3' '4' '5']\n",
      "\n",
      "task_difficulty → unique values:\n",
      "['E' 'M' 'H']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Final_generated_questions_GPT35_clean.csv\", dtype=str)\n",
    "\n",
    "print(\"prompting_strategy → unique values:\")\n",
    "print(df[\"prompting_strategy\"].unique())\n",
    "\n",
    "print(\"\\nquestion_type (raw) → dtype:\", df[\"question_type\"].dtype)\n",
    "print(df[\"question_type\"].unique()[:10], \"...\")\n",
    "\n",
    "print(\"\\nword_difficulty → unique values:\")\n",
    "print(df[\"word_difficulty\"].unique())\n",
    "\n",
    "print(\"\\ntask_difficulty → unique values:\")\n",
    "print(df[\"task_difficulty\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4a6c3b-f150-4568-9e67-362e44b21b74",
   "metadata": {},
   "source": [
    "### Final Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a1502ca8-b0a8-49bb-bb27-8c235c4056ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items after filtering: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fa5c98345e4ee8a869ac4901b82a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] row 6 failed: Expecting value: line 1 column 1 (char 0)\n",
      "[warn] row 24 failed: Expecting value: line 1 column 1 (char 0)\n",
      "[warn] row 52 failed: Expecting value: line 1 column 1 (char 0)\n",
      "[warn] row 84 failed: Expecting value: line 1 column 1 (char 0)\n",
      "✓ Completed – streamed to gpt_o4_mini_evaluations.csv\n"
     ]
    }
   ],
   "source": [
    "# %% FILTERS you care about\n",
    "#     – leave key absent or empty list to keep *all* rows for that column\n",
    "# FILTERS = {\n",
    "#     \"prompting_strategy\": [\"chain_of_thought_plus_role_chain\"],\n",
    "#     \"question_type\":      [2],\n",
    "#     \"word_difficulty\":    [\"1\"],\n",
    "#     \"task_difficulty\":    [\"E\"],          # 'E' | 'M' | 'H'\n",
    "# }\n",
    "FILTERS = {}\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Imports / config\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "import os, json, math, asyncio, pandas as pd, openai\n",
    "from tqdm.notebook import tqdm\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "HUMAN_FILE = \"mcq_human_evals.csv\"\n",
    "ITEM_FILE  = \"Final_generated_questions_GPT35_clean.csv\"\n",
    "OUT_FILE   = \"gpt_o4_mini_evaluations.csv\"\n",
    "\n",
    "MODEL       = \"o4-mini\"\n",
    "TEMPERATURE = 1.0\n",
    "CONCURRENT  = 10          # parallel requests\n",
    "\n",
    "# load_dotenv()                              # uncomment if you use .env\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Load & filter data\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "items_df = pd.read_csv(ITEM_FILE,  dtype=str)\n",
    "human_df = pd.read_csv(HUMAN_FILE, dtype=str)\n",
    "\n",
    "human_df[\"question_type\"] = human_df[\"question_type\"].astype(int)\n",
    "items_df[\"question_type\"] = items_df[\"question_type\"].astype(int)\n",
    "\n",
    "for col, allowed in FILTERS.items():\n",
    "    if allowed:                                   # skip empty lists / None\n",
    "        items_df = items_df[items_df[col].isin([str(x) for x in allowed])]\n",
    "\n",
    "items_df = items_df.head(100)        \n",
    "print(\"Items after filtering:\", len(items_df))\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Helper – robust human‑example formatter  (NaN‑safe)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "def fmt_human_example(row) -> str:\n",
    "    \"\"\"Return one nicely formatted human‑rated example, skipping NaN/empty choices.\"\"\"\n",
    "    choice_lines = []\n",
    "    for i in range(1, 5):\n",
    "        val = row.get(f\"choice_{i}\", \"\")\n",
    "        # skip if val is NaN or empty\n",
    "        if val is None or (isinstance(val, float) and math.isnan(val)):\n",
    "            continue\n",
    "        txt = str(val).strip()\n",
    "        if not txt or txt.lower() == \"nan\":\n",
    "            continue\n",
    "        choice_lines.append(f\"{chr(64+i)}) {txt}\")\n",
    "\n",
    "    choices = \"\\n\".join(choice_lines)\n",
    "\n",
    "    return (\n",
    "        \"### Human‑rated Example\\n\"\n",
    "        f\"Question: {row['question']}\\n{choices}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer_text']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\"\n",
    "        \"Scores:\\n\"\n",
    "        f\"- Instruction Clarity: {row['eval_instruction_score']} ({row['eval_instruction_exp']})\\n\"\n",
    "        f\"- Accuracy of Correct Answer: {row['eval_accuracy_score']} ({row['eval_accuracy_exp']})\\n\"\n",
    "        f\"- Quality of Distractors: {row['eval_distractors_score']} ({row['eval_distractors_exp']})\\n\"\n",
    "        f\"- Word Difficulty Appropriateness: {row['eval_word_diff_score']} ({row['eval_word_diff_exp']})\\n\"\n",
    "        f\"- Task Difficulty Alignment: {row['eval_task_diff_score']} ({row['eval_task_diff_exp']})\\n\"\n",
    "        \"### End Example\\n\"\n",
    "    )\n",
    "\n",
    "# cache the example block once per question_type\n",
    "EXAMPLE_BLOCKS = {\n",
    "    q: \"\\n\".join(fmt_human_example(r) for _, r in g.iterrows())\n",
    "    for q, g in human_df.groupby(\"question_type\")\n",
    "}\n",
    "\n",
    "RUBRIC = \"\"\"\n",
    "Rate on five binary metrics (1 = meets, 0 = does not) and reply JSON only:\n",
    "{\n",
    " \"instr_score\":0/1, \"instr_exp\":\"...\",\n",
    " \"acc_score\":0/1,   \"acc_exp\":\"...\",\n",
    " \"dist_score\":0/1,  \"dist_exp\":\"...\",\n",
    " \"word_score\":0/1,  \"word_exp\":\"...\",\n",
    " \"task_score\":0/1,  \"task_exp\":\"...\",\n",
    " \"total_score\":0-5\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  CSV streaming helper\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "def append_row(row_dict: dict, path: str):\n",
    "    \"\"\"Append one row to CSV; create file+header on first call.\"\"\"\n",
    "    first_write = not os.path.exists(path)\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as fp:\n",
    "        writer = csv.DictWriter(fp, fieldnames=row_dict.keys())\n",
    "        if first_write:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row_dict)\n",
    "\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  Async evaluator\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "sem = asyncio.Semaphore(CONCURRENT)\n",
    "\n",
    "async def rate_item(idx, row):\n",
    "    choice_a = str(row.get(\"choice_a\", \"\") or \"\").strip()\n",
    "    choice_b = str(row.get(\"choice_b\", \"\") or \"\").strip()\n",
    "    choice_c = str(row.get(\"choice_c\", \"\") or \"\").strip()\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"{EXAMPLE_BLOCKS[row['question_type']]}\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "        \"### Item to evaluate\\n\"\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"A) {choice_a}\\n\"\n",
    "        f\"B) {choice_b}\\n\"\n",
    "        f\"C) {choice_c}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\\n\"\n",
    "        f\"{RUBRIC}\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a meticulous K‑12 morphology test reviewer.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    async with sem:\n",
    "        try:\n",
    "            rsp  = await openai.ChatCompletion.acreate(\n",
    "                model=MODEL, temperature=TEMPERATURE, messages=messages\n",
    "            )\n",
    "            data = json.loads(rsp.choices[0].message[\"content\"])\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] row {idx} failed: {e}\")\n",
    "            data = {k: None for k in\n",
    "                    [\"instr_score\",\"instr_exp\",\"acc_score\",\"acc_exp\",\n",
    "                     \"dist_score\",\"dist_exp\",\"word_score\",\"word_exp\",\n",
    "                     \"task_score\",\"task_exp\",\"total_score\"]}\n",
    "\n",
    "    merged = row.to_dict()\n",
    "    merged.update(data)\n",
    "    append_row(merged, OUT_FILE)      # ✨ write immediately\n",
    "    return merged\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  Orchestration\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "async def main():\n",
    "    tasks = [asyncio.create_task(rate_item(i, r))\n",
    "             for i, r in items_df.iterrows()]\n",
    "\n",
    "    # live progress bar as rows finish (order is non‑deterministic)\n",
    "    for f in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "        await f\n",
    "\n",
    "    print(f\"✓ Completed – streamed to {OUT_FILE}\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d357b-27a8-440b-81fb-dae56e62d74f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## replace the current sequential_rl strategy with 'Final_generated_questions_GPT_chain_of_thought_plus_sequential_rl.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c08b61ea-d093-43a5-8af0-9dbc453757d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ builtin rows: 2917 | new rows: 584 → combined: 3501 \n",
      "Saved to Final_generated_questions_GPT35_clean_UPDATED_new.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ── paths ───────────────────────────────────────────────────────────\n",
    "MAIN_CSV   = \"Final_generated_questions_GPT35_clean.csv\"\n",
    "UPDATE_CSV = \"Final_generated_questions_GPT_chain_of_thought_plus_sequential_rl_new.csv\"\n",
    "OUT_CSV    = \"Final_generated_questions_GPT35_clean_UPDATED_new.csv\"\n",
    "\n",
    "TARGET_STRATEGY = \"chain_of_thought_plus_sequential_rl\"\n",
    "\n",
    "# ── load both files ────────────────────────────────────────────────\n",
    "main_df   = pd.read_csv(MAIN_CSV,   dtype=str)\n",
    "update_df = pd.read_csv(UPDATE_CSV, dtype=str)\n",
    "\n",
    "# ── 1) drop all old rows for that strategy from the main data ──────\n",
    "main_df = main_df[ main_df[\"prompting_strategy\"] != TARGET_STRATEGY ]\n",
    "\n",
    "# ── 2) put the update data columns in the *same* order as main_df ──\n",
    "update_df = update_df.reindex(columns=main_df.columns)\n",
    "\n",
    "# ── 3) append the new rows and write out ───────────────────────────\n",
    "final_df = pd.concat([main_df, update_df], ignore_index=True)\n",
    "final_df.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(\"✓ builtin rows:\", len(main_df),\n",
    "      \"| new rows:\", len(update_df),\n",
    "      \"→ combined:\", len(final_df),\n",
    "      \"\\nSaved to\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d2cfcaa2-20ab-4d25-9be6-abf4ac11eb9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_order = ['E', 'M', 'H']\n",
    "strategy_order = [\n",
    "    'zero_shot',\n",
    "    'few_shot',\n",
    "    'chain_of_thought',\n",
    "    'chain_of_thought_plus_role_chain',\n",
    "    'chain_of_thought_plus_sequential',\n",
    "    'chain_of_thought_plus_sequential_rl'\n",
    "]\n",
    "\n",
    "final_df['task_difficulty']      = pd.Categorical(final_df['task_difficulty'],\n",
    "                                            categories=task_order,\n",
    "                                            ordered=True)\n",
    "final_df['prompting_strategy']   = pd.Categorical(final_df['prompting_strategy'],\n",
    "                                            categories=strategy_order,\n",
    "                                            ordered=True)\n",
    "\n",
    "\n",
    "final_df_sorted = final_df.sort_values(\n",
    "    ['question_type', 'word_difficulty',\n",
    "     'task_difficulty', 'prompting_strategy']\n",
    ")\n",
    "final_df_sorted.to_csv(OUT_CSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6785d1-0a12-40aa-b0df-c5ad53001c44",
   "metadata": {},
   "source": [
    "## FINAL FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7f812927-d7e3-49dc-be7a-0f2820a1db31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items after filtering: 3501\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b8cb7bdb9a4a469f31e056405f1e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed – streamed to gpt_41_mini_evaluations_of_gpt_35_UPDATED_new.csv\n"
     ]
    }
   ],
   "source": [
    "# %% FILTERS you care about\n",
    "#     – leave key absent or empty list to keep *all* rows for that column\n",
    "# FILTERS = {\n",
    "#     \"prompting_strategy\": [\"chain_of_thought_plus_role_chain\"],\n",
    "#     \"question_type\":      [2],\n",
    "#     \"word_difficulty\":    [\"1\"],\n",
    "#     \"task_difficulty\":    [\"E\"],          # 'E' | 'M' | 'H'\n",
    "# }\n",
    "FILTERS = {}\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Imports / config\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "import os, json, math, asyncio, pandas as pd, openai\n",
    "from tqdm.notebook import tqdm\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "HUMAN_FILE = \"mcq_human_evals.csv\"\n",
    "ITEM_FILE  = \"Final_generated_questions_GPT35_clean_UPDATED_new.csv\"\n",
    "OUT_FILE   = \"gpt_41_mini_evaluations_of_gpt_35_UPDATED_new.csv\"\n",
    "\n",
    "# - gpt-4.1-mini (created: 2025-04-10 20:49:33)\n",
    "#  - gpt-4.1-mini-2025-04-14 (created: 2025-04-10 20:39:07)\n",
    "\n",
    "MODEL       = \"gpt-4.1-mini\"\n",
    "TEMPERATURE = 0.0\n",
    "CONCURRENT  = 10          # parallel requests\n",
    "\n",
    "# load_dotenv()                              # uncomment if you use .env\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Load & filter data\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "items_df = pd.read_csv(ITEM_FILE,  dtype=str)\n",
    "human_df = pd.read_csv(HUMAN_FILE, dtype=str)\n",
    "\n",
    "human_df[\"question_type\"] = human_df[\"question_type\"].astype(int)\n",
    "items_df[\"question_type\"] = items_df[\"question_type\"].astype(int)\n",
    "\n",
    "for col, allowed in FILTERS.items():\n",
    "    if allowed:                                   # skip empty lists / None\n",
    "        items_df = items_df[items_df[col].isin([str(x) for x in allowed])]\n",
    "\n",
    "# items_df = items_df.head(100)        \n",
    "print(\"Items after filtering:\", len(items_df))\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Helper – robust human‑example formatter  (NaN‑safe)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "def fmt_human_example(row) -> str:\n",
    "    \"\"\"Return one nicely formatted human‑rated example, skipping NaN/empty choices.\"\"\"\n",
    "    choice_lines = []\n",
    "    for i in range(1, 5):\n",
    "        val = row.get(f\"choice_{i}\", \"\")\n",
    "        # skip if val is NaN or empty\n",
    "        if val is None or (isinstance(val, float) and math.isnan(val)):\n",
    "            continue\n",
    "        txt = str(val).strip()\n",
    "        if not txt or txt.lower() == \"nan\":\n",
    "            continue\n",
    "        choice_lines.append(f\"{chr(64+i)}) {txt}\")\n",
    "\n",
    "    choices = \"\\n\".join(choice_lines)\n",
    "\n",
    "    return (\n",
    "        \"### Human‑rated Example\\n\"\n",
    "        f\"Question: {row['question']}\\n{choices}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer_text']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\"\n",
    "        \"Scores:\\n\"\n",
    "        f\"- Instruction Clarity: {row['eval_instruction_score']} ({row['eval_instruction_exp']})\\n\"\n",
    "        f\"- Accuracy of Correct Answer: {row['eval_accuracy_score']} ({row['eval_accuracy_exp']})\\n\"\n",
    "        f\"- Quality of Distractors: {row['eval_distractors_score']} ({row['eval_distractors_exp']})\\n\"\n",
    "        f\"- Word Difficulty Appropriateness: {row['eval_word_diff_score']} ({row['eval_word_diff_exp']})\\n\"\n",
    "        f\"- Task Difficulty Alignment: {row['eval_task_diff_score']} ({row['eval_task_diff_exp']})\\n\"\n",
    "        \"### End Example\\n\"\n",
    "    )\n",
    "\n",
    "# cache the example block once per question_type\n",
    "EXAMPLE_BLOCKS = {\n",
    "    q: \"\\n\".join(fmt_human_example(r) for _, r in g.iterrows())\n",
    "    for q, g in human_df.groupby(\"question_type\")\n",
    "}\n",
    "\n",
    "RUBRIC = \"\"\"\n",
    "Rate on five binary metrics (1 = meets, 0 = does not) and reply JSON only:\n",
    "{\n",
    " \"instr_score\":0/1, \"instr_exp\":\"...\",\n",
    " \"acc_score\":0/1,   \"acc_exp\":\"...\",\n",
    " \"dist_score\":0/1,  \"dist_exp\":\"...\",\n",
    " \"word_score\":0/1,  \"word_exp\":\"...\",\n",
    " \"task_score\":0/1,  \"task_exp\":\"...\",\n",
    " \"total_score\":0-5\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  CSV streaming helper\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "def append_row(row_dict: dict, path: str):\n",
    "    \"\"\"Append one row to CSV; create file+header on first call.\"\"\"\n",
    "    first_write = not os.path.exists(path)\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as fp:\n",
    "        writer = csv.DictWriter(fp, fieldnames=row_dict.keys())\n",
    "        if first_write:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row_dict)\n",
    "\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  Async evaluator\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "sem = asyncio.Semaphore(CONCURRENT)\n",
    "\n",
    "async def rate_item(idx, row):\n",
    "    choice_a = str(row.get(\"choice_a\", \"\") or \"\").strip()\n",
    "    choice_b = str(row.get(\"choice_b\", \"\") or \"\").strip()\n",
    "    choice_c = str(row.get(\"choice_c\", \"\") or \"\").strip()\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"{EXAMPLE_BLOCKS[row['question_type']]}\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "        \"### Item to evaluate\\n\"\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"A) {choice_a}\\n\"\n",
    "        f\"B) {choice_b}\\n\"\n",
    "        f\"C) {choice_c}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\\n\"\n",
    "        f\"{RUBRIC}\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a meticulous K‑12 morphology test reviewer.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    async with sem:\n",
    "        try:\n",
    "            rsp  = await openai.ChatCompletion.acreate(\n",
    "                model=MODEL, temperature=TEMPERATURE, messages=messages\n",
    "            )\n",
    "            data = json.loads(rsp.choices[0].message[\"content\"])\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] row {idx} failed: {e}\")\n",
    "            data = {k: None for k in\n",
    "                    [\"instr_score\",\"instr_exp\",\"acc_score\",\"acc_exp\",\n",
    "                     \"dist_score\",\"dist_exp\",\"word_score\",\"word_exp\",\n",
    "                     \"task_score\",\"task_exp\",\"total_score\"]}\n",
    "\n",
    "    merged = row.to_dict()\n",
    "    merged.update(data)\n",
    "    append_row(merged, OUT_FILE)      # ✨ write immediately\n",
    "    return merged\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  Orchestration\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "async def main():\n",
    "    tasks = [asyncio.create_task(rate_item(i, r))\n",
    "             for i, r in items_df.iterrows()]\n",
    "\n",
    "    # live progress bar as rows finish (order is non‑deterministic)\n",
    "    for f in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "        await f\n",
    "\n",
    "    print(f\"✓ Completed – streamed to {OUT_FILE}\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "14af5149-b770-4581-9be4-8c4a103aa05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓  Saved /blue/babajani.a/babak.ahmadi/NLP_Dorr/Project/MA/gpt_41_mini_evaluations_of_gpt_35_UPDATED_new.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "# --- filenames ---------------------------------------------------------------\n",
    "csv_file   = pathlib.Path(\"gpt_41_mini_evaluations_of_gpt_35_UPDATED_new.csv\")\n",
    "excel_file = csv_file.with_suffix(\".xlsx\")         # same name, .xlsx extension\n",
    "\n",
    "# --- convert -----------------------------------------------------------------\n",
    "df = pd.read_csv(csv_file, encoding=\"utf-8\")        # adjust encoding if needed\n",
    "df.to_excel(excel_file, index=False)\n",
    "\n",
    "print(f\"✓  Saved {excel_file.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce02394-cf83-4dce-a022-1e077cadd77b",
   "metadata": {},
   "source": [
    "## evaluate the items generated by Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c07fd517-b7d2-4f41-b05c-89bad26ea76e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items after filtering: 2143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7727b1e070b54667ab975aeffa78c721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] row 253 failed: Request timed out\n",
      "✓ Completed – streamed to gpt_41_mini_evaluations_of_gemma.csv\n"
     ]
    }
   ],
   "source": [
    "# %% FILTERS you care about\n",
    "#     – leave key absent or empty list to keep *all* rows for that column\n",
    "# FILTERS = {\n",
    "#     \"prompting_strategy\": [\"chain_of_thought_plus_role_chain\"],\n",
    "#     \"question_type\":      [2],\n",
    "#     \"word_difficulty\":    [\"1\"],\n",
    "#     \"task_difficulty\":    [\"E\"],          # 'E' | 'M' | 'H'\n",
    "# }\n",
    "FILTERS = {}\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Imports / config\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "import os, json, math, asyncio, pandas as pd, openai\n",
    "from tqdm.notebook import tqdm\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "HUMAN_FILE = \"mcq_human_evals.csv\"\n",
    "ITEM_FILE  = \"Final_generated_questions_Gemma.csv\"\n",
    "OUT_FILE   = \"gpt_41_mini_evaluations_of_gemma.csv\"\n",
    "\n",
    "# - gpt-4.1-mini (created: 2025-04-10 20:49:33)\n",
    "#  - gpt-4.1-mini-2025-04-14 (created: 2025-04-10 20:39:07)\n",
    "\n",
    "MODEL       = \"gpt-4.1-mini\"\n",
    "TEMPERATURE = 0.0\n",
    "CONCURRENT  = 10          # parallel requests\n",
    "\n",
    "# load_dotenv()                              # uncomment if you use .env\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Load & filter data\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "items_df = pd.read_csv(ITEM_FILE,  dtype=str)\n",
    "human_df = pd.read_csv(HUMAN_FILE, dtype=str)\n",
    "\n",
    "# ── rename columns in items_df so the rest of the script works unchanged ─────\n",
    "items_df = items_df.rename(columns={\n",
    "    \"Prompting_strategy\": \"prompting_strategy\",\n",
    "    \"Question_Type\":      \"question_type\",\n",
    "    \"Word_Difficulty\":    \"word_difficulty\",\n",
    "    \"Task_Difficulty\":    \"task_difficulty\",\n",
    "    \"Question\":           \"question\",\n",
    "    \"Correct_Answer\":     \"correct_answer\",\n",
    "    \"Choice_1\":           \"choice_a\",\n",
    "    \"Choice_2\":           \"choice_b\",\n",
    "    \"Choice_3\":           \"choice_c\",\n",
    "    # keep any other columns (e.g., 'Text') as‑is\n",
    "})\n",
    "\n",
    "human_df[\"question_type\"] = human_df[\"question_type\"].astype(int)\n",
    "items_df[\"question_type\"] = items_df[\"question_type\"].astype(int)\n",
    "\n",
    "for col, allowed in FILTERS.items():\n",
    "    if allowed:                                   # skip empty lists / None\n",
    "        items_df = items_df[items_df[col].isin([str(x) for x in allowed])]\n",
    "\n",
    "# items_df = items_df.head(10)        \n",
    "print(\"Items after filtering:\", len(items_df))\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Helper – robust human‑example formatter  (NaN‑safe)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "def fmt_human_example(row) -> str:\n",
    "    \"\"\"Return one nicely formatted human‑rated example, skipping NaN/empty choices.\"\"\"\n",
    "    choice_lines = []\n",
    "    for i in range(1, 5):\n",
    "        val = row.get(f\"choice_{i}\", \"\")\n",
    "        # skip if val is NaN or empty\n",
    "        if val is None or (isinstance(val, float) and math.isnan(val)):\n",
    "            continue\n",
    "        txt = str(val).strip()\n",
    "        if not txt or txt.lower() == \"nan\":\n",
    "            continue\n",
    "        choice_lines.append(f\"{chr(64+i)}) {txt}\")\n",
    "\n",
    "    choices = \"\\n\".join(choice_lines)\n",
    "\n",
    "    return (\n",
    "        \"### Human‑rated Example\\n\"\n",
    "        f\"Question: {row['question']}\\n{choices}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer_text']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\"\n",
    "        \"Scores:\\n\"\n",
    "        f\"- Instruction Clarity: {row['eval_instruction_score']} ({row['eval_instruction_exp']})\\n\"\n",
    "        f\"- Accuracy of Correct Answer: {row['eval_accuracy_score']} ({row['eval_accuracy_exp']})\\n\"\n",
    "        f\"- Quality of Distractors: {row['eval_distractors_score']} ({row['eval_distractors_exp']})\\n\"\n",
    "        f\"- Word Difficulty Appropriateness: {row['eval_word_diff_score']} ({row['eval_word_diff_exp']})\\n\"\n",
    "        f\"- Task Difficulty Alignment: {row['eval_task_diff_score']} ({row['eval_task_diff_exp']})\\n\"\n",
    "        \"### End Example\\n\"\n",
    "    )\n",
    "\n",
    "# cache the example block once per question_type\n",
    "EXAMPLE_BLOCKS = {\n",
    "    q: \"\\n\".join(fmt_human_example(r) for _, r in g.iterrows())\n",
    "    for q, g in human_df.groupby(\"question_type\")\n",
    "}\n",
    "\n",
    "RUBRIC = \"\"\"\n",
    "Rate on five binary metrics (1 = meets, 0 = does not) and reply JSON only:\n",
    "{\n",
    " \"instr_score\":0/1, \"instr_exp\":\"...\",\n",
    " \"acc_score\":0/1,   \"acc_exp\":\"...\",\n",
    " \"dist_score\":0/1,  \"dist_exp\":\"...\",\n",
    " \"word_score\":0/1,  \"word_exp\":\"...\",\n",
    " \"task_score\":0/1,  \"task_exp\":\"...\",\n",
    " \"total_score\":0-5\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  CSV streaming helper\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "def append_row(row_dict: dict, path: str):\n",
    "    \"\"\"Append one row to CSV; create file+header on first call.\"\"\"\n",
    "    first_write = not os.path.exists(path)\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as fp:\n",
    "        writer = csv.DictWriter(fp, fieldnames=row_dict.keys())\n",
    "        if first_write:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row_dict)\n",
    "\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  Async evaluator\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "sem = asyncio.Semaphore(CONCURRENT)\n",
    "\n",
    "async def rate_item(idx, row):\n",
    "    choice_a = str(row.get(\"choice_a\", \"\") or \"\").strip()\n",
    "    choice_b = str(row.get(\"choice_b\", \"\") or \"\").strip()\n",
    "    choice_c = str(row.get(\"choice_c\", \"\") or \"\").strip()\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"{EXAMPLE_BLOCKS[row['question_type']]}\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "        \"### Item to evaluate\\n\"\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"A) {choice_a}\\n\"\n",
    "        f\"B) {choice_b}\\n\"\n",
    "        f\"C) {choice_c}\\n\"\n",
    "        f\"Correct Answer: {row['correct_answer']}\\n\"\n",
    "        f\"word_difficulty={row['word_difficulty']} \"\n",
    "        f\"task_difficulty={row['task_difficulty']}\\n\\n\"\n",
    "        f\"{RUBRIC}\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a meticulous K‑12 morphology test reviewer.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    async with sem:\n",
    "        try:\n",
    "            rsp  = await openai.ChatCompletion.acreate(\n",
    "                model=MODEL, temperature=TEMPERATURE, messages=messages\n",
    "            )\n",
    "            data = json.loads(rsp.choices[0].message[\"content\"])\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] row {idx} failed: {e}\")\n",
    "            data = {k: None for k in\n",
    "                    [\"instr_score\",\"instr_exp\",\"acc_score\",\"acc_exp\",\n",
    "                     \"dist_score\",\"dist_exp\",\"word_score\",\"word_exp\",\n",
    "                     \"task_score\",\"task_exp\",\"total_score\"]}\n",
    "\n",
    "    merged = row.to_dict()\n",
    "    merged.update(data)\n",
    "    append_row(merged, OUT_FILE)      #  write immediately\n",
    "    return merged\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "#  Orchestration\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "async def main():\n",
    "    tasks = [asyncio.create_task(rate_item(i, r))\n",
    "             for i, r in items_df.iterrows()]\n",
    "\n",
    "    # live progress bar as rows finish (order is non‑deterministic)\n",
    "    for f in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "        await f\n",
    "\n",
    "    print(f\"✓ Completed – streamed to {OUT_FILE}\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c32b5a78-bbba-4152-96a7-8b08cfe16f33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓  Saved /blue/babajani.a/babak.ahmadi/NLP_Dorr/Project/MA/gpt_41_mini_evaluations_of_gemma.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --- filenames ---------------------------------------------------------------\n",
    "csv_file   = pathlib.Path(\"gpt_41_mini_evaluations_of_gemma.csv\")\n",
    "excel_file = csv_file.with_suffix(\".xlsx\")         # same name, .xlsx extension\n",
    "\n",
    "# --- convert -----------------------------------------------------------------\n",
    "df = pd.read_csv(csv_file, encoding=\"utf-8\")        # adjust encoding if needed\n",
    "df.to_excel(excel_file, index=False)\n",
    "\n",
    "print(f\"✓  Saved {excel_file.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
